{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te6rdZgT5yAW"
   },
   "source": [
    "# Copy of Previous RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdsGeayZ523e"
   },
   "source": [
    "### Reminders:\n",
    "\n",
    "Read up on some of these:\n",
    "- https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/\n",
    "- https://fairyonice.github.io/Stateful-LSTM-model-training-in-Keras.html \n",
    "- https://github.com/keras-team/keras/issues/5714\n",
    "- https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/\n",
    "\n",
    "###Shuffle Data!!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ro1oh9-25xM9",
    "outputId": "e2638721-dec3-4b6c-9db0-df28d7fce705"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yOaxkJ58hh"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5Lupm0J6Hre"
   },
   "source": [
    "## Do Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gE8NHB9v6IDc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "from math import isnan\n",
    "from scipy.stats import norm\n",
    "\n",
    "pandas.set_option('display.max_columns', None)\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yhg_bSj36CSt"
   },
   "source": [
    "## Obtain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1LMf25vB5_vs"
   },
   "outputs": [],
   "source": [
    "# read data from drive\n",
    "# csv_train = pandas.read_csv(\"drive/MyDrive/first_10k.csv\")\n",
    "csv_train = pandas.read_csv(\"first_10k.csv\")\n",
    "# csv_train = pandas.read_csv(\"drive/MyDrive/FDC_tracks.csv\")\n",
    "unparsed_train = np.array(csv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L54wTFh-NGU"
   },
   "source": [
    "## Ragged Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KoN7P86yCif6",
    "outputId": "b9ad9880-a607-4ac8-dd65-3e3a802f4a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.588573718740314, 47.716949659038654, 47.92074125234375, 0.0433333333333333, 96.0, 24.0, 343.50521373311966, 274.8641357421875, 3.24600255369344e-07, 137.76808719278492]\n",
      "[-47.69482374702719, -47.64816097490714, -47.78196688638672, 0.0100725857504721, 1.0, 1.0, 176.8656847042481, -77.76896667480467, 7.068405941829982e-10, 0.2999999999999999]\n",
      "[53.93938010927168, 182.16740477086037, 20.384216393930128, 80.4857546970404]\n",
      "[-59.97950523099952, -113.91363367061092, -21.351611455494343, -101.41199494730319]\n"
     ]
    }
   ],
   "source": [
    "_max = [-1000 for i in range(10)]\n",
    "_min = [1000 for i in range(10)]\n",
    "for index,event in enumerate(unparsed_train):\n",
    "  lower = 67\n",
    "  for upper in range(lower+14, event.shape[0]+1, 14):\n",
    "    d = event[lower:upper]\n",
    "    d = np.append(d[:2],d[6:])\n",
    "    for a in range(len(d)):\n",
    "      if d[a] > _max[a]:\n",
    "        _max[a] = d[a]\n",
    "      if d[a] < _min[a]:\n",
    "        _min[a] = d[a]\n",
    "    lower = upper\n",
    "print(_max)\n",
    "print(_min)\n",
    "\n",
    "_TOF_max = [-1000 for i in range(4)]\n",
    "_TOF_min = [1000 for i in range(4)]\n",
    "for index,event in enumerate(unparsed_train):\n",
    "  TOF = event[59:67]\n",
    "  # print(TOF)\n",
    "  for a in range(4):\n",
    "    if TOF[a*2] > _TOF_max[a]:\n",
    "      _TOF_max[a] = TOF[a*2]\n",
    "    if TOF[a*2] < _TOF_min[a]:\n",
    "      _TOF_min[a] = TOF[a*2]\n",
    "print(_TOF_max)\n",
    "print(_TOF_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOfDY5F8yJyl"
   },
   "source": [
    "After trying out many different possibilities, I have found making a RaggedTensor is the way to make variable timesteps.\n",
    "\n",
    "Once you create a RaggedTensor ONLY FOR X DATA, you need to also add:\n",
    "\n",
    "ragged=True\n",
    "\n",
    "to the keras.Inputs() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BfACUWvW-NGd"
   },
   "outputs": [],
   "source": [
    "def ragged_parser(unparsed):\n",
    "  global _min, _max\n",
    "  x_final = []\n",
    "  y_final = []\n",
    "  invCov_final = []\n",
    "  cov_final = []\n",
    "  for event in unparsed:\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    nEvent = event[0]     # all the data split into neat little arrays...\n",
    "    state = event[1:6]\n",
    "    coVar = event[6:31]\n",
    "    invCoVar = event[31:56]\n",
    "    goodnessOfFit = event[56:59]\n",
    "    TOF = event[59:67]\n",
    "\n",
    "    if goodnessOfFit[2] > 0.1:   # Cutting if rms is too high\n",
    "      continue\n",
    "    hits = []\n",
    "    lower = 67\n",
    "    for upper in range(lower+14, event.shape[0]+1, 14): # to flip just go from end to 67 by -14 steps?\n",
    "\n",
    "      # hasNAN = False\n",
    "      # for val in event[lower:upper]:\n",
    "      #   if isnan(val):\n",
    "      #     hasNAN = True\n",
    "      # if not hasNAN:\n",
    "\n",
    "      if not isnan(event[lower]):            # Check if we are done with hits, because data is cut short, the rest will be nan\n",
    "        hit_data = event[lower:upper]                      # retrieving the hit\n",
    "        hit_data = np.append(hit_data[:2],hit_data[6:])    # cutting out the sin and cos data\n",
    "        for z in range(len(hit_data)):\n",
    "          hit_data[z] = (hit_data[z] - _min[z]) / (_max[z] - _min[z])    # we need to normalize the data; this can be moved to a lambda layer in the network if needed.\n",
    "        for i_TOF in range(4):\n",
    "          TOF[i_TOF*2] = (TOF[i_TOF*2] - _TOF_min[i_TOF]) / (_TOF_max[i_TOF] - _TOF_min[i_TOF])\n",
    "        hit_data = np.append(hit_data,TOF)\n",
    "        hits.append(np.ndarray.tolist(hit_data))       # we want it as a list to convert to RaggedTensor later; last time I checked it didnt work with array.\n",
    "      lower = upper\n",
    "    for i in range(len(hits)):   # this could be simplified to just: \"x = hits\" if im not mistaken...\n",
    "      x.append(hits[i])          # however we might need to add y.append(hits[i+1]) for later testing so leaving it like this for now...\n",
    "    y = np.ndarray.tolist(state)   # technically not needed, can be removed later... at first I thought i need to pass RaggedTensor labels, but that is not the case.\n",
    "    x_final.append(x)          # want x_final to be shape (event, hit, 10) as a list\n",
    "    y_final.append(y)          # want y_final to be shape (event, 5)       as a np.array\n",
    "    # y_final.append(y[0])          # want y_final to be shape (event, 5)       as a np.array\n",
    "    invCov_final.append(invCoVar[:])  # want other_f to be shape (event, 25)      as a np.array\n",
    "    cov_final.append(coVar[:])\n",
    "  x_final = tf.ragged.constant(x_final)   # convert list to RaggedTensor because timesteps (number of hits) are variable between events\n",
    "  y_final = np.array(y_final)\n",
    "  invCov_final = np.array(invCov_final)\n",
    "  cov_final = np.array(cov_final)\n",
    "  return [x_final, invCov_final, cov_final, y_final], y_final   # with the custom loss the x_train (input) needs to be a list of [inputs, inverseCovariance, labels]\n",
    "  # return [x_final, invCov_final], y_final   # with the custom loss the x_train (input) needs to be a list of [inputs, inverseCovariance, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7H0PK6Q-NGg",
    "outputId": "6fcb6671-f32c-402a-c1a3-8489231a8d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--==Types==--\n",
      "--x_train:--\n",
      "\n",
      "  -> input_data: x_train[0]\n",
      "  -> type expected: RaggedTensor\n",
      " <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'>\n",
      "\n",
      "  -> invCov: x_train[1]\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "  -> y_train: x_train[2]\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "--y_train:--\n",
      "\n",
      "  -> y_train: y_train\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "--==Shapes==--\n",
      "--x_train:--  \n",
      "num of events: 9301\n",
      "\n",
      "  RaggedTensor | Input:  shape = (9301, 18, 18)\n",
      "  np.array     | InvCov: shape = (9301, 25)\n",
      "  np.array     | Labels: shape = (9301, 25)\n",
      "\n",
      "--x_train:--\n",
      "  np.array     | Labels: shape = (9301, 5)\n",
      "x_train : <tf.RaggedTensor [[0.7692640423774719, 0.765606164932251, 0.22407019138336182, 0.2642019987106323, 0.49473685026168823, 0.9130434989929199, 0.9742012023925781, 0.8189674615859985, 0.006027945317327976, 0.006027945317327976, 0.5265106558799744, 0.0, 0.4158459007740021, 1.0, 0.511589527130127, 0.0, 0.5575219988822937, 0.0], [0.6926482319831848, 0.5690751671791077, 0.3645309507846832, 0.05952613055706024, 0.2631579041481018, 0.8695651888847351, 0.9611853361129761, 0.28649309277534485, 0.033259935677051544, 0.033259935677051544, 0.5311324596405029, 0.0, 0.3861425220966339, 1.0, 0.5238472819328308, 0.0, 0.5605869889259338, 0.0], [0.23875504732131958, 0.24075621366500854, 0.7672280669212341, 0.10457572340965271, 0.5052631497383118, 0.782608687877655, 0.9352953433990479, 0.504021406173706, 0.018304066732525826, 0.018304066732525826, 0.5311729907989502, 0.0, 0.38604220747947693, 1.0, 0.5241410136222839, 0.0, 0.5606038570404053, 0.0], [0.33787238597869873, 0.4484703540802002, 0.6097946166992188, 0.07363839447498322, 0.7157894968986511, 0.739130437374115, 0.7676599621772766, 0.33562812209129333, 0.02666318602859974, 0.02666318602859974, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241480469703674, 0.0, 0.5606039762496948, 0.0], [0.5710769891738892, 0.6744455695152283, 0.37305063009262085, 0.288280189037323, 0.7052631378173828, 0.695652186870575, 0.7548211812973022, 0.7309820652008057, 0.005347346421331167, 0.005347346421331167, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.733315110206604, 0.7239073514938354, 0.263613760471344, 0.041055768728256226, 0.4842105209827423, 0.6521739363670349, 0.7418311238288879, 0.5430606007575989, 0.048400089144706726, 0.048400089144706726, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6543219089508057, 0.5472460389137268, 0.39544129371643066, 0.12399385124444962, 0.2947368323802948, 0.6086956262588501, 0.728915810585022, 0.5731130242347717, 0.015151274390518665, 0.015151274390518665, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.42824169993400574, 0.3272317349910736, 0.6255077123641968, 0.04272006079554558, 0.3052631616592407, 0.5652173757553101, 0.7160916924476624, 0.30231329798698425, 0.046525269746780396, 0.046525269746780396, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.2750035524368286, 0.28259047865867615, 0.7278001308441162, 0.024007415398955345, 0.5157894492149353, 0.52173912525177, 0.703081488609314, 0.3300199508666992, 0.08132698386907578, 0.08132698386907578, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.3927527666091919, 0.47597992420196533, 0.5672175288200378, 0.08297774195671082, 0.6631578803062439, 0.47826087474823, 0.4162442684173584, 0.5024588704109192, 0.023499751463532448, 0.023499751463532448, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.567786455154419, 0.6398308277130127, 0.392142653465271, 0.030097603797912598, 0.6421052813529968, 0.43478259444236755, 0.40303486585617065, 0.2945617437362671, 0.06557145714759827, 0.06557145714759827, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6758876442909241, 0.6613656878471375, 0.3252999782562256, 0.05598178505897522, 0.4736842215061188, 0.3913043439388275, 0.39034584164619446, 0.45713284611701965, 0.035419683903455734, 0.035419683903455734, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6067799925804138, 0.5218054056167603, 0.433348685503006, 0.06648389250040054, 0.3368421196937561, 0.3478260934352875, 0.3770085275173187, 0.44806209206581116, 0.029668668285012245, 0.029668668285012245, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.33219149708747864, 0.3447727859020233, 0.6660985350608826, 0.02286168746650219, 0.5263158082962036, 0.260869562625885, 0.35123491287231445, 0.42745885252952576, 0.085147425532341, 0.085147425532341, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.436598539352417, 0.4929949641227722, 0.5362541675567627, 0.03817495331168175, 0.6105263233184814, 0.21739129722118378, 0.0648939236998558, 0.2867984175682068, 0.052010804414749146, 0.052010804414749146, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6153600811958313, 0.6007120013237, 0.38774821162223816, 0.04457264766097069, 0.4736842215061188, 0.1304347813129425, 0.038748566061258316, 0.4948866367340088, 0.04459531232714653, 0.04459531232714653, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.45056793093681335, 0.40904945135116577, 0.5723205208778381, 0.04106782376766205, 0.42105263471603394, 0.043478261679410934, 0.012867040000855923, 0.33683282136917114, 0.048385992646217346, 0.048385992646217346, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.3925012946128845, 0.4053120017051697, 0.6041624546051025, 0.032487500458955765, 0.5263158082962036, 0.0, 0.00017478904919698834, 0.65003901720047, 0.060900986194610596, 0.060900986194610596, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0]]>\n",
      "y_train : [-4.805 -1.959  9.798  0.297 77.505]\n"
     ]
    }
   ],
   "source": [
    "# split = 450000\n",
    "# split = 3\n",
    "# split = int(unparsed_train.shape[0]*0.90)\n",
    "x_train, y_train = ragged_parser(unparsed_train)\n",
    "# x_train, y_train = ragged_parser(unparsed_train[:split])\n",
    "# x_test , y_test  = ragged_parser(unparsed_train[split:split+100])\n",
    "\n",
    "print(\"--==Types==--\")\n",
    "print(\"--x_train:--\")\n",
    "print(\"\\n  -> input_data: x_train[0]\\n  -> type expected: RaggedTensor\\n \"+str(type(x_train[0])))\n",
    "print(\"\\n  -> invCov: x_train[1]\\n  -> type expected: np.array\\n \"+str(type(x_train[1])))\n",
    "print(\"\\n  -> y_train: x_train[2]\\n  -> type expected: np.array\\n \"+str(type(x_train[2])))\n",
    "print(\"\\n--y_train:--\")\n",
    "print(\"\\n  -> y_train: y_train\\n  -> type expected: np.array\\n \"+str(type(y_train)))\n",
    "\n",
    "print(\"\\n\\n--==Shapes==--\")\n",
    "print(\"--x_train:--  \\nnum of events: \" + str(x_train[0].shape[0]))\n",
    "print(\"\\n  RaggedTensor | Input:  shape = \" + \"(\" + str(x_train[0].shape[0]) + \", \"+ str(x_train[0][0].shape[0]) + \", \"+ str(x_train[0][0][0].shape[0]) + \")\")\n",
    "print(\"  np.array     | InvCov: shape = \" + str(x_train[1].shape))\n",
    "print(\"  np.array     | Labels: shape = \" + str(x_train[2].shape))\n",
    "print(\"\\n--x_train:--\")\n",
    "print(\"  np.array     | Labels: shape = \" + str(y_train.shape))\n",
    "\n",
    "print(\"x_train : \" + str(x_train[0][0]))\n",
    "print(\"y_train : \" + str(y_train[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMvnihIl6ail"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdreWH016c08"
   },
   "source": [
    "## Defining Models\n",
    "\n",
    "- model\n",
    "  - Very basic testing RNN model\n",
    "  - Output every timestep\n",
    "\n",
    "- model_timeless\n",
    "  - Very basic testing RNN model\n",
    "  - Output only at the end\n",
    "\n",
    "- RNNTime\n",
    "  - Advanced\n",
    "  - Time distributed, output every timestep\n",
    "\n",
    "- RNNTimeless\n",
    "  - Advanced\n",
    "  - Only output at final layer\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e8qCwtNw6bFk"
   },
   "outputs": [],
   "source": [
    "def model(x):\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='input_lstm1', return_sequences=True)(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(32, activation='relu'), name=\"TD1-Dense\")(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(14, activation='linear'), name=\"output-Dense\")(x)\n",
    "  return x\n",
    "def model_timeless(x):\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='input_lstm1', return_sequences=False)(x)\n",
    "  x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "  x = keras.layers.Dense(5, activation='relu', name=\"output-Dense\")(x)\n",
    "  return x\n",
    "\n",
    "def RNNTime(x):\n",
    "  x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(32,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(32, activation='relu'), name=\"TD1-Dense\")(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(5, activation='linear'), name=\"output-Dense\")(x)\n",
    "  return x\n",
    "\n",
    "def RNNTimeless(x):\n",
    "  x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=False)(x)\n",
    "  x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "  x = keras.layers.Dense(1, activation='linear', name=\"output-Dense\")(x)\n",
    "  # x = keras.layers.Dense(5, activation='linear', name=\"output-Dense\")(x)\n",
    "  # x = keras.layers.lambda(# normalize)\n",
    "  return x\n",
    "\n",
    "\n",
    "# def RNNTimeless(x):\n",
    "#   x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "#   x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "#   x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=False)(x)\n",
    "#   x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "#   x = keras.layers.Dense(5, activation='linear', name=\"output-Dense\")(x)\n",
    "#   # x = keras.layers.lambda(# normalize)\n",
    "#   return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtf37tFTB5HU"
   },
   "source": [
    "## Custom Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOZIj6R8u0VG"
   },
   "source": [
    "### V1 Originial, unedited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v8iA9da3HLBs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_loss\n",
    "\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = x_train[2][0]\n",
    "# y_test = model.predict([x_train[0][0:1],x_train[1][0:1],x_train[2][0:1]])\n",
    "# y_test = np.squeeze(y_test)\n",
    "# inconv_test = x_train[1][0]\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXxfMKTqTvBC"
   },
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZX3nAlhgTu4h"
   },
   "outputs": [],
   "source": [
    "def customMetric(y_true, y_pred, cov, id=0):\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  cov = K.reshape(cov, (batch_size, 5,5)) # cov  shape is now (batch, 5,5)\n",
    "  # cov = tf.transpose(cov, perm=[0,2,1])     # cov shape is now (batch, 5,5)\n",
    "  y_diff = y_pred[:,id] - y_true[:,id]\n",
    "  # y_diff = K.reshape(y_diff, (batch_size,1))\n",
    "  cov = K.reshape(cov[:,id,id], (batch_size,1))\n",
    "  # print(\"diff:\\n\",y_diff)\n",
    "  print(\"cov:\\n\",cov)\n",
    "  # return (y_diff*y_diff)/(cov[:,id,id])\n",
    "  return tf.math.square(y_diff)/(cov)\n",
    "\n",
    "# ccov = x_train[2][0:6]\n",
    "# ccov = np.reshape(ccov, (6,5,5))\n",
    "\n",
    "# print(ccov)\n",
    "\n",
    "# metric = K.eval(customMetric(y_train[0:6],y_train[1:7],x_train[2][0:6],3))\n",
    "# print(\"metric: \\n\",metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qw41A73h-zGB"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZ7d8PMx652m",
    "outputId": "aab903f4-7785-4519-ef41-4067f641d5e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNNModel\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None, 18)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_lstm1 (LSTM)              (None, None, 128)    75264       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, None, 64)     49408       input_lstm1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm3 (LSTM)                    (None, 64)           33024       lstm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Dense1 (Dense)                  (None, 32)           2080        lstm3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output-Dense (Dense)            (None, 1)            33          Dense1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 159,809\n",
      "Trainable params: 159,809\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --==Not in use?==--\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.8)\n",
    "from keras.layers import Dense\n",
    "\n",
    "# nInput = 10\n",
    "nInput = 18\n",
    "\n",
    "# --==Set seed to get identical results==-- begin\n",
    "# from tensorflow.random import set_seed\n",
    "# np.random.seed(1)\n",
    "# set_seed(2)\n",
    "# --==Set seed to get identical results==-- end\n",
    "\n",
    "#--==Set Weights==--\n",
    "# loss_weights = [1/(sd**2)]\n",
    "# loss_weights = np.array(loss_weights)/sum(loss_weights)\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\", loss_weights=loss_weights, metrics=[\"mae\"])\n",
    "\n",
    "inputs = keras.Input((None,nInput))\n",
    "input_true = keras.Input((5,))\n",
    "input_incov = keras.Input((25,))\n",
    "input_cov_f = keras.Input((25,))\n",
    "all_inputs = [inputs, input_incov, input_cov_f, input_true]\n",
    "# all_inputs = [inputs, input_incov, input_true]\n",
    "\n",
    "# --==Choose model==--\n",
    "# x = model(inputs)\n",
    "# x = model_timeless(inputs)\n",
    "# x = RNNTime(inputs)\n",
    "x = RNNTimeless(inputs)\n",
    "# x = RNNTimeStateful(inputs)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# outs = {\n",
    "#     \"q_pt\":Dense(1, name=\"q_pt\")(x),\n",
    "#     \"phi\":Dense(1, name=\"phi\")(x),\n",
    "#     \"tanl\":Dense(1, name=\"tanl\")(x),\n",
    "#     \"D\":Dense(1, name=\"D\")(x),\n",
    "#     \"z\":Dense(1, name=\"z\")(x)\n",
    "# }\n",
    "\n",
    "# y_dict = {\n",
    "#     \"q_pt\":y_train[:,0],\n",
    "#     \"phi\":y_train[:,1],\n",
    "#     \"tanl\":y_train[:,2],\n",
    "#     \"D\":y_train[:,3],\n",
    "#     \"z\":y_train[:,4]\n",
    "# }\n",
    "\n",
    "# model = keras.Model(inputs=all_inputs, outputs=outs, name=\"RNNModel\")\n",
    "\n",
    "model = keras.Model(inputs=all_inputs, outputs=x, name=\"RNNModel\")\n",
    "\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 0),name=\"q_pt\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 1),name=\"phi\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 2),name=\"tanl\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 3),name=\"D\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 4),name=\"z\")\n",
    "\n",
    "# model.add_loss(customLoss(input_true, x, input_incov))\n",
    "# model.compile(loss=None, optimizer=optimizer, metrics=[\"mae\"])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkjx1-6Gr76F"
   },
   "source": [
    "### Custom Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "t2pxxUfH7sjY"
   },
   "outputs": [],
   "source": [
    "def concat_hist(H1,H2):\n",
    "  H = {}\n",
    "  for i in H1.keys():\n",
    "    H[i] = list(np.append(np.array(H1[i]),np.array(H2[i])))\n",
    "  return H\n",
    "H = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjLh8Eh07HRW",
    "outputId": "e8312305-0acd-4240-c575-5b48161d1655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 6.2043 - mae: 1.6964\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 6.2773 - mae: 1.7157\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 6.0458 - mae: 1.6722\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 5.8509 - mae: 1.6296\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 5.7758 - mae: 1.6219\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 5.8604 - mae: 1.6366\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 5.7082 - mae: 1.6239\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 5.6342 - mae: 1.6197\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 5.8594 - mae: 1.6443\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 5.6952 - mae: 1.6238\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 5.9511 - mae: 1.6329\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 6.1302 - mae: 1.6528\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 5.9804 - mae: 1.6460\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 5.9233 - mae: 1.6586\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 5.6950 - mae: 1.6310\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 5.5988 - mae: 1.6231\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 5.2805 - mae: 1.5638\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 5.8675 - mae: 1.6483\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 6.1175 - mae: 1.6807\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 5.9244 - mae: 1.6409\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 5.8845 - mae: 1.6383\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 5.9068 - mae: 1.6516\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 5.7227 - mae: 1.6217\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 5.6742 - mae: 1.6165\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 6.6932 - mae: 1.7056\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 7.1905 - mae: 1.8220\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 6.8996 - mae: 1.7963\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 6.1892 - mae: 1.6974\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 5.6138 - mae: 1.6236\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 5.3384 - mae: 1.5944\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 5.0474 - mae: 1.5495\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 5.2485 - mae: 1.5790\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 6.0580 - mae: 1.6541\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 6.3350 - mae: 1.7042\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 5.9735 - mae: 1.6577\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 5.9275 - mae: 1.6648\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 5.7426 - mae: 1.6387\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 5.9193 - mae: 1.6652\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 6.4254 - mae: 1.6824\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 10.8477 - mae: 2.0429\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 10.9104 - mae: 2.1182\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 8.2162 - mae: 1.8971\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 6.7976 - mae: 1.7476\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 6.1673 - mae: 1.6901\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 5.6256 - mae: 1.6215\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 5.4802 - mae: 1.5723\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 4.6624 - mae: 1.4774\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 4.3795 - mae: 1.4367\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 4.2963 - mae: 1.4300\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 3.9345 - mae: 1.3557\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 3.8492 - mae: 1.3519\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 3.9169 - mae: 1.3564\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 3.8669 - mae: 1.3444\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 3.8605 - mae: 1.3504\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 3.8059 - mae: 1.3441\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 3.8306 - mae: 1.3463\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 3.8060 - mae: 1.3391\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 3.8049 - mae: 1.3392\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 3.7672 - mae: 1.3384\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 3.7753 - mae: 1.3455\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 3.7454 - mae: 1.3230\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 3.8021 - mae: 1.3361\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 4.1374 - mae: 1.3963\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 4.3851 - mae: 1.4221\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 4.8773 - mae: 1.4910\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 4.9356 - mae: 1.5225\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 5.7973 - mae: 1.6339\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 5.6034 - mae: 1.6225\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 5.5734 - mae: 1.6061\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 5.5255 - mae: 1.6066\n",
      "Epoch 71/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 6.3199 - mae: 1.6836\n",
      "Epoch 72/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 12.2883 - mae: 2.0413\n",
      "Epoch 73/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 131.0969 - mae: 5.1156\n",
      "Epoch 74/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 204.9948 - mae: 6.0438\n",
      "Epoch 75/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 137.2863 - mae: 5.3079\n",
      "Epoch 76/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 128.2072 - mae: 4.9708\n",
      "Epoch 77/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 204.3921 - mae: 5.1580\n",
      "Epoch 78/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 156.1233 - mae: 4.9900\n",
      "Epoch 79/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 95.2416 - mae: 4.5894\n",
      "Epoch 80/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 61.5805 - mae: 3.8902\n",
      "Epoch 81/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 43.9391 - mae: 3.4866\n",
      "Epoch 82/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 37.7624 - mae: 3.2722\n",
      "Epoch 83/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 27.2045 - mae: 2.9970\n",
      "Epoch 84/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 22.8414 - mae: 2.7584\n",
      "Epoch 85/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 18.9484 - mae: 2.6232\n",
      "Epoch 86/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 16.1886 - mae: 2.4459\n",
      "Epoch 87/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 14.3874 - mae: 2.3319\n",
      "Epoch 88/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 13.2089 - mae: 2.2837\n",
      "Epoch 89/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 11.9186 - mae: 2.1652\n",
      "Epoch 90/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 11.0788 - mae: 2.1049\n",
      "Epoch 91/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 10.1890 - mae: 2.0307\n",
      "Epoch 92/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 9.3956 - mae: 1.9679\n",
      "Epoch 93/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 8.9019 - mae: 1.9249\n",
      "Epoch 94/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 8.2761 - mae: 1.8762\n",
      "Epoch 95/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 7.6243 - mae: 1.8004\n",
      "Epoch 96/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 7.2900 - mae: 1.7763\n",
      "Epoch 97/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 6.9095 - mae: 1.7449\n",
      "Epoch 98/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 6.5388 - mae: 1.7043\n",
      "Epoch 99/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 6.2915 - mae: 1.6766\n",
      "Epoch 100/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 6.0206 - mae: 1.6413\n",
      "Epoch 101/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 5.7993 - mae: 1.6202\n",
      "Epoch 102/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 5.6415 - mae: 1.6001\n",
      "Epoch 103/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 5.3322 - mae: 1.5621\n",
      "Epoch 104/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 5.2574 - mae: 1.5453\n",
      "Epoch 105/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 5.3165 - mae: 1.5498\n",
      "Epoch 106/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 5.3597 - mae: 1.5717\n",
      "Epoch 107/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 4.9032 - mae: 1.5091\n",
      "Epoch 108/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 4.6986 - mae: 1.4811\n",
      "Epoch 109/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 4.4752 - mae: 1.4452\n",
      "Epoch 110/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 4.3857 - mae: 1.4312\n",
      "Epoch 111/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 4.2799 - mae: 1.4138\n",
      "Epoch 112/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 4.1940 - mae: 1.3978\n",
      "Epoch 113/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 4.1377 - mae: 1.3921\n",
      "Epoch 114/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 4.1585 - mae: 1.3894\n",
      "Epoch 115/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 4.4512 - mae: 1.4028\n",
      "Epoch 116/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 4.7334 - mae: 1.4713\n",
      "Epoch 117/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 4.8895 - mae: 1.4835\n",
      "Epoch 118/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 4.3812 - mae: 1.4488\n",
      "Epoch 119/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 4.0216 - mae: 1.3889\n",
      "Epoch 120/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 3.7843 - mae: 1.3414\n",
      "Epoch 121/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 3.7242 - mae: 1.3327\n",
      "Epoch 122/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 3.7204 - mae: 1.3390\n",
      "Epoch 123/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 3.7696 - mae: 1.3437\n",
      "Epoch 124/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 3.8677 - mae: 1.3558\n",
      "Epoch 125/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 3.7526 - mae: 1.3253\n",
      "Epoch 126/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 3.9374 - mae: 1.3560\n",
      "Epoch 127/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 3.9564 - mae: 1.3424\n",
      "Epoch 128/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 3.9809 - mae: 1.3670\n",
      "Epoch 129/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 4.0818 - mae: 1.3775\n",
      "Epoch 130/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 3.9797 - mae: 1.3673\n",
      "Epoch 131/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 3.8891 - mae: 1.3608\n",
      "Epoch 132/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 3.5753 - mae: 1.2976\n",
      "Epoch 133/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 3.7413 - mae: 1.3340\n",
      "Epoch 134/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.7362 - mae: 1.3292\n",
      "Epoch 135/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 3.4610 - mae: 1.2898\n",
      "Epoch 136/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 3.2685 - mae: 1.2473\n",
      "Epoch 137/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 3.2552 - mae: 1.2509\n",
      "Epoch 138/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 3.1042 - mae: 1.2173\n",
      "Epoch 139/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 3.1534 - mae: 1.2318\n",
      "Epoch 140/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 3.1628 - mae: 1.2368\n",
      "Epoch 141/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 3.4093 - mae: 1.2641\n",
      "Epoch 142/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 3.2933 - mae: 1.2531\n",
      "Epoch 143/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 3.4375 - mae: 1.2861\n",
      "Epoch 144/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 3.4648 - mae: 1.2809\n",
      "Epoch 145/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 3.3978 - mae: 1.2676\n",
      "Epoch 146/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.5177 - mae: 1.2873\n",
      "Epoch 147/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 3.7188 - mae: 1.3109\n",
      "Epoch 148/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 3.8690 - mae: 1.3429\n",
      "Epoch 149/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 3.6759 - mae: 1.3316\n",
      "Epoch 150/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 3.7622 - mae: 1.3162\n",
      "Epoch 151/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 4.4603 - mae: 1.4145\n",
      "Epoch 152/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 4.2697 - mae: 1.4023\n",
      "Epoch 153/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 4.3340 - mae: 1.4180\n",
      "Epoch 154/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 3.9013 - mae: 1.3543\n",
      "Epoch 155/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.4922 - mae: 1.2995\n",
      "Epoch 156/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 3.1950 - mae: 1.2530\n",
      "Epoch 157/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 3.0682 - mae: 1.2160\n",
      "Epoch 158/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 3.0788 - mae: 1.2179\n",
      "Epoch 159/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 2.8651 - mae: 1.1764\n",
      "Epoch 160/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 2.9447 - mae: 1.2105\n",
      "Epoch 161/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 2.8186 - mae: 1.1720\n",
      "Epoch 162/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 2.7718 - mae: 1.1544\n",
      "Epoch 163/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 2.7289 - mae: 1.1453\n",
      "Epoch 164/300\n",
      "37/37 [==============================] - 3s 79ms/step - loss: 2.6583 - mae: 1.1246\n",
      "Epoch 165/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 2.6988 - mae: 1.1441\n",
      "Epoch 166/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 3.2182 - mae: 1.2196\n",
      "Epoch 167/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.2477 - mae: 1.2175\n",
      "Epoch 168/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 3.4601 - mae: 1.2526\n",
      "Epoch 169/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.3147 - mae: 1.2457\n",
      "Epoch 170/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 3.3243 - mae: 1.2548\n",
      "Epoch 171/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 3.5642 - mae: 1.3022\n",
      "Epoch 172/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 3.6609 - mae: 1.3131\n",
      "Epoch 173/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 3.9689 - mae: 1.3465\n",
      "Epoch 174/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 3.5454 - mae: 1.2928\n",
      "Epoch 175/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.2166 - mae: 1.2381\n",
      "Epoch 176/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 3.4556 - mae: 1.2740\n",
      "Epoch 177/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 3.5212 - mae: 1.3025\n",
      "Epoch 178/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 3.3823 - mae: 1.2757\n",
      "Epoch 179/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 3.5020 - mae: 1.3099\n",
      "Epoch 180/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 3.1008 - mae: 1.2360\n",
      "Epoch 181/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 2.7550 - mae: 1.1667\n",
      "Epoch 182/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 2.5668 - mae: 1.1261\n",
      "Epoch 183/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 2.6404 - mae: 1.1366\n",
      "Epoch 184/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 2.5245 - mae: 1.1137\n",
      "Epoch 185/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 2.5058 - mae: 1.1177\n",
      "Epoch 186/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 2.4469 - mae: 1.0942\n",
      "Epoch 187/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 2.4998 - mae: 1.1058\n",
      "Epoch 188/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 2.9695 - mae: 1.2109\n",
      "Epoch 189/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 3.1361 - mae: 1.2352\n",
      "Epoch 190/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 3.3880 - mae: 1.3002\n",
      "Epoch 191/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 3.4722 - mae: 1.2606\n",
      "Epoch 192/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 3.5360 - mae: 1.2972\n",
      "Epoch 193/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.5490 - mae: 1.3031\n",
      "Epoch 194/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 3.3661 - mae: 1.2791\n",
      "Epoch 195/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 2.8946 - mae: 1.1929\n",
      "Epoch 196/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 2.9983 - mae: 1.2121\n",
      "Epoch 197/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 2.7241 - mae: 1.1488\n",
      "Epoch 198/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 3.3066 - mae: 1.2553\n",
      "Epoch 199/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 3.0746 - mae: 1.2172\n",
      "Epoch 200/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 2.9611 - mae: 1.1903\n",
      "Epoch 201/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.1122 - mae: 1.2100\n",
      "Epoch 202/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 3.0721 - mae: 1.2220\n",
      "Epoch 203/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 3.1891 - mae: 1.2440\n",
      "Epoch 204/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 3.4140 - mae: 1.2648\n",
      "Epoch 205/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 3.4451 - mae: 1.3023\n",
      "Epoch 206/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 3.4622 - mae: 1.3057\n",
      "Epoch 207/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 3.0047 - mae: 1.2161\n",
      "Epoch 208/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 2.9590 - mae: 1.1989\n",
      "Epoch 209/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 2.6798 - mae: 1.1451\n",
      "Epoch 210/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 2.6797 - mae: 1.1573\n",
      "Epoch 211/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 2.6220 - mae: 1.1534\n",
      "Epoch 212/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 2.9651 - mae: 1.2052\n",
      "Epoch 213/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 2.9019 - mae: 1.1936\n",
      "Epoch 214/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 2.8184 - mae: 1.1929\n",
      "Epoch 215/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 2.5689 - mae: 1.1194\n",
      "Epoch 216/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 2.5295 - mae: 1.1195\n",
      "Epoch 217/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 2.4475 - mae: 1.1157\n",
      "Epoch 218/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 2.5502 - mae: 1.1245\n",
      "Epoch 219/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 2.4807 - mae: 1.1249\n",
      "Epoch 220/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 2.4804 - mae: 1.1091\n",
      "Epoch 221/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 2.5463 - mae: 1.1208\n",
      "Epoch 222/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 2.4861 - mae: 1.1105\n",
      "Epoch 223/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 2.5603 - mae: 1.1156\n",
      "Epoch 224/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 2.6316 - mae: 1.1350\n",
      "Epoch 225/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 3.2835 - mae: 1.2608\n",
      "Epoch 226/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 3.4089 - mae: 1.2631\n",
      "Epoch 227/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 3.1773 - mae: 1.2275\n",
      "Epoch 228/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 5.8266 - mae: 1.5754\n",
      "Epoch 229/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 12.3712 - mae: 2.1787\n",
      "Epoch 230/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 26.4662 - mae: 2.7608\n",
      "Epoch 231/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 129.3075 - mae: 5.4511\n",
      "Epoch 232/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 267.0833 - mae: 6.4692\n",
      "Epoch 233/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 242.5588 - mae: 6.0430\n",
      "Epoch 234/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 213.6864 - mae: 5.4700\n",
      "Epoch 235/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 163.7817 - mae: 5.5132\n",
      "Epoch 236/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 149.8804 - mae: 5.1686\n",
      "Epoch 237/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 148.9306 - mae: 5.0088\n",
      "Epoch 238/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 65.5221 - mae: 4.1652\n",
      "Epoch 239/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 43.5156 - mae: 3.6285\n",
      "Epoch 240/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 34.1535 - mae: 3.3622\n",
      "Epoch 241/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 27.3371 - mae: 3.0619\n",
      "Epoch 242/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 22.3711 - mae: 2.8501\n",
      "Epoch 243/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 18.0218 - mae: 2.6223\n",
      "Epoch 244/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 15.2989 - mae: 2.4713\n",
      "Epoch 245/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 12.8429 - mae: 2.3401\n",
      "Epoch 246/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 10.8866 - mae: 2.1820\n",
      "Epoch 247/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 9.4378 - mae: 2.0535\n",
      "Epoch 248/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 8.2960 - mae: 1.9509\n",
      "Epoch 249/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 7.3626 - mae: 1.8518\n",
      "Epoch 250/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 6.6676 - mae: 1.7764\n",
      "Epoch 251/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 6.1423 - mae: 1.7086\n",
      "Epoch 252/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 5.6254 - mae: 1.6518\n",
      "Epoch 253/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 5.2639 - mae: 1.6090\n",
      "Epoch 254/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 4.8753 - mae: 1.5553\n",
      "Epoch 255/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 4.5432 - mae: 1.5098\n",
      "Epoch 256/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 4.2746 - mae: 1.4676\n",
      "Epoch 257/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 4.0074 - mae: 1.4269\n",
      "Epoch 258/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.8406 - mae: 1.4037\n",
      "Epoch 259/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.6530 - mae: 1.3740\n",
      "Epoch 260/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 3.4528 - mae: 1.3341\n",
      "Epoch 261/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 3.2717 - mae: 1.2984\n",
      "Epoch 262/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 3.1953 - mae: 1.2908\n",
      "Epoch 263/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 3.0645 - mae: 1.2571\n",
      "Epoch 264/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 2.9773 - mae: 1.2434\n",
      "Epoch 265/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 2.8805 - mae: 1.2181\n",
      "Epoch 266/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 2.8614 - mae: 1.2216\n",
      "Epoch 267/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 2.7271 - mae: 1.1835\n",
      "Epoch 268/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 2.7238 - mae: 1.1761\n",
      "Epoch 269/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 2.5616 - mae: 1.1545\n",
      "Epoch 270/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 2.5617 - mae: 1.1500\n",
      "Epoch 271/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 2.4894 - mae: 1.1392\n",
      "Epoch 272/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 2.3385 - mae: 1.1035\n",
      "Epoch 273/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 2.2682 - mae: 1.0881\n",
      "Epoch 274/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 2.2724 - mae: 1.0938\n",
      "Epoch 275/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 2.2360 - mae: 1.0783\n",
      "Epoch 276/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 2.2450 - mae: 1.0703\n",
      "Epoch 277/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 2.1827 - mae: 1.0538\n",
      "Epoch 278/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 2.1467 - mae: 1.0532\n",
      "Epoch 279/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 2.0412 - mae: 1.0357\n",
      "Epoch 280/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 1.9490 - mae: 1.0075\n",
      "Epoch 281/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 1.9288 - mae: 1.0006\n",
      "Epoch 282/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 1.9293 - mae: 1.0024\n",
      "Epoch 283/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 1.9025 - mae: 0.9907\n",
      "Epoch 284/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 1.8612 - mae: 0.9705\n",
      "Epoch 285/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 1.8821 - mae: 0.9728\n",
      "Epoch 286/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 1.9040 - mae: 0.9812\n",
      "Epoch 287/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 1.9735 - mae: 0.9902\n",
      "Epoch 288/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 2.0009 - mae: 0.9860\n",
      "Epoch 289/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.6297 - mae: 1.2351\n",
      "Epoch 290/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 2.7750 - mae: 1.1482\n",
      "Epoch 291/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 2.4253 - mae: 1.0883\n",
      "Epoch 292/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 1.9550 - mae: 1.0037\n",
      "Epoch 293/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1.8627 - mae: 0.9761\n",
      "Epoch 294/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 1.7122 - mae: 0.9369\n",
      "Epoch 295/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 1.6707 - mae: 0.9279\n",
      "Epoch 296/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 1.5979 - mae: 0.9125\n",
      "Epoch 297/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 1.5333 - mae: 0.8904\n",
      "Epoch 298/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 1.5115 - mae: 0.8871\n",
      "Epoch 299/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 1.4591 - mae: 0.8705\n",
      "Epoch 300/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 1.4705 - mae: 0.8733\n"
     ]
    }
   ],
   "source": [
    "H_t = []\n",
    "index = 2\n",
    "\n",
    "# Using custom loss and gen\n",
    "es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01, patience=25, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=300, validation_data=(x_test, y_test), verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=300, shuffle=True, verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train[:,2], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es])\n",
    "# H_t.append(model.fit(x=x_train, y=y_train[:,2], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es]).history)\n",
    "H_t.append(model.fit(x=x_train, y=y_train[:,index], batch_size=256, epochs=300, verbose=1, shuffle=True, callback=[es]).history)\n",
    "if H == None:\n",
    "  H = H_t[-1]\n",
    "else:\n",
    "  H = concat_hist(H,H_t[-1])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=100, validation_data=test_gen, validation_steps=50, validation_batch_size=32, verbose=1)\n",
    "\n",
    "# Example how it kind of looks like\n",
    "# H = model.fit(x=[x_train, invCov, y_train], y=y_train, batch_size=64, epochs=100, verbose=1)\n",
    "\n",
    "# Overfit\n",
    "# es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=100, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=1, epochs=100, verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=1, epochs=100, verbose=1, validation_data=(x_test,y_test), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ5XBwDV7MGY"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 1\n",
    "final_loss = round(H[\"loss\"][-1],2)\n",
    "date = \"03-03\"\n",
    "def gen_name(m_type):\n",
    "    global model_index, index, date, final_loss\n",
    "    variable = [\"q_pt\",\"phi\",\"tanl\",\"D\",\"z\"][index]\n",
    "    m_str = \"models/\" + str(date) + \"-2021_\" + str(variable) + \"-\" + str(model_index) + \"_loss=\" + str(final_loss) + \".\" + str(m_type)\n",
    "    model_index+=1 # create a check for if file exists, for now just increment\n",
    "    return m_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "xWvST6o27MYI"
   },
   "outputs": [],
   "source": [
    "# model.save('model.h5', save_format=\"h5\")\n",
    "# TODO check if file exists, increment counter\n",
    "# model.save('drive/MyDrive/Models/RealRNN_1-3-2021_141Ep_Onlytanl-2.h5', save_format=\"h5\")\n",
    "model.save(gen_name(\"h5\",date,final_loss), save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6LGBSia7fg3"
   },
   "source": [
    "## Graph loss and mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "OUPAStLMtq7m",
    "outputId": "d4bada3c-6f91-4a9c-b6d7-d7abe8cc1689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(H.keys())\n",
    "# print(\"loss: \", H[\"loss\"])\n",
    "# print(\"mae: \", H[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 0\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "ax[0].plot(H[\"loss\"][lim:])\n",
    "# ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "ax[0].legend([\"train\",\"val\"])\n",
    "ax[0].grid(True)\n",
    "\n",
    "\n",
    "ax[1].plot(H[\"mae\"][lim:])\n",
    "# ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "ax[1].legend([\"train\",\"val\"])\n",
    "ax[1].grid(True)\n",
    "\n",
    "model_index -= 1\n",
    "plt.savefig(gen_name(\"png\",date,final_loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_array = np.array(list(H.items()))\n",
    "save_array = np.array([an_array[0][1],an_array[1][1]])\n",
    "model_index -= 1\n",
    "np.save(gen_name(\"npy\",date,final_loss),save_array,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw4xzpuKYEld"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZHLFMS1YE40",
    "outputId": "09e30fe4-cc1b-444f-83b3-c74d1b923762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aay:   9.798481516481372\n",
      "pred:  9.878983\n",
      "\n",
      "aay:   19.78603099301803\n",
      "pred:  21.045963\n",
      "\n",
      "aay:   1.919117959169239\n",
      "pred:  1.5047722\n",
      "\n",
      "aay:   7.587641081821801\n",
      "pred:  9.573143\n",
      "\n",
      "aay:   26.422719761303856\n",
      "pred:  28.726074\n",
      "\n",
      "aay:   20.99114845743568\n",
      "pred:  20.24589\n",
      "\n",
      "aay:   6.70415530333724\n",
      "pred:  5.4513245\n",
      "\n",
      "aay:   6.379456165241193\n",
      "pred:  7.418966\n",
      "\n",
      "aay:   14.103035290726634\n",
      "pred:  13.159975\n",
      "\n",
      "aay:   80.67714491393993\n",
      "pred:  82.088356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True, precision=3)\n",
    "from time import sleep\n",
    "\n",
    "for i in range(10):\n",
    "  aax = x_train[0][i:i+1]\n",
    "  oax = x_train[1][i:i+1]\n",
    "  cax = x_train[2][i:i+1]\n",
    "  yax = x_train[3][i:i+1]\n",
    "  aay = y_train[i][index]\n",
    "  pred = model.predict([aax, oax, cax, yax])[0][0]\n",
    "  diff = pred - aay\n",
    "  # print(\"i: \",i)\n",
    "  # print(\"aax:  \",aax[0,0])\n",
    "  print(\"aay:  \",aay)\n",
    "  print(\"pred: \",pred)\n",
    "  # print(\"diff: \",diff)\n",
    "  print(\"\")\n",
    "\n",
    "  # plt.plot(aay[0])\n",
    "  # plt.plot(pred[0])\n",
    "  # plt.show()\n",
    "# [Q/PT, phi, tanl, D, z]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T56J0X6g7O2k"
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgLQh0jf7Ova"
   },
   "outputs": [],
   "source": [
    "# model.save('drive/MyDrive/RealRNN_1-2-2021_2.h5', save_format=\"h5\")\n",
    "model = keras.models.load_model('drive/MyDrive/Models/RealRNN_1-3-2021_300Ep_Onlyphi.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X72YH9S87cvW"
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H.keys())\n",
    "# print(\"loss: \", H[\"loss\"])\n",
    "# print(\"mae: \", H[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 0\n",
    "\n",
    "if len(H.keys()) > 4:\n",
    "  # fig, ax = plt.subplots(4,1,figsize=(5,20))\n",
    "  fig, ax = plt.subplots(3,1,figsize=(5,15))\n",
    "  fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "  ax[0].plot(H[\"loss\"][lim:])\n",
    "  # ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "  ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "  ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "  # ax[0].set_yscale(\"log\")\n",
    "  ax[0].legend([\"train\",\"val\"])\n",
    "  ax[0].grid(True)\n",
    "\n",
    "\n",
    "  ax[1].plot(H[\"mae\"][lim:])\n",
    "  # ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "  ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "  ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "  ax[1].legend([\"train\",\"val\"])\n",
    "  ax[1].grid(True)\n",
    "\n",
    "  ax[2].plot(H[\"q_pt\"][lim:])\n",
    "  ax[2].plot(H[\"phi\"][lim:])\n",
    "  ax[2].plot(H[\"tanl\"][lim:])\n",
    "  ax[2].plot(H[\"D\"][lim:])\n",
    "  ax[2].plot(H[\"z\"][lim:])\n",
    "  ax[2].set_title(\"data vs epoch\", fontsize=20)\n",
    "  ax[2].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[2].set_ylabel(\"data\", fontsize=15)\n",
    "  ax[2].legend([\"q_pt\",\"phi\",\"tanl\",\"D\",\"z\"])\n",
    "  # ax[2].legend([\"phi\",\"tanl\",\"D\",\"z\"])\n",
    "  # ax[2].legend([\"phi\",\"D\",\"z\"])\n",
    "  ax[2].grid(True)\n",
    "\n",
    "  # ax[3].plot(H.history[\"val_q_pt\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_phi\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_tanl\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_D\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_z\"][lim:])\n",
    "  # ax[3].set_title(\"data vs epoch\", fontsize=20)\n",
    "  # ax[3].set_xlabel(\"epoch\", fontsize=15)\n",
    "  # ax[3].set_ylabel(\"data\", fontsize=15)\n",
    "  # ax[3].legend([\"val_q_pt\",\"val_phi\",\"val_tanl\",\"val_D\",\"val_z\"])\n",
    "  # # ax[3].legend([\"val_phi\",\"val_tanl\",\"val_D\",\"val_z\"])\n",
    "  # # ax[3].legend([\"val_phi\",\"val_D\",\"val_z\"])\n",
    "  # ax[3].grid(True)\n",
    "\n",
    "else:\n",
    "  fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "  fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "  ax[0].plot(H[\"loss\"][lim:])\n",
    "  # ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "  ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "  ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "  # ax[0].set_yscale(\"log\")\n",
    "  ax[0].legend([\"train\",\"val\"])\n",
    "  ax[0].grid(True)\n",
    "\n",
    "\n",
    "  ax[1].plot(H[\"mae\"][lim:])\n",
    "  # ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "  ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "  ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "  ax[1].legend([\"train\",\"val\"])\n",
    "  ax[1].grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "k6XSgSit7dE3",
    "outputId": "b3a1caaf-1b21-4348-bfe7-1315ccb39171"
   },
   "outputs": [],
   "source": [
    "print(H.history.keys())\n",
    "print(\"loss: \", H.history[\"loss\"])\n",
    "print(\"mae: \", H.history[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 2\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "ax[0].plot(H.history[\"loss\"][lim:])\n",
    "# ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[0].legend([\"train\",\"val\"])\n",
    "ax[0].grid(True)\n",
    "\n",
    "\n",
    "ax[1].plot(H.history[\"mae\"][lim:])\n",
    "# ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "ax[1].legend([\"train\",\"val\"])\n",
    "ax[1].grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOi6MC8u7swr"
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqqb7Riq7tAY"
   },
   "outputs": [],
   "source": [
    "# Maybe copy over previous function and edit that?\n",
    "def graph(pred, true, diff):\n",
    "\n",
    "  values = [\"u\",\"v\",\"sin(v)\",\"cos(v)\",\"sin(u)\",\"cos(u)\",\"s\",\"ds\",\"wire\",\"glayer\",\"z\",\"time\",\"dE_amp\",\"q\"]\n",
    "  limits = [[\"todo\"]]\n",
    "\n",
    "  size = len(values)\n",
    "\n",
    "  fig, axs = plt.subplots(4,size,figsize=(size*5,20))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "\n",
    "  for i in range(size):\n",
    "    (mu, sigma) = norm.fit(diff[:,i])\n",
    "    print(\"data\" , values[i] ,\" |: mu: \", mu, \"sigma: \" , sigma)\n",
    "    _, bins, _ = axs[0,i].hist(diff[:,i], 20, density=True)\n",
    "    y = norm.pdf(bins, mu, sigma)\n",
    "    l = axs[0,i].plot(bins, y, 'r--', linewidth=2)\n",
    "\n",
    "    axs[0,i].set_title(values[i] + ' diff')\n",
    "    axs[0,i].set_ylabel('freq')\n",
    "    axs[0,i].set_xlabel(values[i] + ' diff')\n",
    "\n",
    "  #--------------------------------------\n",
    "  # PREDICTED VS TRUE\n",
    "  #--------------------------------------\n",
    "    \n",
    "  for i in range(size):\n",
    "    axs[1,i].scatter(true[:,i],pred[:,i])\n",
    "    axs[1,i].grid(True)\n",
    "\n",
    "    axs[1,i].set_title(values[i] + ' (predicted vs true)')\n",
    "    axs[1,i].set_ylabel('pred ' + values[i])\n",
    "    axs[1,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "    # axs[1,i].set_xlim(limits[i])\n",
    "    # axs[1,i].set_ylim(limits[i])\n",
    "    # axs[1,i].plot(limits[i],limits[i], color='b')\n",
    "\n",
    "  #--------------------------------------\n",
    "  # DIFFERENCE VS TRUE\n",
    "  #--------------------------------------\n",
    "\n",
    "  for i in range(size):\n",
    "    axs[2,i].scatter(true[:,i],diff[:,i])\n",
    "    l, r = axs[2,i].get_xlim()\n",
    "    axs[2,i].hlines(0, l, r)\n",
    "    axs[2,i].grid(True)\n",
    "\n",
    "    axs[2,i].set_title(values[i] + ' (difference vs true)')\n",
    "    axs[2,i].set_ylabel('diff ' + values[i])\n",
    "    axs[2,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "  #--------------------------------------\n",
    "  # DIFFERENCE VS TRUE 2D HIST\n",
    "  #--------------------------------------\n",
    "\n",
    "  for i in range(size):\n",
    "    axs[3,i].hist2d(true[:,i],diff[:,i],bins=20)\n",
    "\n",
    "    axs[2,i].set_title(values[i] + ' (difference vs true)')\n",
    "    axs[2,i].set_ylabel('diff ' + values[i])\n",
    "    axs[2,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "  fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHuUmXPCiXt9"
   },
   "outputs": [],
   "source": [
    "def gen_test_data(x_test, y_test, size=1000):\n",
    "  pred = model.predict(x_test)\n",
    "  diff = pred - y_test\n",
    "  return pred, y_test, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AMRhD6Wi7Xh"
   },
   "outputs": [],
   "source": [
    "graph(gen_test_data(x_test, y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAUyMneo7Xj1"
   },
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBsHjTgG7X2y"
   },
   "outputs": [],
   "source": [
    "# make test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWkYEQvFZ7HC"
   },
   "source": [
    "# Verification of proper data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqKxKcOQYEO4"
   },
   "source": [
    "## Using Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml3HHzoEaAhr"
   },
   "outputs": [],
   "source": [
    "aax, aay = next(train_gen)\n",
    "print(aax.shape)\n",
    "print(aay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv72llYMaUYd"
   },
   "outputs": [],
   "source": [
    "print(\"x\",aax[0])\n",
    "print(\"y\",aay[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcwDKKLLe079"
   },
   "source": [
    "## Non Genenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlU3tRGpYxQV",
    "outputId": "a24ded2d-d1d0-4555-84a9-428edf5b2e7d"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  aax = x_train[i]\n",
    "  aay = y_train[i]\n",
    "  # print(aax.shape)\n",
    "  # print(aay.shape)\n",
    "  # print(\"x\",aax)\n",
    "  print(\"y\",aay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCpP2wsfdeGl"
   },
   "source": [
    "## Graphs of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAxLbw94GYzm"
   },
   "source": [
    "### filter_ignore\n",
    "\n",
    "Filters out large and small values and graphs them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiUKfbtfAoLm"
   },
   "outputs": [],
   "source": [
    "def filter_ignore(var,min=None,max=None,bins=25,ylog=False,xlog=False,cut=True):\n",
    "  list_ignore = []\n",
    "\n",
    "  print(\"--== {} ==--\\n\".format(var))\n",
    "\n",
    "  largest = 0\n",
    "  smallest = 0\n",
    "  for i in range(len(csv_train[var])):\n",
    "    if csv_train[var][i] > csv_train[var][largest]:\n",
    "      largest = i\n",
    "    if csv_train[var][i] < csv_train[var][smallest]:\n",
    "      smallest = i\n",
    "  print(\"largest value:  ({}, {:.3f})\".format(largest,csv_train[var][largest]))\n",
    "  print(\"smallest value: ({}, {:.3f})\".format(smallest,csv_train[var][smallest]))\n",
    "\n",
    "  print(\"\")\n",
    "\n",
    "  if min:\n",
    "    for i in range(len(csv_train[var])):\n",
    "      if csv_train[var][i] < min:\n",
    "        list_ignore.append(i)\n",
    "    print(\"min IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "  if max:\n",
    "    for i in range(len(csv_train[var])):\n",
    "      if csv_train[var][i] > max:\n",
    "        list_ignore.append(i)\n",
    "    print(\"max IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "  if min and max:\n",
    "    print(\"total IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "    plt.hist(csv_train[var],range=[min,max],bins=bins)\n",
    "  elif min:\n",
    "    plt.hist(csv_train[var],range=[min,csv_train[var][largest]],bins=bins)\n",
    "  elif max:\n",
    "    plt.hist(csv_train[var],range=[csv_train[var][smallest],max],bins=bins)\n",
    "  else:\n",
    "    plt.hist(csv_train[var],bins=bins)\n",
    "  \n",
    "  plt.title(var)\n",
    "  if cut:\n",
    "    plt.xlim(left=min,right=max)\n",
    "  if ylog:\n",
    "    plt.yscale(\"log\")\n",
    "  if xlog:\n",
    "    plt.xscale(\"log\")\n",
    "  plt.show()\n",
    "  return list_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "BwsJ0YIoBiew",
    "outputId": "962724f0-0845-4ab5-8b27-043de6744e44"
   },
   "outputs": [],
   "source": [
    "filter_ignore(\"q_over_pt\",min=-4000,bins=30,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "2Wyjq07YGjBF",
    "outputId": "9a82db49-7e7e-49aa-8d4b-a0da738ec1f5"
   },
   "outputs": [],
   "source": [
    "filter_ignore(\"tanl\",max=1000,bins=25,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xauTTOyrHPyT"
   },
   "outputs": [],
   "source": [
    "rms_ignore = filter_ignore(\"rms\",max=0.1,bins=25,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "ERa-RZDbN6Hm",
    "outputId": "ac4577c9-f719-47d4-bc16-8fdbb7e413a5"
   },
   "outputs": [],
   "source": [
    "# 'q_over_pt', 'phi', 'tanl', 'D', 'z'\n",
    "# filter_ignore(\"D\", min=-200, ylog=True,bins=25)\n",
    "filter_ignore(\"z\",bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBO9wKCH2ePH"
   },
   "outputs": [],
   "source": [
    "csv_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc3WNfPaYwaL"
   },
   "source": [
    "### 1D Hist of all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "2VqlQxZRgTB7",
    "outputId": "2a5ce7d3-1665-4d71-80f5-5b7ed7ceb673"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"phi\"],bins=50) # -3 to 3, even distrib\n",
    "plt.title(\"phi\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"D\"],range=[-3000,80],bins=25) # -3000 to 50, but val in 65\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"D\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"z\"],bins=100)\n",
    "plt.title(\"z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TbqYK55L05mB",
    "outputId": "81dc2a8d-b572-4505-983a-8958a9bea8b6"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "# fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "plt.hist(csv_train[\"cov_00\"],range=[0,1e8],bins=25) # 0 to 1e13\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"cov_00\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"cov_01\"],bins=25) # -1e6 to over 1e5\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"cov_01\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"chisq\"],bins=25) # 0 to 200\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"chisq\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"Ndof\"],range=[0,44],bins=45) # ? this one weird 0 to ~43\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Ndof\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"rms\"],range=[0,0.1],bins=25) # \n",
    "# plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"rms\")\n",
    "plt.show()\n",
    "# ---\n",
    "# plt.hist(csv_train[\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "plt.hist(csv_train[csv_train[\"t_start_cntr_valid\"] == 1][\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "plt.title(\"t_start_cntr\")\n",
    "plt.show()\n",
    "\n",
    "# plt.hist(csv_train[\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "plt.hist(csv_train[csv_train[\"t_tof_valid\"] == 1][\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "plt.title(\"t_tof\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_bcal\"],bins=25) # ~-22 to 20\n",
    "plt.title(\"t_bcal\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_fcal\"],bins=25) # ~-100 to ~75\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"t_fcal\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"t_start_cntr_valid\"],bins=25) # a lot more 0s\n",
    "plt.title(\"t_start_cntr_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_tof_valid\"],bins=25) # about 5050\n",
    "plt.title(\"t_tof_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_bcal_valid\"],bins=25) # almost all 0s\n",
    "plt.title(\"t_bcal_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_fcal_valid\"],bins=25) # almost all 0s\n",
    "plt.title(\"t_fcal_valid\")\n",
    "plt.show()\n",
    "# ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbzN70C9MCrQ"
   },
   "source": [
    "### 1D Hist of Hit1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VGzHLbKsLUZ9",
    "outputId": "0e1a0a14-8881-439d-c2f3-f0ec09006cc4"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"hit1_u\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_u\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_v\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_v\")\n",
    "plt.show()\n",
    "# plt.hist(csv_train[\"hit1_sinv\"],bins=25) # most are 0.96603 almost all are around that though\n",
    "# plt.title(\"hit1_sinv\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_cosv\"],bins=25) # most -0.2585\n",
    "# plt.title(\"hit1_cosv\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_sinu\"],bins=25) # most 0.96585\n",
    "# plt.title(\"hit1_sinu\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_cosu\"],bins=25) # most 0.2591\n",
    "# plt.title(\"hit1_cosu\")\n",
    "# plt.show()\n",
    "plt.hist(csv_train[\"hit1_s\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_s\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_ds\"],bins=25) # 0.01 to 0.04\n",
    "plt.title(\"hit1_ds\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_wire\"],bins=101,range=[0,100]) # 0 to 100\n",
    "plt.title(\"hit1_wire\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_glayer\"],bins=25,range=[0,26]) # 6 to 23\n",
    "plt.title(\"hit1_glayer\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_z\"],bins=25) # spaced out between 180 and 340\n",
    "plt.title(\"hit1_z\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_time\"],bins=25) # -75 to 270\n",
    "plt.title(\"hit1_time\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_dE_amp\"],bins=25) # 0 to 2e-7\n",
    "plt.title(\"hit1_dE_amp\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_q\"],bins=25) # 0 to 85\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"hit1_q\")\n",
    "plt.show()\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxIRSBXpYzU7"
   },
   "source": [
    "### 2D Scatters of various data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "8hgtNdVxY-4_",
    "outputId": "9e12924e-9cb0-492c-fcbb-8cbc7ed4680e"
   },
   "outputs": [],
   "source": [
    "plt.scatter(csv_train[\"tanl\"],abs(csv_train[\"q_over_pt\"]),s=0.01) # a lot more 0s\n",
    "plt.title(\"q_over_pt vs tanl\")\n",
    "plt.xlabel(\"tanl\")\n",
    "plt.ylabel(\"q_over_pt\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "4pjOeJd85uWM",
    "outputId": "887b1312-0399-4223-ff5d-576875baa7e8"
   },
   "outputs": [],
   "source": [
    "# all create a plus sign\n",
    "plt.scatter(csv_train[\"t_start_cntr\"],csv_train[\"t_tof\"]) # a lot more 0s\n",
    "plt.title(\"t_tof vs t_start_cntr\")\n",
    "plt.xlabel(\"t_start_cntr\")\n",
    "plt.ylabel(\"t_tof\")\n",
    "plt.show()\n",
    "\n",
    "# plt.hist(csv_train[\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "# plt.hist(csv_train[\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "# plt.hist(csv_train[\"t_bcal\"],bins=25) # ~-22 to 20\n",
    "# plt.hist(csv_train[\"t_fcal\"],bins=25) # ~-100 to ~75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "IFF2_7npOseE",
    "outputId": "8e8bec48-6610-41d1-8faa-64814bb75af9"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"hit1_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit2_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit3_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit4_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit5_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit6_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit10_glayer\"],bins=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdRImNvKZ0Fa"
   },
   "source": [
    "### 2D Scatters of various hit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mxK94YV2FM2"
   },
   "outputs": [],
   "source": [
    "# Oval\n",
    "plt.scatter(csv_train[\"hit1_u\"],csv_train[\"hit1_v\"]) # -3 to 3, even distrib\n",
    "plt.title(\"v vs u\")\n",
    "plt.xlabel(\"u\")\n",
    "plt.ylabel(\"v\")\n",
    "plt.show()\n",
    "\n",
    "# like a flame\n",
    "plt.scatter(csv_train[\"hit1_s\"],csv_train[\"hit1_ds\"]) # -3 to 3, even distrib\n",
    "plt.title(\"ds vs s\")\n",
    "plt.xlabel(\"s\")\n",
    "plt.ylabel(\"ds\")\n",
    "plt.show()\n",
    "\n",
    "# hit1_wire, with single letters, forms an oval\n",
    "plt.scatter(csv_train[\"hit1_wire\"],csv_train[\"hit1_s\"]) # -3 to 3, even distrib\n",
    "plt.title(\"hit1_s vs hit1_wire\")\n",
    "plt.xlabel(\"hit1_wire\")\n",
    "plt.ylabel(\"hit1_s\")\n",
    "plt.show()\n",
    "\n",
    "# go up in steps\n",
    "plt.scatter(csv_train[\"hit1_glayer\"],csv_train[\"hit1_z\"]) # -3 to 3, even distrib\n",
    "plt.title(\"z vs glayer\")\n",
    "plt.xlabel(\"glayer\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.show()\n",
    "\n",
    "# 1:1\n",
    "plt.scatter(csv_train[\"hit1_q\"],csv_train[\"hit1_dE_amp\"]) # -3 to 3, even distrib\n",
    "plt.title(\"dE_amp vs q\")\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"dE_amp\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhNGqPbNQ8fX"
   },
   "outputs": [],
   "source": [
    "aax = \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dM6aySx8xO-"
   },
   "source": [
    "# Depricated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie8Rhqf765N5"
   },
   "source": [
    "## Non Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpJHFBHy74vp"
   },
   "outputs": [],
   "source": [
    "# --==Not in use?==--\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.8)\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "nInput = 10\n",
    "\n",
    "# inputs = keras.layers.Input((None,nInput))\n",
    "# print(\"train shape of one batch:\", x_train.shape[1:])\n",
    "\n",
    "# --==Set seed to get identical results==-- begin\n",
    "# from tensorflow.random import set_seed\n",
    "# np.random.seed(1)\n",
    "# set_seed(2)\n",
    "# --==Set seed to get identical results==-- end\n",
    "\n",
    "\n",
    "#--==Set Weights==--\n",
    "# loss_weights = [1/(sd**2)]\n",
    "# loss_weights = np.array(loss_weights)/sum(loss_weights)\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\", loss_weights=loss_weights, metrics=[\"mae\"])\n",
    "\n",
    "inputs = keras.Input((None,nInput),ragged=True)\n",
    "\n",
    "# --==Choose model==--\n",
    "# x = model(inputs)\n",
    "# x = model_timeless(inputs)\n",
    "# x = RNNTime(inputs)\n",
    "x = RNNTimeless(inputs)\n",
    "# x = RNNTimeStateful(inputs)\n",
    "\n",
    "\n",
    "outs = {\n",
    "    \"q_pt\":Dense(1, name=\"q_pt\")(x),\n",
    "    \"phi\":Dense(1, name=\"phi\")(x),\n",
    "    \"tanl\":Dense(1, name=\"tanl\")(x),\n",
    "    \"D\":Dense(1, name=\"D\")(x),\n",
    "    \"z\":Dense(1, name=\"z\")(x)\n",
    "}\n",
    "\n",
    "y_dict = {\n",
    "    \"q_pt\":y_train[:,0],\n",
    "    \"phi\":y_train[:,1],\n",
    "    \"tanl\":y_train[:,2],\n",
    "    \"D\":y_train[:,3],\n",
    "    \"z\":y_train[:,4]\n",
    "}\n",
    "\n",
    "\n",
    "# model = keras.Model(inputs=inputs, outputs=x, name=\"RNNModel\")\n",
    "# model = keras.Model(inputs=inputs, outputs=x, name=\"RNNModel\")\n",
    "model = keras.Model(inputs=inputs, outputs=outs, name=\"RNNModel\")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hw07sS4jFgJI"
   },
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01, patience=50, mode='min', verbose=1, restore_best_weights=True)\n",
    "rag_x = x_train[0]\n",
    "H = model.fit(x=rag_x, y=y_dict, batch_size=64, epochs=100, verbose=1, callbacks=[es])\n",
    "\n",
    "# Overfit\n",
    "# es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=100, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train[:10], y=y_train[:10], batch_size=1, epochs=200, verbose=1, shuffle=True, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMcu1m3k7rHK"
   },
   "source": [
    "## Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUBj5VjBuyL9"
   },
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7-v4CJdyQk8",
    "outputId": "940df566-bd20-4356-a2cb-da3bcfaa5fd9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "\n",
    "  # print(y_pred)\n",
    "\n",
    "  y_pred_a = []\n",
    "  for k in y_pred.keys():\n",
    "    y_pred_a.append(np.squeeze(y_pred[k]))\n",
    "\n",
    "  y_pred = np.array(y_pred_a).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  # print(y_pred.shape)\n",
    "  print(y_pred)\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "# for k in y_test.keys():\n",
    "#   y_test[k] = np.squeeze(y_test[k])\n",
    "# print(y_test)\n",
    "\n",
    "# print(y_test)\n",
    "# y_test_a = []\n",
    "# for k in y_test.keys():\n",
    "#   y_test_a.append(y_test[k])\n",
    "# y_test = np.squeeze(np.array(y_test_a))\n",
    "# print(y_test.shape)\n",
    "# print(y_test)\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test, y_test, inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAP0kzRvu2hz"
   },
   "source": [
    "### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7mcvkxuuweK",
    "outputId": "1197158f-904e-4103-f45a-2f015a71f639"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(q_pt_true, phi_true, tanl_true, D_true, z_true, q_pt_pred, phi_pred, tanl_pred, D_pred, z_pred, invcov):\n",
    "\n",
    "\n",
    "  y_pred = [q_pt_pred, phi_pred, tanl_pred, D_pred, z_pred]\n",
    "  # y_pred = np.array(y_pred).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  y_pred = tf.cast(y_pred, \"float64\")\n",
    "\n",
    "  y_true = [q_pt_true, phi_true, tanl_true, D_true, z_true]\n",
    "  # y_true = np.array(y_true).astype(\"float64\")\n",
    "  y_true = tf.transpose(y_true, perm=(1,0))\n",
    "  y_true = tf.cast(y_true, \"float64\")\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "y_test_a = []\n",
    "for k in y_test.keys():\n",
    "  y_test_a.append(y_test[k])\n",
    "y_test = np.squeeze(np.array(y_test_a))\n",
    "y_test = np.swapaxes(y_test, 0, 1)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test[:,0],x_test[:,1],x_test[:,2],x_test[:,3],x_test[:,4], y_test[:,0], y_test[:,1],y_test[:,2],y_test[:,3],y_test[:,4], inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kuUpxAuz1sz"
   },
   "source": [
    "### V4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRUvQVjHAD2U"
   },
   "source": [
    "So, we have the prediction and true vector\n",
    "\n",
    "$$\n",
    "y_{pred}=\n",
    "\\begin{bmatrix}\n",
    "q\\_pt \\\\ phi \\\\ tanl \\\\ D \\\\ z\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We have the inverse covariance matrix, we'll label it $C^{-1}$:\n",
    "\n",
    "and $y_p = y_{predict}$\n",
    "\n",
    "$$\n",
    "C^{-1} = \n",
    "\\begin{bmatrix}\n",
    "qq & qp & qt & qd & qz \\\\\n",
    "qp & pp & pt & pd & pz \\\\\n",
    "qt & pt & tt & td & tz \\\\\n",
    "qd & pd & td & dd & dz \\\\\n",
    "qz & pz & tz & dz & zz \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus, the formula before was:\n",
    "\n",
    "$$\n",
    "loss = C^{-1} \\cdot \\vec{y_p}  \\cdot \\vec{y_p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{dot} =\n",
    "  \\begin{bmatrix}\n",
    "    qq & qp & qt & qd & qz \\\\\n",
    "    qp & pp & pt & pd & pz \\\\\n",
    "    qt & pt & tt & td & tz \\\\\n",
    "    qd & pd & td & dd & dz \\\\\n",
    "    qz & pz & tz & dz & zz \n",
    "  \\end{bmatrix} \n",
    "  \\cdot\n",
    "  \\begin{bmatrix}\n",
    "    q\\_pt \\\\ phi \\\\ tanl \\\\ D \\\\ z\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{dot} = \n",
    "  \\begin{bmatrix}\n",
    "    qq*q\\_pt + qp*phi + qt*tanl + qd*D + qz*z \\\\\n",
    "    qp*q\\_pt + pp*phi + pt*tanl + pd*D + pz*z \\\\\n",
    "    qt*q\\_pt + pt*phi + tt*tanl + td*D + tz*z \\\\\n",
    "    qd*q\\_pt + pd*phi + td*tanl + dd*D + dz*z \\\\\n",
    "    qz*q\\_pt + dz*phi + tz*tanl + pz*D + zz*z\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Now, working on the output split for each variable, im looking for ways to seperate the variables after that operation.\n",
    "So maybe just sum the q_pt column of the matrix and multiply by q_pt?\n",
    "\n",
    "Maybe this would work? :\n",
    "\n",
    "$$\n",
    "loss_{q\\_pt} =  y_p^{q\\_pt} * \\sum_{i=0}^{4} C^{-1}_{qi}\n",
    "$$\n",
    "\n",
    "Where $C^{-1}_q$ is one row or column of the matrix of that variable\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8shIh2axz1g_",
    "outputId": "bbc9620f-ee91-4ed5-a02c-8e29e698e46b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  # print(y_pred)\n",
    "\n",
    "  # Theoretically:\n",
    "  # K.dot(invcov[0,:] * q_pt,q_pt)    # ?\n",
    "  \n",
    "\n",
    "\n",
    "  y_pred_a = []\n",
    "  for k in y_pred.keys():\n",
    "    y_pred_a.append(np.squeeze(y_pred[k]))\n",
    "\n",
    "  y_pred = np.array(y_pred_a).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  # print(y_pred.shape)\n",
    "  print(y_pred)\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "# for k in y_test.keys():\n",
    "#   y_test[k] = np.squeeze(y_test[k])\n",
    "# print(y_test)\n",
    "\n",
    "# print(y_test)\n",
    "# y_test_a = []\n",
    "# for k in y_test.keys():\n",
    "#   y_test_a.append(y_test[k])\n",
    "# y_test = np.squeeze(np.array(y_test_a))\n",
    "# print(y_test.shape)\n",
    "# print(y_test)\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test, y_test, inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7htvZHbENgC7"
   },
   "source": [
    "### V5 unedited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK6jqei9Nf5W"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  # y_dict = {\n",
    "  #     \"q_pt\":y_diff[0]*y_diff[0],\n",
    "  #     \"phi\":y_diff[0]*y_diff[0]\n",
    "  # }\n",
    "  # y_diff[0] / invcov[0][0]\n",
    "  return y_loss\n",
    "\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = x_train[2][0]\n",
    "# y_test = model.predict([x_train[0][0:1],x_train[1][0:1],x_train[2][0:1]])\n",
    "# y_test = np.squeeze(y_test)\n",
    "# inconv_test = x_train[1][0]\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "# # print('loss shape: '    + str(loss.shape)    )\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4XBVGilP4tc"
   },
   "source": [
    "### V6 New Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsGirPS0P4jb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "def customLoss(m_invcov):\n",
    "  def customLoss_fn(y_true, y_pred):\n",
    "    batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "    y_pred = tf.cast(K.reshape(y_pred, (batch_size, 5,1)),\"float64\") # y_pred  shape is now (batch, 5,1)\n",
    "    y_true = tf.cast(K.reshape(y_true, (batch_size, 5,1)),\"float64\") # y_state shape is now (batch, 5,1)\n",
    "    invcov = tf.cast(K.reshape(m_invcov, (batch_size, 5,5)),\"float64\") # invcov  shape is now (batch, 5,5)\n",
    "    \n",
    "    # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "    invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "    \n",
    "    # Difference between prediction and true state vectors\n",
    "    y_diff = y_pred - y_true\n",
    "\n",
    "    # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "    y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "    y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "    y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "    y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "    return y_loss\n",
    "  return customLoss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkuUUNJWjM_i"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "\n",
    "  y_pred = tf.cast(K.reshape(y_pred, (batch_size, 5,1)),\"float64\") # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = tf.cast(K.reshape(y_true, (batch_size, 5,1)),\"float64\") # y_state shape is now (batch, 5,1)\n",
    "  invcov = tf.cast(K.reshape(invcov, (batch_size, 5,5)),\"float64\") # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqNp5hG98yPr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DBknRRcl6LMm"
   ],
   "name": "CopyOfRNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
