{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te6rdZgT5yAW"
   },
   "source": [
    "# Copy of Previous RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdsGeayZ523e"
   },
   "source": [
    "### Reminders:\n",
    "\n",
    "Read up on some of these:\n",
    "- https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/\n",
    "- https://fairyonice.github.io/Stateful-LSTM-model-training-in-Keras.html \n",
    "- https://github.com/keras-team/keras/issues/5714\n",
    "- https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/\n",
    "\n",
    "###Shuffle Data!!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ro1oh9-25xM9",
    "outputId": "e2638721-dec3-4b6c-9db0-df28d7fce705"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yOaxkJ58hh"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5Lupm0J6Hre"
   },
   "source": [
    "## Do Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gE8NHB9v6IDc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "from math import isnan\n",
    "from scipy.stats import norm\n",
    "\n",
    "pandas.set_option('display.max_columns', None)\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yhg_bSj36CSt"
   },
   "source": [
    "## Obtain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1LMf25vB5_vs"
   },
   "outputs": [],
   "source": [
    "# read data from drive\n",
    "# csv_train = pandas.read_csv(\"drive/MyDrive/first_10k.csv\")\n",
    "csv_train = pandas.read_csv(\"first_10k.csv\")\n",
    "# csv_train = pandas.read_csv(\"drive/MyDrive/FDC_tracks.csv\")\n",
    "unparsed_train = np.array(csv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L54wTFh-NGU"
   },
   "source": [
    "## Ragged Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KoN7P86yCif6",
    "outputId": "b9ad9880-a607-4ac8-dd65-3e3a802f4a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.588573718740314, 47.716949659038654, 47.92074125234375, 0.0433333333333333, 96.0, 24.0, 343.50521373311966, 274.8641357421875, 3.24600255369344e-07, 137.76808719278492]\n",
      "[-47.69482374702719, -47.64816097490714, -47.78196688638672, 0.0100725857504721, 1.0, 1.0, 176.8656847042481, -77.76896667480467, 7.068405941829982e-10, 0.2999999999999999]\n",
      "[53.93938010927168, 182.16740477086037, 20.384216393930128, 80.4857546970404]\n",
      "[-59.97950523099952, -113.91363367061092, -21.351611455494343, -101.41199494730319]\n"
     ]
    }
   ],
   "source": [
    "_max = [-1000 for i in range(10)]\n",
    "_min = [1000 for i in range(10)]\n",
    "for index,event in enumerate(unparsed_train):\n",
    "  lower = 67\n",
    "  for upper in range(lower+14, event.shape[0]+1, 14):\n",
    "    d = event[lower:upper]\n",
    "    d = np.append(d[:2],d[6:])\n",
    "    for a in range(len(d)):\n",
    "      if d[a] > _max[a]:\n",
    "        _max[a] = d[a]\n",
    "      if d[a] < _min[a]:\n",
    "        _min[a] = d[a]\n",
    "    lower = upper\n",
    "print(_max)\n",
    "print(_min)\n",
    "\n",
    "_TOF_max = [-1000 for i in range(4)]\n",
    "_TOF_min = [1000 for i in range(4)]\n",
    "for index,event in enumerate(unparsed_train):\n",
    "  TOF = event[59:67]\n",
    "  # print(TOF)\n",
    "  for a in range(4):\n",
    "    if TOF[a*2] > _TOF_max[a]:\n",
    "      _TOF_max[a] = TOF[a*2]\n",
    "    if TOF[a*2] < _TOF_min[a]:\n",
    "      _TOF_min[a] = TOF[a*2]\n",
    "print(_TOF_max)\n",
    "print(_TOF_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOfDY5F8yJyl"
   },
   "source": [
    "After trying out many different possibilities, I have found making a RaggedTensor is the way to make variable timesteps.\n",
    "\n",
    "Once you create a RaggedTensor ONLY FOR X DATA, you need to also add:\n",
    "\n",
    "ragged=True\n",
    "\n",
    "to the keras.Inputs() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BfACUWvW-NGd"
   },
   "outputs": [],
   "source": [
    "def ragged_parser(unparsed):\n",
    "  global _min, _max\n",
    "  x_final = []\n",
    "  y_final = []\n",
    "  invCov_final = []\n",
    "  cov_final = []\n",
    "  for event in unparsed:\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    nEvent = event[0]     # all the data split into neat little arrays...\n",
    "    state = event[1:6]\n",
    "    coVar = event[6:31]\n",
    "    invCoVar = event[31:56]\n",
    "    goodnessOfFit = event[56:59]\n",
    "    TOF = event[59:67]\n",
    "\n",
    "    if goodnessOfFit[2] > 0.1:   # Cutting if rms is too high\n",
    "      continue\n",
    "    hits = []\n",
    "    lower = 67\n",
    "    for upper in range(lower+14, event.shape[0]+1, 14): # to flip just go from end to 67 by -14 steps?\n",
    "\n",
    "      # hasNAN = False\n",
    "      # for val in event[lower:upper]:\n",
    "      #   if isnan(val):\n",
    "      #     hasNAN = True\n",
    "      # if not hasNAN:\n",
    "\n",
    "      if not isnan(event[lower]):            # Check if we are done with hits, because data is cut short, the rest will be nan\n",
    "        hit_data = event[lower:upper]                      # retrieving the hit\n",
    "        hit_data = np.append(hit_data[:2],hit_data[6:])    # cutting out the sin and cos data\n",
    "        for z in range(len(hit_data)):\n",
    "          hit_data[z] = (hit_data[z] - _min[z]) / (_max[z] - _min[z])    # we need to normalize the data; this can be moved to a lambda layer in the network if needed.\n",
    "        for i_TOF in range(4):\n",
    "          TOF[i_TOF*2] = (TOF[i_TOF*2] - _TOF_min[i_TOF]) / (_TOF_max[i_TOF] - _TOF_min[i_TOF])\n",
    "        hit_data = np.append(hit_data,TOF)\n",
    "        hits.append(np.ndarray.tolist(hit_data))       # we want it as a list to convert to RaggedTensor later; last time I checked it didnt work with array.\n",
    "      lower = upper\n",
    "    for i in range(len(hits)):   # this could be simplified to just: \"x = hits\" if im not mistaken...\n",
    "      x.append(hits[i])          # however we might need to add y.append(hits[i+1]) for later testing so leaving it like this for now...\n",
    "    y = np.ndarray.tolist(state)   # technically not needed, can be removed later... at first I thought i need to pass RaggedTensor labels, but that is not the case.\n",
    "    x_final.append(x)          # want x_final to be shape (event, hit, 10) as a list\n",
    "    y_final.append(y)          # want y_final to be shape (event, 5)       as a np.array\n",
    "    # y_final.append(y[0])          # want y_final to be shape (event, 5)       as a np.array\n",
    "    invCov_final.append(invCoVar[:])  # want other_f to be shape (event, 25)      as a np.array\n",
    "    cov_final.append(coVar[:])\n",
    "  x_final = tf.ragged.constant(x_final)   # convert list to RaggedTensor because timesteps (number of hits) are variable between events\n",
    "  y_final = np.array(y_final)\n",
    "  invCov_final = np.array(invCov_final)\n",
    "  cov_final = np.array(cov_final)\n",
    "  return [x_final, invCov_final, cov_final, y_final], y_final   # with the custom loss the x_train (input) needs to be a list of [inputs, inverseCovariance, labels]\n",
    "  # return [x_final, invCov_final], y_final   # with the custom loss the x_train (input) needs to be a list of [inputs, inverseCovariance, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7H0PK6Q-NGg",
    "outputId": "6fcb6671-f32c-402a-c1a3-8489231a8d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--==Types==--\n",
      "--x_train:--\n",
      "\n",
      "  -> input_data: x_train[0]\n",
      "  -> type expected: RaggedTensor\n",
      " <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'>\n",
      "\n",
      "  -> invCov: x_train[1]\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "  -> y_train: x_train[2]\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "--y_train:--\n",
      "\n",
      "  -> y_train: y_train\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "--==Shapes==--\n",
      "--x_train:--  \n",
      "num of events: 9301\n",
      "\n",
      "  RaggedTensor | Input:  shape = (9301, 18, 18)\n",
      "  np.array     | InvCov: shape = (9301, 25)\n",
      "  np.array     | Labels: shape = (9301, 25)\n",
      "\n",
      "--x_train:--\n",
      "  np.array     | Labels: shape = (9301, 5)\n",
      "x_train : <tf.RaggedTensor [[0.7692640423774719, 0.765606164932251, 0.22407019138336182, 0.2642019987106323, 0.49473685026168823, 0.9130434989929199, 0.9742012023925781, 0.8189674615859985, 0.006027945317327976, 0.006027945317327976, 0.5265106558799744, 0.0, 0.4158459007740021, 1.0, 0.511589527130127, 0.0, 0.5575219988822937, 0.0], [0.6926482319831848, 0.5690751671791077, 0.3645309507846832, 0.05952613055706024, 0.2631579041481018, 0.8695651888847351, 0.9611853361129761, 0.28649309277534485, 0.033259935677051544, 0.033259935677051544, 0.5311324596405029, 0.0, 0.3861425220966339, 1.0, 0.5238472819328308, 0.0, 0.5605869889259338, 0.0], [0.23875504732131958, 0.24075621366500854, 0.7672280669212341, 0.10457572340965271, 0.5052631497383118, 0.782608687877655, 0.9352953433990479, 0.504021406173706, 0.018304066732525826, 0.018304066732525826, 0.5311729907989502, 0.0, 0.38604220747947693, 1.0, 0.5241410136222839, 0.0, 0.5606038570404053, 0.0], [0.33787238597869873, 0.4484703540802002, 0.6097946166992188, 0.07363839447498322, 0.7157894968986511, 0.739130437374115, 0.7676599621772766, 0.33562812209129333, 0.02666318602859974, 0.02666318602859974, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241480469703674, 0.0, 0.5606039762496948, 0.0], [0.5710769891738892, 0.6744455695152283, 0.37305063009262085, 0.288280189037323, 0.7052631378173828, 0.695652186870575, 0.7548211812973022, 0.7309820652008057, 0.005347346421331167, 0.005347346421331167, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.733315110206604, 0.7239073514938354, 0.263613760471344, 0.041055768728256226, 0.4842105209827423, 0.6521739363670349, 0.7418311238288879, 0.5430606007575989, 0.048400089144706726, 0.048400089144706726, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6543219089508057, 0.5472460389137268, 0.39544129371643066, 0.12399385124444962, 0.2947368323802948, 0.6086956262588501, 0.728915810585022, 0.5731130242347717, 0.015151274390518665, 0.015151274390518665, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.42824169993400574, 0.3272317349910736, 0.6255077123641968, 0.04272006079554558, 0.3052631616592407, 0.5652173757553101, 0.7160916924476624, 0.30231329798698425, 0.046525269746780396, 0.046525269746780396, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.2750035524368286, 0.28259047865867615, 0.7278001308441162, 0.024007415398955345, 0.5157894492149353, 0.52173912525177, 0.703081488609314, 0.3300199508666992, 0.08132698386907578, 0.08132698386907578, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.3927527666091919, 0.47597992420196533, 0.5672175288200378, 0.08297774195671082, 0.6631578803062439, 0.47826087474823, 0.4162442684173584, 0.5024588704109192, 0.023499751463532448, 0.023499751463532448, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.567786455154419, 0.6398308277130127, 0.392142653465271, 0.030097603797912598, 0.6421052813529968, 0.43478259444236755, 0.40303486585617065, 0.2945617437362671, 0.06557145714759827, 0.06557145714759827, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6758876442909241, 0.6613656878471375, 0.3252999782562256, 0.05598178505897522, 0.4736842215061188, 0.3913043439388275, 0.39034584164619446, 0.45713284611701965, 0.035419683903455734, 0.035419683903455734, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6067799925804138, 0.5218054056167603, 0.433348685503006, 0.06648389250040054, 0.3368421196937561, 0.3478260934352875, 0.3770085275173187, 0.44806209206581116, 0.029668668285012245, 0.029668668285012245, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.33219149708747864, 0.3447727859020233, 0.6660985350608826, 0.02286168746650219, 0.5263158082962036, 0.260869562625885, 0.35123491287231445, 0.42745885252952576, 0.085147425532341, 0.085147425532341, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.436598539352417, 0.4929949641227722, 0.5362541675567627, 0.03817495331168175, 0.6105263233184814, 0.21739129722118378, 0.0648939236998558, 0.2867984175682068, 0.052010804414749146, 0.052010804414749146, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6153600811958313, 0.6007120013237, 0.38774821162223816, 0.04457264766097069, 0.4736842215061188, 0.1304347813129425, 0.038748566061258316, 0.4948866367340088, 0.04459531232714653, 0.04459531232714653, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.45056793093681335, 0.40904945135116577, 0.5723205208778381, 0.04106782376766205, 0.42105263471603394, 0.043478261679410934, 0.012867040000855923, 0.33683282136917114, 0.048385992646217346, 0.048385992646217346, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.3925012946128845, 0.4053120017051697, 0.6041624546051025, 0.032487500458955765, 0.5263158082962036, 0.0, 0.00017478904919698834, 0.65003901720047, 0.060900986194610596, 0.060900986194610596, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0]]>\n",
      "y_train : [-4.805 -1.959  9.798  0.297 77.505]\n"
     ]
    }
   ],
   "source": [
    "# split = 450000\n",
    "# split = 3\n",
    "# split = int(unparsed_train.shape[0]*0.90)\n",
    "x_train, y_train = ragged_parser(unparsed_train)\n",
    "# x_train, y_train = ragged_parser(unparsed_train[:split])\n",
    "# x_test , y_test  = ragged_parser(unparsed_train[split:split+100])\n",
    "\n",
    "print(\"--==Types==--\")\n",
    "print(\"--x_train:--\")\n",
    "print(\"\\n  -> input_data: x_train[0]\\n  -> type expected: RaggedTensor\\n \"+str(type(x_train[0])))\n",
    "print(\"\\n  -> invCov: x_train[1]\\n  -> type expected: np.array\\n \"+str(type(x_train[1])))\n",
    "print(\"\\n  -> y_train: x_train[2]\\n  -> type expected: np.array\\n \"+str(type(x_train[2])))\n",
    "print(\"\\n--y_train:--\")\n",
    "print(\"\\n  -> y_train: y_train\\n  -> type expected: np.array\\n \"+str(type(y_train)))\n",
    "\n",
    "print(\"\\n\\n--==Shapes==--\")\n",
    "print(\"--x_train:--  \\nnum of events: \" + str(x_train[0].shape[0]))\n",
    "print(\"\\n  RaggedTensor | Input:  shape = \" + \"(\" + str(x_train[0].shape[0]) + \", \"+ str(x_train[0][0].shape[0]) + \", \"+ str(x_train[0][0][0].shape[0]) + \")\")\n",
    "print(\"  np.array     | InvCov: shape = \" + str(x_train[1].shape))\n",
    "print(\"  np.array     | Labels: shape = \" + str(x_train[2].shape))\n",
    "print(\"\\n--x_train:--\")\n",
    "print(\"  np.array     | Labels: shape = \" + str(y_train.shape))\n",
    "\n",
    "print(\"x_train : \" + str(x_train[0][0]))\n",
    "print(\"y_train : \" + str(y_train[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMvnihIl6ail"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdreWH016c08"
   },
   "source": [
    "## Defining Models\n",
    "\n",
    "- model\n",
    "  - Very basic testing RNN model\n",
    "  - Output every timestep\n",
    "\n",
    "- model_timeless\n",
    "  - Very basic testing RNN model\n",
    "  - Output only at the end\n",
    "\n",
    "- RNNTime\n",
    "  - Advanced\n",
    "  - Time distributed, output every timestep\n",
    "\n",
    "- RNNTimeless\n",
    "  - Advanced\n",
    "  - Only output at final layer\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e8qCwtNw6bFk"
   },
   "outputs": [],
   "source": [
    "def model(x):\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='input_lstm1', return_sequences=True)(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(32, activation='relu'), name=\"TD1-Dense\")(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(14, activation='linear'), name=\"output-Dense\")(x)\n",
    "  return x\n",
    "def model_timeless(x):\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='input_lstm1', return_sequences=False)(x)\n",
    "  x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "  x = keras.layers.Dense(5, activation='relu', name=\"output-Dense\")(x)\n",
    "  return x\n",
    "\n",
    "def RNNTime(x):\n",
    "  x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(32,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(32, activation='relu'), name=\"TD1-Dense\")(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(5, activation='linear'), name=\"output-Dense\")(x)\n",
    "  return x\n",
    "\n",
    "def RNNTimeless(x):\n",
    "  x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=False)(x)\n",
    "  x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "  x = keras.layers.Dense(1, activation='linear', name=\"output-Dense\")(x)\n",
    "  # x = keras.layers.Dense(5, activation='linear', name=\"output-Dense\")(x)\n",
    "  # x = keras.layers.lambda(# normalize)\n",
    "  return x\n",
    "\n",
    "\n",
    "# def RNNTimeless(x):\n",
    "#   x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "#   x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "#   x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=False)(x)\n",
    "#   x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "#   x = keras.layers.Dense(5, activation='linear', name=\"output-Dense\")(x)\n",
    "#   # x = keras.layers.lambda(# normalize)\n",
    "#   return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtf37tFTB5HU"
   },
   "source": [
    "## Custom Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOZIj6R8u0VG"
   },
   "source": [
    "### V1 Originial, unedited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v8iA9da3HLBs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_loss\n",
    "\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = x_train[2][0]\n",
    "# y_test = model.predict([x_train[0][0:1],x_train[1][0:1],x_train[2][0:1]])\n",
    "# y_test = np.squeeze(y_test)\n",
    "# inconv_test = x_train[1][0]\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXxfMKTqTvBC"
   },
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZX3nAlhgTu4h"
   },
   "outputs": [],
   "source": [
    "def customMetric(y_true, y_pred, cov, id=0):\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  cov = K.reshape(cov, (batch_size, 5,5)) # cov  shape is now (batch, 5,5)\n",
    "  # cov = tf.transpose(cov, perm=[0,2,1])     # cov shape is now (batch, 5,5)\n",
    "  y_diff = y_pred[:,id] - y_true[:,id]\n",
    "  # y_diff = K.reshape(y_diff, (batch_size,1))\n",
    "  cov = K.reshape(cov[:,id,id], (batch_size,1))\n",
    "  # print(\"diff:\\n\",y_diff)\n",
    "  print(\"cov:\\n\",cov)\n",
    "  # return (y_diff*y_diff)/(cov[:,id,id])\n",
    "  return tf.math.square(y_diff)/(cov)\n",
    "\n",
    "# ccov = x_train[2][0:6]\n",
    "# ccov = np.reshape(ccov, (6,5,5))\n",
    "\n",
    "# print(ccov)\n",
    "\n",
    "# metric = K.eval(customMetric(y_train[0:6],y_train[1:7],x_train[2][0:6],3))\n",
    "# print(\"metric: \\n\",metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qw41A73h-zGB"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZ7d8PMx652m",
    "outputId": "aab903f4-7785-4519-ef41-4067f641d5e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNNModel\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 18)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_lstm1 (LSTM)              (None, None, 128)    75264       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, None, 64)     49408       input_lstm1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm3 (LSTM)                    (None, 64)           33024       lstm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Dense1 (Dense)                  (None, 32)           2080        lstm3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output-Dense (Dense)            (None, 1)            33          Dense1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 159,809\n",
      "Trainable params: 159,809\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --==Not in use?==--\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.8)\n",
    "from keras.layers import Dense\n",
    "\n",
    "# nInput = 10\n",
    "nInput = 18\n",
    "\n",
    "# --==Set seed to get identical results==-- begin\n",
    "# from tensorflow.random import set_seed\n",
    "# np.random.seed(1)\n",
    "# set_seed(2)\n",
    "# --==Set seed to get identical results==-- end\n",
    "\n",
    "#--==Set Weights==--\n",
    "# loss_weights = [1/(sd**2)]\n",
    "# loss_weights = np.array(loss_weights)/sum(loss_weights)\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\", loss_weights=loss_weights, metrics=[\"mae\"])\n",
    "\n",
    "inputs = keras.Input((None,nInput))\n",
    "input_true = keras.Input((5,))\n",
    "input_incov = keras.Input((25,))\n",
    "input_cov_f = keras.Input((25,))\n",
    "all_inputs = [inputs, input_incov, input_cov_f, input_true]\n",
    "# all_inputs = [inputs, input_incov, input_true]\n",
    "\n",
    "# --==Choose model==--\n",
    "# x = model(inputs)\n",
    "# x = model_timeless(inputs)\n",
    "# x = RNNTime(inputs)\n",
    "x = RNNTimeless(inputs)\n",
    "# x = RNNTimeStateful(inputs)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# outs = {\n",
    "#     \"q_pt\":Dense(1, name=\"q_pt\")(x),\n",
    "#     \"phi\":Dense(1, name=\"phi\")(x),\n",
    "#     \"tanl\":Dense(1, name=\"tanl\")(x),\n",
    "#     \"D\":Dense(1, name=\"D\")(x),\n",
    "#     \"z\":Dense(1, name=\"z\")(x)\n",
    "# }\n",
    "\n",
    "# y_dict = {\n",
    "#     \"q_pt\":y_train[:,0],\n",
    "#     \"phi\":y_train[:,1],\n",
    "#     \"tanl\":y_train[:,2],\n",
    "#     \"D\":y_train[:,3],\n",
    "#     \"z\":y_train[:,4]\n",
    "# }\n",
    "\n",
    "# model = keras.Model(inputs=all_inputs, outputs=outs, name=\"RNNModel\")\n",
    "\n",
    "model = keras.Model(inputs=all_inputs, outputs=x, name=\"RNNModel\")\n",
    "\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 0),name=\"q_pt\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 1),name=\"phi\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 2),name=\"tanl\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 3),name=\"D\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 4),name=\"z\")\n",
    "\n",
    "# model.add_loss(customLoss(input_true, x, input_incov))\n",
    "# model.compile(loss=None, optimizer=optimizer, metrics=[\"mae\"])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkjx1-6Gr76F"
   },
   "source": [
    "### Custom Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t2pxxUfH7sjY"
   },
   "outputs": [],
   "source": [
    "def concat_hist(H1,H2):\n",
    "  H = {}\n",
    "  for i in H1.keys():\n",
    "    H[i] = list(np.append(np.array(H1[i]),np.array(H2[i])))\n",
    "  return H\n",
    "H = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjLh8Eh07HRW",
    "outputId": "e8312305-0acd-4240-c575-5b48161d1655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 90.5193 - mae: 4.3574\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 87.8191 - mae: 4.3394\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 87.0162 - mae: 4.3019\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 86.5982 - mae: 4.3120\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 85.5263 - mae: 4.2560\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 85.9267 - mae: 4.2316\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 85.2254 - mae: 4.2309\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 85.0848 - mae: 4.2363\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 85.0412 - mae: 4.2228\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 84.6255 - mae: 4.2147\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 84.5020 - mae: 4.1706\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 84.0471 - mae: 4.1729\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 84.5438 - mae: 4.2076\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 84.1722 - mae: 4.2148\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 84.0388 - mae: 4.1684\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 83.6464 - mae: 4.1907\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 83.6880 - mae: 4.1889\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 83.3589 - mae: 4.1140\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 83.6691 - mae: 4.1542\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 82.9586 - mae: 4.1212\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 82.8326 - mae: 4.1354\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 82.8868 - mae: 4.1233\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 83.3230 - mae: 4.1564\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 82.7627 - mae: 4.1217\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 82.4550 - mae: 4.1167\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 82.9427 - mae: 4.1296\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 83.0086 - mae: 4.1449\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 82.2662 - mae: 4.1087\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 81.8009 - mae: 4.0997\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 82.1557 - mae: 4.1087\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 82.4246 - mae: 4.1221\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 82.3736 - mae: 4.1469\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 81.6371 - mae: 4.0960\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 81.2497 - mae: 4.0562\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 80.6974 - mae: 4.0574\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 80.5451 - mae: 4.0781\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 80.7062 - mae: 4.0633\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 81.3729 - mae: 4.0880\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 79.7850 - mae: 4.0520\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 82.2400 - mae: 4.1708\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 80.0165 - mae: 4.0516\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 79.0100 - mae: 4.0322\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 79.7126 - mae: 4.0473\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 78.9349 - mae: 4.0353\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 79.2886 - mae: 4.0697\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 77.9810 - mae: 4.0174\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 77.3909 - mae: 3.9867\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 78.6917 - mae: 4.0278\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 77.8877 - mae: 4.0332\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 77.7445 - mae: 3.9896\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 77.7350 - mae: 3.9965\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 78.6395 - mae: 4.0142\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 77.8401 - mae: 4.0197\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 76.3919 - mae: 3.9624\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 76.5693 - mae: 3.9476\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 79.9480 - mae: 4.0737\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 77.8279 - mae: 4.0047\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 75.5566 - mae: 3.9378\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 75.0176 - mae: 3.9367\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 75.5640 - mae: 3.9386\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 74.4476 - mae: 3.9040\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 73.6377 - mae: 3.9202\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 72.9972 - mae: 3.8879\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 73.2332 - mae: 3.8992\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 72.6516 - mae: 3.8886\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 72.9450 - mae: 3.8759\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 71.6166 - mae: 3.8614\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 71.7809 - mae: 3.8580\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 70.4688 - mae: 3.8375\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 70.0957 - mae: 3.8226\n",
      "Epoch 71/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 69.6737 - mae: 3.8029\n",
      "Epoch 72/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 71.2972 - mae: 3.8386\n",
      "Epoch 73/300\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 73.3761 - mae: 3.9111\n",
      "Epoch 74/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 68.7930 - mae: 3.8116\n",
      "Epoch 75/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 67.3988 - mae: 3.7569\n",
      "Epoch 76/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 66.5383 - mae: 3.7246\n",
      "Epoch 77/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 66.5566 - mae: 3.7202\n",
      "Epoch 78/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 67.9749 - mae: 3.7541\n",
      "Epoch 79/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 66.0057 - mae: 3.7420\n",
      "Epoch 80/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 66.4194 - mae: 3.7747\n",
      "Epoch 81/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 64.0356 - mae: 3.6580\n",
      "Epoch 82/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 63.9137 - mae: 3.6555\n",
      "Epoch 83/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 63.9248 - mae: 3.7129\n",
      "Epoch 84/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 63.4251 - mae: 3.7038\n",
      "Epoch 85/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 62.6159 - mae: 3.6432\n",
      "Epoch 86/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 60.7838 - mae: 3.5737\n",
      "Epoch 87/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 62.3633 - mae: 3.6196\n",
      "Epoch 88/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 60.6833 - mae: 3.6121\n",
      "Epoch 89/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 58.4558 - mae: 3.5404\n",
      "Epoch 90/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 58.3058 - mae: 3.5194\n",
      "Epoch 91/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 63.9689 - mae: 3.6863\n",
      "Epoch 92/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 59.4828 - mae: 3.6056\n",
      "Epoch 93/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 68.4705 - mae: 3.7861\n",
      "Epoch 94/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 63.0733 - mae: 3.6739\n",
      "Epoch 95/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 57.6752 - mae: 3.5587\n",
      "Epoch 96/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 54.2413 - mae: 3.4468\n",
      "Epoch 97/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 54.4518 - mae: 3.4840\n",
      "Epoch 98/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 52.8480 - mae: 3.3998\n",
      "Epoch 99/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 53.2752 - mae: 3.4402\n",
      "Epoch 100/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 50.1781 - mae: 3.3638\n",
      "Epoch 101/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 49.0328 - mae: 3.3320\n",
      "Epoch 102/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 46.6058 - mae: 3.2541\n",
      "Epoch 103/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 45.1550 - mae: 3.2172\n",
      "Epoch 104/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 46.6461 - mae: 3.3087\n",
      "Epoch 105/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 45.8378 - mae: 3.2643\n",
      "Epoch 106/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 46.0473 - mae: 3.2625\n",
      "Epoch 107/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 47.2975 - mae: 3.2878\n",
      "Epoch 108/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 46.8673 - mae: 3.3283\n",
      "Epoch 109/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 40.7711 - mae: 3.1224\n",
      "Epoch 110/300\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 39.5067 - mae: 3.0773\n",
      "Epoch 111/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 42.1525 - mae: 3.1739\n",
      "Epoch 112/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 39.8804 - mae: 3.1192\n",
      "Epoch 113/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 39.2037 - mae: 3.0722\n",
      "Epoch 114/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 37.3160 - mae: 3.0136\n",
      "Epoch 115/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 34.5991 - mae: 2.9250\n",
      "Epoch 116/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 36.9249 - mae: 3.0418\n",
      "Epoch 117/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 34.7036 - mae: 2.9536\n",
      "Epoch 118/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 32.8251 - mae: 2.8613\n",
      "Epoch 119/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 32.6190 - mae: 2.8512\n",
      "Epoch 120/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 31.9895 - mae: 2.8191\n",
      "Epoch 121/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 30.5548 - mae: 2.7614\n",
      "Epoch 122/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 29.7066 - mae: 2.7398\n",
      "Epoch 123/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 29.0163 - mae: 2.7165\n",
      "Epoch 124/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 28.7740 - mae: 2.6800\n",
      "Epoch 125/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 27.7573 - mae: 2.6771\n",
      "Epoch 126/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 27.7095 - mae: 2.6748\n",
      "Epoch 127/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 26.9419 - mae: 2.6522\n",
      "Epoch 128/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 26.9171 - mae: 2.6496\n",
      "Epoch 129/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 26.3634 - mae: 2.6046\n",
      "Epoch 130/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 26.9299 - mae: 2.5943\n",
      "Epoch 131/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 25.2253 - mae: 2.5559\n",
      "Epoch 132/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 31.0667 - mae: 2.7867\n",
      "Epoch 133/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 27.7817 - mae: 2.6695\n",
      "Epoch 134/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 25.0848 - mae: 2.5329\n",
      "Epoch 135/300\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 23.6363 - mae: 2.4554\n",
      "Epoch 136/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 23.5082 - mae: 2.4836\n",
      "Epoch 137/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 22.9168 - mae: 2.4324\n",
      "Epoch 138/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 22.0711 - mae: 2.3957\n",
      "Epoch 139/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 21.7391 - mae: 2.3642\n",
      "Epoch 140/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 20.8574 - mae: 2.3103\n",
      "Epoch 141/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 20.7452 - mae: 2.3054\n",
      "Epoch 142/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 19.8772 - mae: 2.2522\n",
      "Epoch 143/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 19.5965 - mae: 2.2422\n",
      "Epoch 144/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 19.3425 - mae: 2.2342\n",
      "Epoch 145/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 18.8667 - mae: 2.1962\n",
      "Epoch 146/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 18.6272 - mae: 2.1861\n",
      "Epoch 147/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 18.4417 - mae: 2.1861\n",
      "Epoch 148/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 17.9721 - mae: 2.1401\n",
      "Epoch 149/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 17.8213 - mae: 2.1436\n",
      "Epoch 150/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 18.1047 - mae: 2.1826\n",
      "Epoch 151/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 17.8899 - mae: 2.1697\n",
      "Epoch 152/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 18.0226 - mae: 2.1925\n",
      "Epoch 153/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 17.7684 - mae: 2.1777\n",
      "Epoch 154/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 16.9619 - mae: 2.1305\n",
      "Epoch 155/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 16.9715 - mae: 2.1239\n",
      "Epoch 156/300\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 17.2026 - mae: 2.1303\n",
      "Epoch 157/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 16.3539 - mae: 2.0685\n",
      "Epoch 158/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 15.9402 - mae: 2.0472\n",
      "Epoch 159/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 15.5951 - mae: 2.0113\n",
      "Epoch 160/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 14.9115 - mae: 1.9590\n",
      "Epoch 161/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 14.7890 - mae: 1.9746\n",
      "Epoch 162/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 14.9285 - mae: 1.9914\n",
      "Epoch 163/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 14.6127 - mae: 1.9505\n",
      "Epoch 164/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 14.4924 - mae: 1.9541\n",
      "Epoch 165/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 15.2840 - mae: 2.0330\n",
      "Epoch 166/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 14.5378 - mae: 1.9909\n",
      "Epoch 167/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 14.1574 - mae: 1.9472\n",
      "Epoch 168/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 25.8482 - mae: 2.5589\n",
      "Epoch 169/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 27.0696 - mae: 2.6531\n",
      "Epoch 170/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 25.8550 - mae: 2.5942\n",
      "Epoch 171/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 20.4031 - mae: 2.3224\n",
      "Epoch 172/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 17.5931 - mae: 2.1918\n",
      "Epoch 173/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 16.1969 - mae: 2.0947\n",
      "Epoch 174/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 15.7843 - mae: 2.0571\n",
      "Epoch 175/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 13.5386 - mae: 1.9294\n",
      "Epoch 176/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 12.4466 - mae: 1.8747\n",
      "Epoch 177/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 11.7459 - mae: 1.8028\n",
      "Epoch 178/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 11.1850 - mae: 1.7871\n",
      "Epoch 179/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 10.8274 - mae: 1.7391\n",
      "Epoch 180/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 10.3649 - mae: 1.6980\n",
      "Epoch 181/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 10.0257 - mae: 1.6743\n",
      "Epoch 182/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 17.8556 - mae: 2.1983\n",
      "Epoch 183/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 16.2446 - mae: 2.1269\n",
      "Epoch 184/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 13.3967 - mae: 1.8975\n",
      "Epoch 185/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 11.8215 - mae: 1.7653\n",
      "Epoch 186/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 11.3888 - mae: 1.7526\n",
      "Epoch 187/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 10.8797 - mae: 1.7249\n",
      "Epoch 188/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 10.1238 - mae: 1.6761\n",
      "Epoch 189/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 9.6369 - mae: 1.6456\n",
      "Epoch 190/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 9.3892 - mae: 1.6284\n",
      "Epoch 191/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 9.5151 - mae: 1.6367\n",
      "Epoch 192/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 8.8165 - mae: 1.5738\n",
      "Epoch 193/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 8.6594 - mae: 1.5529\n",
      "Epoch 194/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 8.2063 - mae: 1.5197\n",
      "Epoch 195/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 8.0899 - mae: 1.4974\n",
      "Epoch 196/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 8.0837 - mae: 1.4984\n",
      "Epoch 197/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 7.7104 - mae: 1.4455\n",
      "Epoch 198/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 7.4085 - mae: 1.4152\n",
      "Epoch 199/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 7.2461 - mae: 1.4118\n",
      "Epoch 200/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 7.3647 - mae: 1.4404\n",
      "Epoch 201/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 7.3435 - mae: 1.4382\n",
      "Epoch 202/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 7.2236 - mae: 1.4221\n",
      "Epoch 203/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 6.9642 - mae: 1.3910\n",
      "Epoch 204/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 7.0738 - mae: 1.4198\n",
      "Epoch 205/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 6.9912 - mae: 1.4140\n",
      "Epoch 206/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 6.9652 - mae: 1.4161\n",
      "Epoch 207/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 6.6368 - mae: 1.3824\n",
      "Epoch 208/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 6.6285 - mae: 1.3705\n",
      "Epoch 209/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 6.8972 - mae: 1.4232\n",
      "Epoch 210/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 6.6415 - mae: 1.4033\n",
      "Epoch 211/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 6.6139 - mae: 1.4007\n",
      "Epoch 212/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 6.6403 - mae: 1.4070\n",
      "Epoch 213/300\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 7.3947 - mae: 1.4938\n",
      "Epoch 214/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 7.0624 - mae: 1.4625\n",
      "Epoch 215/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 6.7464 - mae: 1.4140\n",
      "Epoch 216/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 6.1875 - mae: 1.3625\n",
      "Epoch 217/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 5.8623 - mae: 1.3210\n",
      "Epoch 218/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 5.6869 - mae: 1.2895\n",
      "Epoch 219/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 5.4709 - mae: 1.2643\n",
      "Epoch 220/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 5.5290 - mae: 1.2695\n",
      "Epoch 221/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 5.4971 - mae: 1.2896\n",
      "Epoch 222/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 5.1390 - mae: 1.2274\n",
      "Epoch 223/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 5.3934 - mae: 1.2586\n",
      "Epoch 224/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 5.1798 - mae: 1.2285\n",
      "Epoch 225/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 5.4509 - mae: 1.2825\n",
      "Epoch 226/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 5.5246 - mae: 1.2911\n",
      "Epoch 227/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 5.4314 - mae: 1.2886\n",
      "Epoch 228/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 5.6323 - mae: 1.2898\n",
      "Epoch 229/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 5.2062 - mae: 1.2594\n",
      "Epoch 230/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 5.9400 - mae: 1.3682\n",
      "Epoch 231/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 5.8049 - mae: 1.3279\n",
      "Epoch 232/300\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 6.2490 - mae: 1.3890\n",
      "Epoch 233/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 5.7195 - mae: 1.3525\n",
      "Epoch 234/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 5.1905 - mae: 1.2677\n",
      "Epoch 235/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 4.7510 - mae: 1.2101\n",
      "Epoch 236/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 4.3138 - mae: 1.1345\n",
      "Epoch 237/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 4.3324 - mae: 1.1355\n",
      "Epoch 238/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 4.1864 - mae: 1.1230\n",
      "Epoch 239/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 4.4373 - mae: 1.1759\n",
      "Epoch 240/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 4.5840 - mae: 1.1954\n",
      "Epoch 241/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 4.1261 - mae: 1.1314\n",
      "Epoch 242/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 4.0588 - mae: 1.1020\n",
      "Epoch 243/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 4.0530 - mae: 1.1138\n",
      "Epoch 244/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 4.9196 - mae: 1.2518\n",
      "Epoch 245/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 4.8231 - mae: 1.2429\n",
      "Epoch 246/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 4.8130 - mae: 1.2386\n",
      "Epoch 247/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 4.5367 - mae: 1.2076\n",
      "Epoch 248/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 4.3945 - mae: 1.1942\n",
      "Epoch 249/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 3.9888 - mae: 1.1315\n",
      "Epoch 250/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 3.7719 - mae: 1.1009\n",
      "Epoch 251/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 3.6756 - mae: 1.0744\n",
      "Epoch 252/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.4337 - mae: 1.0283\n",
      "Epoch 253/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 3.2085 - mae: 0.9726\n",
      "Epoch 254/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 3.0935 - mae: 0.9629\n",
      "Epoch 255/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 3.1397 - mae: 0.9751\n",
      "Epoch 256/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 3.1052 - mae: 0.9820\n",
      "Epoch 257/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 3.0690 - mae: 0.9686\n",
      "Epoch 258/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 3.2651 - mae: 1.0166\n",
      "Epoch 259/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 3.1656 - mae: 0.9975\n",
      "Epoch 260/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 3.3315 - mae: 1.0372\n",
      "Epoch 261/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 3.4529 - mae: 1.0508\n",
      "Epoch 262/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 3.3763 - mae: 1.0521\n",
      "Epoch 263/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 3.4601 - mae: 1.0549\n",
      "Epoch 264/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 3.4393 - mae: 1.0672\n",
      "Epoch 265/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 3.3190 - mae: 1.0454\n",
      "Epoch 266/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 3.2461 - mae: 1.0284\n",
      "Epoch 267/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 3.3059 - mae: 1.0213\n",
      "Epoch 268/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 2.9729 - mae: 0.9835\n",
      "Epoch 269/300\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 3.1323 - mae: 1.0082\n",
      "Epoch 270/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 3.2998 - mae: 1.0500\n",
      "Epoch 271/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 3.4745 - mae: 1.0881\n",
      "Epoch 272/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 3.4737 - mae: 1.0828\n",
      "Epoch 273/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 3.1372 - mae: 1.0198\n",
      "Epoch 274/300\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 2.9741 - mae: 0.9943\n",
      "Epoch 275/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 3.0978 - mae: 1.0124\n",
      "Epoch 276/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 3.0848 - mae: 1.0134\n",
      "Epoch 277/300\n",
      "37/37 [==============================] - 4s 111ms/step - loss: 3.0300 - mae: 1.0148\n",
      "Epoch 278/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 2.8276 - mae: 0.9691\n",
      "Epoch 279/300\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 2.6519 - mae: 0.9360\n",
      "Epoch 280/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 2.7608 - mae: 0.9531\n",
      "Epoch 281/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 2.8071 - mae: 0.9748\n",
      "Epoch 282/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 2.4969 - mae: 0.8996\n",
      "Epoch 283/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 2.4921 - mae: 0.9065\n",
      "Epoch 284/300\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 2.4772 - mae: 0.8948\n",
      "Epoch 285/300\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 2.5228 - mae: 0.9074\n",
      "Epoch 286/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 2.6071 - mae: 0.9321\n",
      "Epoch 287/300\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 2.5630 - mae: 0.9307\n",
      "Epoch 288/300\n",
      "37/37 [==============================] - 4s 113ms/step - loss: 2.4626 - mae: 0.9035\n",
      "Epoch 289/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 2.4762 - mae: 0.9040\n",
      "Epoch 290/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 3.0452 - mae: 1.0233\n",
      "Epoch 291/300\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 2.8041 - mae: 0.9782\n",
      "Epoch 292/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 2.6887 - mae: 0.9621\n",
      "Epoch 293/300\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 2.4849 - mae: 0.9222\n",
      "Epoch 294/300\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 3.3118 - mae: 1.0521\n",
      "Epoch 295/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 3.1264 - mae: 1.0490\n",
      "Epoch 296/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 3.9366 - mae: 1.1168\n",
      "Epoch 297/300\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 28.2874 - mae: 2.5176\n",
      "Epoch 298/300\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 48.0485 - mae: 3.2651\n",
      "Epoch 299/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 31.6602 - mae: 2.7392\n",
      "Epoch 300/300\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 22.2906 - mae: 2.3907\n"
     ]
    }
   ],
   "source": [
    "H_t = []\n",
    "index = 3\n",
    "\n",
    "# Using custom loss and gen\n",
    "es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01, patience=25, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=300, validation_data=(x_test, y_test), verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=300, shuffle=True, verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train[:,2], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es])\n",
    "# H_t.append(model.fit(x=x_train, y=y_train[:,2], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es]).history)\n",
    "H_t.append(model.fit(x=x_train, y=y_train[:,index], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es]).history)\n",
    "if H == None:\n",
    "  H = H_t[-1]\n",
    "else:\n",
    "  H = concat_hist(H,H_t[-1])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=100, validation_data=test_gen, validation_steps=50, validation_batch_size=32, verbose=1)\n",
    "\n",
    "# Example how it kind of looks like\n",
    "# H = model.fit(x=[x_train, invCov, y_train], y=y_train, batch_size=64, epochs=100, verbose=1)\n",
    "\n",
    "# Overfit\n",
    "# es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=100, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=1, epochs=100, verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=1, epochs=100, verbose=1, validation_data=(x_test,y_test), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ5XBwDV7MGY"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 1\n",
    "final_loss = round(H[\"loss\"][-1],2)\n",
    "date = \"03-03\"\n",
    "def gen_name(m_type):\n",
    "    global model_index, index, date, final_loss\n",
    "    variable = [\"q_pt\",\"phi\",\"tanl\",\"D\",\"z\"][index]\n",
    "    m_str = \"models/\" + str(date) + \"-2021_\" + str(variable) + \"-\" + str(model_index) + \"_loss=\" + str(final_loss) + \".\" + str(m_type)\n",
    "    model_index+=1 # create a check for if file exists, for now just increment\n",
    "    return m_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWvST6o27MYI"
   },
   "outputs": [],
   "source": [
    "# model.save('model.h5', save_format=\"h5\")\n",
    "# TODO check if file exists, increment counter\n",
    "# model.save('drive/MyDrive/Models/RealRNN_1-3-2021_141Ep_Onlytanl-2.h5', save_format=\"h5\")\n",
    "model.save(gen_name(\"h5\"), save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6LGBSia7fg3"
   },
   "source": [
    "## Graph loss and mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "OUPAStLMtq7m",
    "outputId": "d4bada3c-6f91-4a9c-b6d7-d7abe8cc1689"
   },
   "outputs": [],
   "source": [
    "print(H.keys())\n",
    "# print(\"loss: \", H[\"loss\"])\n",
    "# print(\"mae: \", H[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 0\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "ax[0].plot(H[\"loss\"][lim:])\n",
    "# ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "ax[0].legend([\"train\",\"val\"])\n",
    "ax[0].grid(True)\n",
    "\n",
    "\n",
    "ax[1].plot(H[\"mae\"][lim:])\n",
    "# ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "ax[1].legend([\"train\",\"val\"])\n",
    "ax[1].grid(True)\n",
    "\n",
    "model_index -= 1\n",
    "plt.savefig(gen_name(\"png\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_array = np.array(list(H.items()))\n",
    "save_array = np.array([an_array[0][1],an_array[1][1]])\n",
    "model_index -= 1\n",
    "np.save(gen_name(\"npy\"),save_array,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw4xzpuKYEld"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZHLFMS1YE40",
    "outputId": "09e30fe4-cc1b-444f-83b3-c74d1b923762"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=3)\n",
    "from time import sleep\n",
    "\n",
    "for i in range(10):\n",
    "  aax = x_train[0][i:i+1]\n",
    "  oax = x_train[1][i:i+1]\n",
    "  cax = x_train[2][i:i+1]\n",
    "  yax = x_train[3][i:i+1]\n",
    "  aay = y_train[i][index]\n",
    "  pred = model.predict([aax, oax, cax, yax])[0][0]\n",
    "  diff = pred - aay\n",
    "  # print(\"i: \",i)\n",
    "  # print(\"aax:  \",aax[0,0])\n",
    "  print(\"aay:  \",aay)\n",
    "  print(\"pred: \",pred)\n",
    "  # print(\"diff: \",diff)\n",
    "  print(\"\")\n",
    "\n",
    "  # plt.plot(aay[0])\n",
    "  # plt.plot(pred[0])\n",
    "  # plt.show()\n",
    "# [Q/PT, phi, tanl, D, z]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T56J0X6g7O2k"
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgLQh0jf7Ova"
   },
   "outputs": [],
   "source": [
    "# model.save('drive/MyDrive/RealRNN_1-2-2021_2.h5', save_format=\"h5\")\n",
    "model = keras.models.load_model('drive/MyDrive/Models/RealRNN_1-3-2021_300Ep_Onlyphi.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X72YH9S87cvW"
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H.keys())\n",
    "# print(\"loss: \", H[\"loss\"])\n",
    "# print(\"mae: \", H[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 0\n",
    "\n",
    "if len(H.keys()) > 4:\n",
    "  # fig, ax = plt.subplots(4,1,figsize=(5,20))\n",
    "  fig, ax = plt.subplots(3,1,figsize=(5,15))\n",
    "  fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "  ax[0].plot(H[\"loss\"][lim:])\n",
    "  # ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "  ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "  ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "  # ax[0].set_yscale(\"log\")\n",
    "  ax[0].legend([\"train\",\"val\"])\n",
    "  ax[0].grid(True)\n",
    "\n",
    "\n",
    "  ax[1].plot(H[\"mae\"][lim:])\n",
    "  # ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "  ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "  ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "  ax[1].legend([\"train\",\"val\"])\n",
    "  ax[1].grid(True)\n",
    "\n",
    "  ax[2].plot(H[\"q_pt\"][lim:])\n",
    "  ax[2].plot(H[\"phi\"][lim:])\n",
    "  ax[2].plot(H[\"tanl\"][lim:])\n",
    "  ax[2].plot(H[\"D\"][lim:])\n",
    "  ax[2].plot(H[\"z\"][lim:])\n",
    "  ax[2].set_title(\"data vs epoch\", fontsize=20)\n",
    "  ax[2].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[2].set_ylabel(\"data\", fontsize=15)\n",
    "  ax[2].legend([\"q_pt\",\"phi\",\"tanl\",\"D\",\"z\"])\n",
    "  # ax[2].legend([\"phi\",\"tanl\",\"D\",\"z\"])\n",
    "  # ax[2].legend([\"phi\",\"D\",\"z\"])\n",
    "  ax[2].grid(True)\n",
    "\n",
    "  # ax[3].plot(H.history[\"val_q_pt\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_phi\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_tanl\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_D\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_z\"][lim:])\n",
    "  # ax[3].set_title(\"data vs epoch\", fontsize=20)\n",
    "  # ax[3].set_xlabel(\"epoch\", fontsize=15)\n",
    "  # ax[3].set_ylabel(\"data\", fontsize=15)\n",
    "  # ax[3].legend([\"val_q_pt\",\"val_phi\",\"val_tanl\",\"val_D\",\"val_z\"])\n",
    "  # # ax[3].legend([\"val_phi\",\"val_tanl\",\"val_D\",\"val_z\"])\n",
    "  # # ax[3].legend([\"val_phi\",\"val_D\",\"val_z\"])\n",
    "  # ax[3].grid(True)\n",
    "\n",
    "else:\n",
    "  fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "  fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "  ax[0].plot(H[\"loss\"][lim:])\n",
    "  # ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "  ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "  ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "  # ax[0].set_yscale(\"log\")\n",
    "  ax[0].legend([\"train\",\"val\"])\n",
    "  ax[0].grid(True)\n",
    "\n",
    "\n",
    "  ax[1].plot(H[\"mae\"][lim:])\n",
    "  # ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "  ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "  ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "  ax[1].legend([\"train\",\"val\"])\n",
    "  ax[1].grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "k6XSgSit7dE3",
    "outputId": "b3a1caaf-1b21-4348-bfe7-1315ccb39171"
   },
   "outputs": [],
   "source": [
    "print(H.history.keys())\n",
    "print(\"loss: \", H.history[\"loss\"])\n",
    "print(\"mae: \", H.history[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 2\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "ax[0].plot(H.history[\"loss\"][lim:])\n",
    "# ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[0].legend([\"train\",\"val\"])\n",
    "ax[0].grid(True)\n",
    "\n",
    "\n",
    "ax[1].plot(H.history[\"mae\"][lim:])\n",
    "# ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "ax[1].legend([\"train\",\"val\"])\n",
    "ax[1].grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOi6MC8u7swr"
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqqb7Riq7tAY"
   },
   "outputs": [],
   "source": [
    "# Maybe copy over previous function and edit that?\n",
    "def graph(pred, true, diff):\n",
    "\n",
    "  values = [\"u\",\"v\",\"sin(v)\",\"cos(v)\",\"sin(u)\",\"cos(u)\",\"s\",\"ds\",\"wire\",\"glayer\",\"z\",\"time\",\"dE_amp\",\"q\"]\n",
    "  limits = [[\"todo\"]]\n",
    "\n",
    "  size = len(values)\n",
    "\n",
    "  fig, axs = plt.subplots(4,size,figsize=(size*5,20))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "\n",
    "  for i in range(size):\n",
    "    (mu, sigma) = norm.fit(diff[:,i])\n",
    "    print(\"data\" , values[i] ,\" |: mu: \", mu, \"sigma: \" , sigma)\n",
    "    _, bins, _ = axs[0,i].hist(diff[:,i], 20, density=True)\n",
    "    y = norm.pdf(bins, mu, sigma)\n",
    "    l = axs[0,i].plot(bins, y, 'r--', linewidth=2)\n",
    "\n",
    "    axs[0,i].set_title(values[i] + ' diff')\n",
    "    axs[0,i].set_ylabel('freq')\n",
    "    axs[0,i].set_xlabel(values[i] + ' diff')\n",
    "\n",
    "  #--------------------------------------\n",
    "  # PREDICTED VS TRUE\n",
    "  #--------------------------------------\n",
    "    \n",
    "  for i in range(size):\n",
    "    axs[1,i].scatter(true[:,i],pred[:,i])\n",
    "    axs[1,i].grid(True)\n",
    "\n",
    "    axs[1,i].set_title(values[i] + ' (predicted vs true)')\n",
    "    axs[1,i].set_ylabel('pred ' + values[i])\n",
    "    axs[1,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "    # axs[1,i].set_xlim(limits[i])\n",
    "    # axs[1,i].set_ylim(limits[i])\n",
    "    # axs[1,i].plot(limits[i],limits[i], color='b')\n",
    "\n",
    "  #--------------------------------------\n",
    "  # DIFFERENCE VS TRUE\n",
    "  #--------------------------------------\n",
    "\n",
    "  for i in range(size):\n",
    "    axs[2,i].scatter(true[:,i],diff[:,i])\n",
    "    l, r = axs[2,i].get_xlim()\n",
    "    axs[2,i].hlines(0, l, r)\n",
    "    axs[2,i].grid(True)\n",
    "\n",
    "    axs[2,i].set_title(values[i] + ' (difference vs true)')\n",
    "    axs[2,i].set_ylabel('diff ' + values[i])\n",
    "    axs[2,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "  #--------------------------------------\n",
    "  # DIFFERENCE VS TRUE 2D HIST\n",
    "  #--------------------------------------\n",
    "\n",
    "  for i in range(size):\n",
    "    axs[3,i].hist2d(true[:,i],diff[:,i],bins=20)\n",
    "\n",
    "    axs[2,i].set_title(values[i] + ' (difference vs true)')\n",
    "    axs[2,i].set_ylabel('diff ' + values[i])\n",
    "    axs[2,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "  fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHuUmXPCiXt9"
   },
   "outputs": [],
   "source": [
    "def gen_test_data(x_test, y_test, size=1000):\n",
    "  pred = model.predict(x_test)\n",
    "  diff = pred - y_test\n",
    "  return pred, y_test, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AMRhD6Wi7Xh"
   },
   "outputs": [],
   "source": [
    "graph(gen_test_data(x_test, y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAUyMneo7Xj1"
   },
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBsHjTgG7X2y"
   },
   "outputs": [],
   "source": [
    "# make test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWkYEQvFZ7HC"
   },
   "source": [
    "# Verification of proper data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqKxKcOQYEO4"
   },
   "source": [
    "## Using Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml3HHzoEaAhr"
   },
   "outputs": [],
   "source": [
    "aax, aay = next(train_gen)\n",
    "print(aax.shape)\n",
    "print(aay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv72llYMaUYd"
   },
   "outputs": [],
   "source": [
    "print(\"x\",aax[0])\n",
    "print(\"y\",aay[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcwDKKLLe079"
   },
   "source": [
    "## Non Genenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlU3tRGpYxQV",
    "outputId": "a24ded2d-d1d0-4555-84a9-428edf5b2e7d"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  aax = x_train[i]\n",
    "  aay = y_train[i]\n",
    "  # print(aax.shape)\n",
    "  # print(aay.shape)\n",
    "  # print(\"x\",aax)\n",
    "  print(\"y\",aay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCpP2wsfdeGl"
   },
   "source": [
    "## Graphs of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAxLbw94GYzm"
   },
   "source": [
    "### filter_ignore\n",
    "\n",
    "Filters out large and small values and graphs them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiUKfbtfAoLm"
   },
   "outputs": [],
   "source": [
    "def filter_ignore(var,min=None,max=None,bins=25,ylog=False,xlog=False,cut=True):\n",
    "  list_ignore = []\n",
    "\n",
    "  print(\"--== {} ==--\\n\".format(var))\n",
    "\n",
    "  largest = 0\n",
    "  smallest = 0\n",
    "  for i in range(len(csv_train[var])):\n",
    "    if csv_train[var][i] > csv_train[var][largest]:\n",
    "      largest = i\n",
    "    if csv_train[var][i] < csv_train[var][smallest]:\n",
    "      smallest = i\n",
    "  print(\"largest value:  ({}, {:.3f})\".format(largest,csv_train[var][largest]))\n",
    "  print(\"smallest value: ({}, {:.3f})\".format(smallest,csv_train[var][smallest]))\n",
    "\n",
    "  print(\"\")\n",
    "\n",
    "  if min:\n",
    "    for i in range(len(csv_train[var])):\n",
    "      if csv_train[var][i] < min:\n",
    "        list_ignore.append(i)\n",
    "    print(\"min IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "  if max:\n",
    "    for i in range(len(csv_train[var])):\n",
    "      if csv_train[var][i] > max:\n",
    "        list_ignore.append(i)\n",
    "    print(\"max IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "  if min and max:\n",
    "    print(\"total IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "    plt.hist(csv_train[var],range=[min,max],bins=bins)\n",
    "  elif min:\n",
    "    plt.hist(csv_train[var],range=[min,csv_train[var][largest]],bins=bins)\n",
    "  elif max:\n",
    "    plt.hist(csv_train[var],range=[csv_train[var][smallest],max],bins=bins)\n",
    "  else:\n",
    "    plt.hist(csv_train[var],bins=bins)\n",
    "  \n",
    "  plt.title(var)\n",
    "  if cut:\n",
    "    plt.xlim(left=min,right=max)\n",
    "  if ylog:\n",
    "    plt.yscale(\"log\")\n",
    "  if xlog:\n",
    "    plt.xscale(\"log\")\n",
    "  plt.show()\n",
    "  return list_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "BwsJ0YIoBiew",
    "outputId": "962724f0-0845-4ab5-8b27-043de6744e44"
   },
   "outputs": [],
   "source": [
    "filter_ignore(\"q_over_pt\",min=-4000,bins=30,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "2Wyjq07YGjBF",
    "outputId": "9a82db49-7e7e-49aa-8d4b-a0da738ec1f5"
   },
   "outputs": [],
   "source": [
    "filter_ignore(\"tanl\",max=1000,bins=25,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xauTTOyrHPyT"
   },
   "outputs": [],
   "source": [
    "rms_ignore = filter_ignore(\"rms\",max=0.1,bins=25,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "ERa-RZDbN6Hm",
    "outputId": "ac4577c9-f719-47d4-bc16-8fdbb7e413a5"
   },
   "outputs": [],
   "source": [
    "# 'q_over_pt', 'phi', 'tanl', 'D', 'z'\n",
    "# filter_ignore(\"D\", min=-200, ylog=True,bins=25)\n",
    "filter_ignore(\"z\",bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBO9wKCH2ePH"
   },
   "outputs": [],
   "source": [
    "csv_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc3WNfPaYwaL"
   },
   "source": [
    "### 1D Hist of all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "2VqlQxZRgTB7",
    "outputId": "2a5ce7d3-1665-4d71-80f5-5b7ed7ceb673"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"phi\"],bins=50) # -3 to 3, even distrib\n",
    "plt.title(\"phi\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"D\"],range=[-3000,80],bins=25) # -3000 to 50, but val in 65\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"D\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"z\"],bins=100)\n",
    "plt.title(\"z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TbqYK55L05mB",
    "outputId": "81dc2a8d-b572-4505-983a-8958a9bea8b6"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "# fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "plt.hist(csv_train[\"cov_00\"],range=[0,1e8],bins=25) # 0 to 1e13\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"cov_00\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"cov_01\"],bins=25) # -1e6 to over 1e5\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"cov_01\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"chisq\"],bins=25) # 0 to 200\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"chisq\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"Ndof\"],range=[0,44],bins=45) # ? this one weird 0 to ~43\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Ndof\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"rms\"],range=[0,0.1],bins=25) # \n",
    "# plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"rms\")\n",
    "plt.show()\n",
    "# ---\n",
    "# plt.hist(csv_train[\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "plt.hist(csv_train[csv_train[\"t_start_cntr_valid\"] == 1][\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "plt.title(\"t_start_cntr\")\n",
    "plt.show()\n",
    "\n",
    "# plt.hist(csv_train[\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "plt.hist(csv_train[csv_train[\"t_tof_valid\"] == 1][\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "plt.title(\"t_tof\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_bcal\"],bins=25) # ~-22 to 20\n",
    "plt.title(\"t_bcal\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_fcal\"],bins=25) # ~-100 to ~75\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"t_fcal\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"t_start_cntr_valid\"],bins=25) # a lot more 0s\n",
    "plt.title(\"t_start_cntr_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_tof_valid\"],bins=25) # about 5050\n",
    "plt.title(\"t_tof_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_bcal_valid\"],bins=25) # almost all 0s\n",
    "plt.title(\"t_bcal_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_fcal_valid\"],bins=25) # almost all 0s\n",
    "plt.title(\"t_fcal_valid\")\n",
    "plt.show()\n",
    "# ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbzN70C9MCrQ"
   },
   "source": [
    "### 1D Hist of Hit1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VGzHLbKsLUZ9",
    "outputId": "0e1a0a14-8881-439d-c2f3-f0ec09006cc4"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"hit1_u\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_u\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_v\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_v\")\n",
    "plt.show()\n",
    "# plt.hist(csv_train[\"hit1_sinv\"],bins=25) # most are 0.96603 almost all are around that though\n",
    "# plt.title(\"hit1_sinv\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_cosv\"],bins=25) # most -0.2585\n",
    "# plt.title(\"hit1_cosv\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_sinu\"],bins=25) # most 0.96585\n",
    "# plt.title(\"hit1_sinu\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_cosu\"],bins=25) # most 0.2591\n",
    "# plt.title(\"hit1_cosu\")\n",
    "# plt.show()\n",
    "plt.hist(csv_train[\"hit1_s\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_s\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_ds\"],bins=25) # 0.01 to 0.04\n",
    "plt.title(\"hit1_ds\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_wire\"],bins=101,range=[0,100]) # 0 to 100\n",
    "plt.title(\"hit1_wire\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_glayer\"],bins=25,range=[0,26]) # 6 to 23\n",
    "plt.title(\"hit1_glayer\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_z\"],bins=25) # spaced out between 180 and 340\n",
    "plt.title(\"hit1_z\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_time\"],bins=25) # -75 to 270\n",
    "plt.title(\"hit1_time\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_dE_amp\"],bins=25) # 0 to 2e-7\n",
    "plt.title(\"hit1_dE_amp\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_q\"],bins=25) # 0 to 85\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"hit1_q\")\n",
    "plt.show()\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxIRSBXpYzU7"
   },
   "source": [
    "### 2D Scatters of various data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "8hgtNdVxY-4_",
    "outputId": "9e12924e-9cb0-492c-fcbb-8cbc7ed4680e"
   },
   "outputs": [],
   "source": [
    "plt.scatter(csv_train[\"tanl\"],abs(csv_train[\"q_over_pt\"]),s=0.01) # a lot more 0s\n",
    "plt.title(\"q_over_pt vs tanl\")\n",
    "plt.xlabel(\"tanl\")\n",
    "plt.ylabel(\"q_over_pt\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "4pjOeJd85uWM",
    "outputId": "887b1312-0399-4223-ff5d-576875baa7e8"
   },
   "outputs": [],
   "source": [
    "# all create a plus sign\n",
    "plt.scatter(csv_train[\"t_start_cntr\"],csv_train[\"t_tof\"]) # a lot more 0s\n",
    "plt.title(\"t_tof vs t_start_cntr\")\n",
    "plt.xlabel(\"t_start_cntr\")\n",
    "plt.ylabel(\"t_tof\")\n",
    "plt.show()\n",
    "\n",
    "# plt.hist(csv_train[\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "# plt.hist(csv_train[\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "# plt.hist(csv_train[\"t_bcal\"],bins=25) # ~-22 to 20\n",
    "# plt.hist(csv_train[\"t_fcal\"],bins=25) # ~-100 to ~75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "IFF2_7npOseE",
    "outputId": "8e8bec48-6610-41d1-8faa-64814bb75af9"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"hit1_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit2_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit3_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit4_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit5_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit6_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit10_glayer\"],bins=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdRImNvKZ0Fa"
   },
   "source": [
    "### 2D Scatters of various hit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mxK94YV2FM2"
   },
   "outputs": [],
   "source": [
    "# Oval\n",
    "plt.scatter(csv_train[\"hit1_u\"],csv_train[\"hit1_v\"]) # -3 to 3, even distrib\n",
    "plt.title(\"v vs u\")\n",
    "plt.xlabel(\"u\")\n",
    "plt.ylabel(\"v\")\n",
    "plt.show()\n",
    "\n",
    "# like a flame\n",
    "plt.scatter(csv_train[\"hit1_s\"],csv_train[\"hit1_ds\"]) # -3 to 3, even distrib\n",
    "plt.title(\"ds vs s\")\n",
    "plt.xlabel(\"s\")\n",
    "plt.ylabel(\"ds\")\n",
    "plt.show()\n",
    "\n",
    "# hit1_wire, with single letters, forms an oval\n",
    "plt.scatter(csv_train[\"hit1_wire\"],csv_train[\"hit1_s\"]) # -3 to 3, even distrib\n",
    "plt.title(\"hit1_s vs hit1_wire\")\n",
    "plt.xlabel(\"hit1_wire\")\n",
    "plt.ylabel(\"hit1_s\")\n",
    "plt.show()\n",
    "\n",
    "# go up in steps\n",
    "plt.scatter(csv_train[\"hit1_glayer\"],csv_train[\"hit1_z\"]) # -3 to 3, even distrib\n",
    "plt.title(\"z vs glayer\")\n",
    "plt.xlabel(\"glayer\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.show()\n",
    "\n",
    "# 1:1\n",
    "plt.scatter(csv_train[\"hit1_q\"],csv_train[\"hit1_dE_amp\"]) # -3 to 3, even distrib\n",
    "plt.title(\"dE_amp vs q\")\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"dE_amp\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhNGqPbNQ8fX"
   },
   "outputs": [],
   "source": [
    "aax = \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dM6aySx8xO-"
   },
   "source": [
    "# Depricated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie8Rhqf765N5"
   },
   "source": [
    "## Non Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpJHFBHy74vp"
   },
   "outputs": [],
   "source": [
    "# --==Not in use?==--\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.8)\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "nInput = 10\n",
    "\n",
    "# inputs = keras.layers.Input((None,nInput))\n",
    "# print(\"train shape of one batch:\", x_train.shape[1:])\n",
    "\n",
    "# --==Set seed to get identical results==-- begin\n",
    "# from tensorflow.random import set_seed\n",
    "# np.random.seed(1)\n",
    "# set_seed(2)\n",
    "# --==Set seed to get identical results==-- end\n",
    "\n",
    "\n",
    "#--==Set Weights==--\n",
    "# loss_weights = [1/(sd**2)]\n",
    "# loss_weights = np.array(loss_weights)/sum(loss_weights)\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\", loss_weights=loss_weights, metrics=[\"mae\"])\n",
    "\n",
    "inputs = keras.Input((None,nInput),ragged=True)\n",
    "\n",
    "# --==Choose model==--\n",
    "# x = model(inputs)\n",
    "# x = model_timeless(inputs)\n",
    "# x = RNNTime(inputs)\n",
    "x = RNNTimeless(inputs)\n",
    "# x = RNNTimeStateful(inputs)\n",
    "\n",
    "\n",
    "outs = {\n",
    "    \"q_pt\":Dense(1, name=\"q_pt\")(x),\n",
    "    \"phi\":Dense(1, name=\"phi\")(x),\n",
    "    \"tanl\":Dense(1, name=\"tanl\")(x),\n",
    "    \"D\":Dense(1, name=\"D\")(x),\n",
    "    \"z\":Dense(1, name=\"z\")(x)\n",
    "}\n",
    "\n",
    "y_dict = {\n",
    "    \"q_pt\":y_train[:,0],\n",
    "    \"phi\":y_train[:,1],\n",
    "    \"tanl\":y_train[:,2],\n",
    "    \"D\":y_train[:,3],\n",
    "    \"z\":y_train[:,4]\n",
    "}\n",
    "\n",
    "\n",
    "# model = keras.Model(inputs=inputs, outputs=x, name=\"RNNModel\")\n",
    "# model = keras.Model(inputs=inputs, outputs=x, name=\"RNNModel\")\n",
    "model = keras.Model(inputs=inputs, outputs=outs, name=\"RNNModel\")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hw07sS4jFgJI"
   },
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01, patience=50, mode='min', verbose=1, restore_best_weights=True)\n",
    "rag_x = x_train[0]\n",
    "H = model.fit(x=rag_x, y=y_dict, batch_size=64, epochs=100, verbose=1, callbacks=[es])\n",
    "\n",
    "# Overfit\n",
    "# es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=100, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train[:10], y=y_train[:10], batch_size=1, epochs=200, verbose=1, shuffle=True, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMcu1m3k7rHK"
   },
   "source": [
    "## Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUBj5VjBuyL9"
   },
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7-v4CJdyQk8",
    "outputId": "940df566-bd20-4356-a2cb-da3bcfaa5fd9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "\n",
    "  # print(y_pred)\n",
    "\n",
    "  y_pred_a = []\n",
    "  for k in y_pred.keys():\n",
    "    y_pred_a.append(np.squeeze(y_pred[k]))\n",
    "\n",
    "  y_pred = np.array(y_pred_a).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  # print(y_pred.shape)\n",
    "  print(y_pred)\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "# for k in y_test.keys():\n",
    "#   y_test[k] = np.squeeze(y_test[k])\n",
    "# print(y_test)\n",
    "\n",
    "# print(y_test)\n",
    "# y_test_a = []\n",
    "# for k in y_test.keys():\n",
    "#   y_test_a.append(y_test[k])\n",
    "# y_test = np.squeeze(np.array(y_test_a))\n",
    "# print(y_test.shape)\n",
    "# print(y_test)\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test, y_test, inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAP0kzRvu2hz"
   },
   "source": [
    "### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7mcvkxuuweK",
    "outputId": "1197158f-904e-4103-f45a-2f015a71f639"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(q_pt_true, phi_true, tanl_true, D_true, z_true, q_pt_pred, phi_pred, tanl_pred, D_pred, z_pred, invcov):\n",
    "\n",
    "\n",
    "  y_pred = [q_pt_pred, phi_pred, tanl_pred, D_pred, z_pred]\n",
    "  # y_pred = np.array(y_pred).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  y_pred = tf.cast(y_pred, \"float64\")\n",
    "\n",
    "  y_true = [q_pt_true, phi_true, tanl_true, D_true, z_true]\n",
    "  # y_true = np.array(y_true).astype(\"float64\")\n",
    "  y_true = tf.transpose(y_true, perm=(1,0))\n",
    "  y_true = tf.cast(y_true, \"float64\")\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "y_test_a = []\n",
    "for k in y_test.keys():\n",
    "  y_test_a.append(y_test[k])\n",
    "y_test = np.squeeze(np.array(y_test_a))\n",
    "y_test = np.swapaxes(y_test, 0, 1)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test[:,0],x_test[:,1],x_test[:,2],x_test[:,3],x_test[:,4], y_test[:,0], y_test[:,1],y_test[:,2],y_test[:,3],y_test[:,4], inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kuUpxAuz1sz"
   },
   "source": [
    "### V4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRUvQVjHAD2U"
   },
   "source": [
    "So, we have the prediction and true vector\n",
    "\n",
    "$$\n",
    "y_{pred}=\n",
    "\\begin{bmatrix}\n",
    "q\\_pt \\\\ phi \\\\ tanl \\\\ D \\\\ z\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We have the inverse covariance matrix, we'll label it $C^{-1}$:\n",
    "\n",
    "and $y_p = y_{predict}$\n",
    "\n",
    "$$\n",
    "C^{-1} = \n",
    "\\begin{bmatrix}\n",
    "qq & qp & qt & qd & qz \\\\\n",
    "qp & pp & pt & pd & pz \\\\\n",
    "qt & pt & tt & td & tz \\\\\n",
    "qd & pd & td & dd & dz \\\\\n",
    "qz & pz & tz & dz & zz \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus, the formula before was:\n",
    "\n",
    "$$\n",
    "loss = C^{-1} \\cdot \\vec{y_p}  \\cdot \\vec{y_p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{dot} =\n",
    "  \\begin{bmatrix}\n",
    "    qq & qp & qt & qd & qz \\\\\n",
    "    qp & pp & pt & pd & pz \\\\\n",
    "    qt & pt & tt & td & tz \\\\\n",
    "    qd & pd & td & dd & dz \\\\\n",
    "    qz & pz & tz & dz & zz \n",
    "  \\end{bmatrix} \n",
    "  \\cdot\n",
    "  \\begin{bmatrix}\n",
    "    q\\_pt \\\\ phi \\\\ tanl \\\\ D \\\\ z\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{dot} = \n",
    "  \\begin{bmatrix}\n",
    "    qq*q\\_pt + qp*phi + qt*tanl + qd*D + qz*z \\\\\n",
    "    qp*q\\_pt + pp*phi + pt*tanl + pd*D + pz*z \\\\\n",
    "    qt*q\\_pt + pt*phi + tt*tanl + td*D + tz*z \\\\\n",
    "    qd*q\\_pt + pd*phi + td*tanl + dd*D + dz*z \\\\\n",
    "    qz*q\\_pt + dz*phi + tz*tanl + pz*D + zz*z\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Now, working on the output split for each variable, im looking for ways to seperate the variables after that operation.\n",
    "So maybe just sum the q_pt column of the matrix and multiply by q_pt?\n",
    "\n",
    "Maybe this would work? :\n",
    "\n",
    "$$\n",
    "loss_{q\\_pt} =  y_p^{q\\_pt} * \\sum_{i=0}^{4} C^{-1}_{qi}\n",
    "$$\n",
    "\n",
    "Where $C^{-1}_q$ is one row or column of the matrix of that variable\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8shIh2axz1g_",
    "outputId": "bbc9620f-ee91-4ed5-a02c-8e29e698e46b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  # print(y_pred)\n",
    "\n",
    "  # Theoretically:\n",
    "  # K.dot(invcov[0,:] * q_pt,q_pt)    # ?\n",
    "  \n",
    "\n",
    "\n",
    "  y_pred_a = []\n",
    "  for k in y_pred.keys():\n",
    "    y_pred_a.append(np.squeeze(y_pred[k]))\n",
    "\n",
    "  y_pred = np.array(y_pred_a).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  # print(y_pred.shape)\n",
    "  print(y_pred)\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "# for k in y_test.keys():\n",
    "#   y_test[k] = np.squeeze(y_test[k])\n",
    "# print(y_test)\n",
    "\n",
    "# print(y_test)\n",
    "# y_test_a = []\n",
    "# for k in y_test.keys():\n",
    "#   y_test_a.append(y_test[k])\n",
    "# y_test = np.squeeze(np.array(y_test_a))\n",
    "# print(y_test.shape)\n",
    "# print(y_test)\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test, y_test, inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7htvZHbENgC7"
   },
   "source": [
    "### V5 unedited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK6jqei9Nf5W"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  # y_dict = {\n",
    "  #     \"q_pt\":y_diff[0]*y_diff[0],\n",
    "  #     \"phi\":y_diff[0]*y_diff[0]\n",
    "  # }\n",
    "  # y_diff[0] / invcov[0][0]\n",
    "  return y_loss\n",
    "\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = x_train[2][0]\n",
    "# y_test = model.predict([x_train[0][0:1],x_train[1][0:1],x_train[2][0:1]])\n",
    "# y_test = np.squeeze(y_test)\n",
    "# inconv_test = x_train[1][0]\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "# # print('loss shape: '    + str(loss.shape)    )\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4XBVGilP4tc"
   },
   "source": [
    "### V6 New Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsGirPS0P4jb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "def customLoss(m_invcov):\n",
    "  def customLoss_fn(y_true, y_pred):\n",
    "    batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "    y_pred = tf.cast(K.reshape(y_pred, (batch_size, 5,1)),\"float64\") # y_pred  shape is now (batch, 5,1)\n",
    "    y_true = tf.cast(K.reshape(y_true, (batch_size, 5,1)),\"float64\") # y_state shape is now (batch, 5,1)\n",
    "    invcov = tf.cast(K.reshape(m_invcov, (batch_size, 5,5)),\"float64\") # invcov  shape is now (batch, 5,5)\n",
    "    \n",
    "    # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "    invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "    \n",
    "    # Difference between prediction and true state vectors\n",
    "    y_diff = y_pred - y_true\n",
    "\n",
    "    # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "    y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "    y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "    y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "    y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "    return y_loss\n",
    "  return customLoss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkuUUNJWjM_i"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "\n",
    "  y_pred = tf.cast(K.reshape(y_pred, (batch_size, 5,1)),\"float64\") # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = tf.cast(K.reshape(y_true, (batch_size, 5,1)),\"float64\") # y_state shape is now (batch, 5,1)\n",
    "  invcov = tf.cast(K.reshape(invcov, (batch_size, 5,5)),\"float64\") # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqNp5hG98yPr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DBknRRcl6LMm"
   ],
   "name": "CopyOfRNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
