{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te6rdZgT5yAW"
   },
   "source": [
    "# Copy of Previous RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdsGeayZ523e"
   },
   "source": [
    "### Reminders:\n",
    "\n",
    "Read up on some of these:\n",
    "- https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/\n",
    "- https://fairyonice.github.io/Stateful-LSTM-model-training-in-Keras.html \n",
    "- https://github.com/keras-team/keras/issues/5714\n",
    "- https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/\n",
    "\n",
    "###Shuffle Data!!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ro1oh9-25xM9",
    "outputId": "e2638721-dec3-4b6c-9db0-df28d7fce705"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yOaxkJ58hh"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5Lupm0J6Hre"
   },
   "source": [
    "## Do Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gE8NHB9v6IDc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "from math import isnan\n",
    "from scipy.stats import norm\n",
    "\n",
    "pandas.set_option('display.max_columns', None)\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yhg_bSj36CSt"
   },
   "source": [
    "## Obtain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1LMf25vB5_vs"
   },
   "outputs": [],
   "source": [
    "# read data from drive\n",
    "# csv_train = pandas.read_csv(\"drive/MyDrive/first_10k.csv\")\n",
    "csv_train = pandas.read_csv(\"first_10k.csv\")\n",
    "# csv_train = pandas.read_csv(\"drive/MyDrive/FDC_tracks.csv\")\n",
    "unparsed_train = np.array(csv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L54wTFh-NGU"
   },
   "source": [
    "## Ragged Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KoN7P86yCif6",
    "outputId": "b9ad9880-a607-4ac8-dd65-3e3a802f4a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.588573718740314, 47.716949659038654, 47.92074125234375, 0.0433333333333333, 96.0, 24.0, 343.50521373311966, 274.8641357421875, 3.24600255369344e-07, 137.76808719278492]\n",
      "[-47.69482374702719, -47.64816097490714, -47.78196688638672, 0.0100725857504721, 1.0, 1.0, 176.8656847042481, -77.76896667480467, 7.068405941829982e-10, 0.2999999999999999]\n",
      "[53.93938010927168, 182.16740477086037, 20.384216393930128, 80.4857546970404]\n",
      "[-59.97950523099952, -113.91363367061092, -21.351611455494343, -101.41199494730319]\n"
     ]
    }
   ],
   "source": [
    "_max = [-1000 for i in range(10)]\n",
    "_min = [1000 for i in range(10)]\n",
    "for index,event in enumerate(unparsed_train):\n",
    "  lower = 67\n",
    "  for upper in range(lower+14, event.shape[0]+1, 14):\n",
    "    d = event[lower:upper]\n",
    "    d = np.append(d[:2],d[6:])\n",
    "    for a in range(len(d)):\n",
    "      if d[a] > _max[a]:\n",
    "        _max[a] = d[a]\n",
    "      if d[a] < _min[a]:\n",
    "        _min[a] = d[a]\n",
    "    lower = upper\n",
    "print(_max)\n",
    "print(_min)\n",
    "\n",
    "_TOF_max = [-1000 for i in range(4)]\n",
    "_TOF_min = [1000 for i in range(4)]\n",
    "for index,event in enumerate(unparsed_train):\n",
    "  TOF = event[59:67]\n",
    "  # print(TOF)\n",
    "  for a in range(4):\n",
    "    if TOF[a*2] > _TOF_max[a]:\n",
    "      _TOF_max[a] = TOF[a*2]\n",
    "    if TOF[a*2] < _TOF_min[a]:\n",
    "      _TOF_min[a] = TOF[a*2]\n",
    "print(_TOF_max)\n",
    "print(_TOF_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOfDY5F8yJyl"
   },
   "source": [
    "After trying out many different possibilities, I have found making a RaggedTensor is the way to make variable timesteps.\n",
    "\n",
    "Once you create a RaggedTensor ONLY FOR X DATA, you need to also add:\n",
    "\n",
    "ragged=True\n",
    "\n",
    "to the keras.Inputs() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BfACUWvW-NGd"
   },
   "outputs": [],
   "source": [
    "def ragged_parser(unparsed):\n",
    "  global _min, _max\n",
    "  x_final = []\n",
    "  y_final = []\n",
    "  invCov_final = []\n",
    "  cov_final = []\n",
    "  for event in unparsed:\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    nEvent = event[0]     # all the data split into neat little arrays...\n",
    "    state = event[1:6]\n",
    "    coVar = event[6:31]\n",
    "    invCoVar = event[31:56]\n",
    "    goodnessOfFit = event[56:59]\n",
    "    TOF = event[59:67]\n",
    "\n",
    "    if goodnessOfFit[2] > 0.1:   # Cutting if rms is too high\n",
    "      continue\n",
    "    hits = []\n",
    "    lower = 67\n",
    "    for upper in range(lower+14, event.shape[0]+1, 14): # to flip just go from end to 67 by -14 steps?\n",
    "\n",
    "      # hasNAN = False\n",
    "      # for val in event[lower:upper]:\n",
    "      #   if isnan(val):\n",
    "      #     hasNAN = True\n",
    "      # if not hasNAN:\n",
    "\n",
    "      if not isnan(event[lower]):            # Check if we are done with hits, because data is cut short, the rest will be nan\n",
    "        hit_data = event[lower:upper]                      # retrieving the hit\n",
    "        hit_data = np.append(hit_data[:2],hit_data[6:])    # cutting out the sin and cos data\n",
    "        for z in range(len(hit_data)):\n",
    "          hit_data[z] = (hit_data[z] - _min[z]) / (_max[z] - _min[z])    # we need to normalize the data; this can be moved to a lambda layer in the network if needed.\n",
    "        for i_TOF in range(4):\n",
    "          TOF[i_TOF*2] = (TOF[i_TOF*2] - _TOF_min[i_TOF]) / (_TOF_max[i_TOF] - _TOF_min[i_TOF])\n",
    "        hit_data = np.append(hit_data,TOF)\n",
    "        hits.append(np.ndarray.tolist(hit_data))       # we want it as a list to convert to RaggedTensor later; last time I checked it didnt work with array.\n",
    "      lower = upper\n",
    "    for i in range(len(hits)):   # this could be simplified to just: \"x = hits\" if im not mistaken...\n",
    "      x.append(hits[i])          # however we might need to add y.append(hits[i+1]) for later testing so leaving it like this for now...\n",
    "    y = np.ndarray.tolist(state)   # technically not needed, can be removed later... at first I thought i need to pass RaggedTensor labels, but that is not the case.\n",
    "    x_final.append(x)          # want x_final to be shape (event, hit, 10) as a list\n",
    "    y_final.append(y)          # want y_final to be shape (event, 5)       as a np.array\n",
    "    # y_final.append(y[0])          # want y_final to be shape (event, 5)       as a np.array\n",
    "    invCov_final.append(invCoVar[:])  # want other_f to be shape (event, 25)      as a np.array\n",
    "    cov_final.append(coVar[:])\n",
    "  x_final = tf.ragged.constant(x_final)   # convert list to RaggedTensor because timesteps (number of hits) are variable between events\n",
    "  y_final = np.array(y_final)\n",
    "  invCov_final = np.array(invCov_final)\n",
    "  cov_final = np.array(cov_final)\n",
    "  return [x_final, invCov_final, cov_final, y_final], y_final   # with the custom loss the x_train (input) needs to be a list of [inputs, inverseCovariance, labels]\n",
    "  # return [x_final, invCov_final], y_final   # with the custom loss the x_train (input) needs to be a list of [inputs, inverseCovariance, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7H0PK6Q-NGg",
    "outputId": "6fcb6671-f32c-402a-c1a3-8489231a8d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--==Types==--\n",
      "--x_train:--\n",
      "\n",
      "  -> input_data: x_train[0]\n",
      "  -> type expected: RaggedTensor\n",
      " <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'>\n",
      "\n",
      "  -> invCov: x_train[1]\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "  -> y_train: x_train[2]\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "--y_train:--\n",
      "\n",
      "  -> y_train: y_train\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "--==Shapes==--\n",
      "--x_train:--  \n",
      "num of events: 9301\n",
      "\n",
      "  RaggedTensor | Input:  shape = (9301, 18, 18)\n",
      "  np.array     | InvCov: shape = (9301, 25)\n",
      "  np.array     | Labels: shape = (9301, 25)\n",
      "\n",
      "--x_train:--\n",
      "  np.array     | Labels: shape = (9301, 5)\n",
      "x_train : <tf.RaggedTensor [[0.7692640423774719, 0.765606164932251, 0.22407019138336182, 0.2642019987106323, 0.49473685026168823, 0.9130434989929199, 0.9742012023925781, 0.8189674615859985, 0.006027945317327976, 0.006027945317327976, 0.5265106558799744, 0.0, 0.4158459007740021, 1.0, 0.511589527130127, 0.0, 0.5575219988822937, 0.0], [0.6926482319831848, 0.5690751671791077, 0.3645309507846832, 0.05952613055706024, 0.2631579041481018, 0.8695651888847351, 0.9611853361129761, 0.28649309277534485, 0.033259935677051544, 0.033259935677051544, 0.5311324596405029, 0.0, 0.3861425220966339, 1.0, 0.5238472819328308, 0.0, 0.5605869889259338, 0.0], [0.23875504732131958, 0.24075621366500854, 0.7672280669212341, 0.10457572340965271, 0.5052631497383118, 0.782608687877655, 0.9352953433990479, 0.504021406173706, 0.018304066732525826, 0.018304066732525826, 0.5311729907989502, 0.0, 0.38604220747947693, 1.0, 0.5241410136222839, 0.0, 0.5606038570404053, 0.0], [0.33787238597869873, 0.4484703540802002, 0.6097946166992188, 0.07363839447498322, 0.7157894968986511, 0.739130437374115, 0.7676599621772766, 0.33562812209129333, 0.02666318602859974, 0.02666318602859974, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241480469703674, 0.0, 0.5606039762496948, 0.0], [0.5710769891738892, 0.6744455695152283, 0.37305063009262085, 0.288280189037323, 0.7052631378173828, 0.695652186870575, 0.7548211812973022, 0.7309820652008057, 0.005347346421331167, 0.005347346421331167, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.733315110206604, 0.7239073514938354, 0.263613760471344, 0.041055768728256226, 0.4842105209827423, 0.6521739363670349, 0.7418311238288879, 0.5430606007575989, 0.048400089144706726, 0.048400089144706726, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6543219089508057, 0.5472460389137268, 0.39544129371643066, 0.12399385124444962, 0.2947368323802948, 0.6086956262588501, 0.728915810585022, 0.5731130242347717, 0.015151274390518665, 0.015151274390518665, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.42824169993400574, 0.3272317349910736, 0.6255077123641968, 0.04272006079554558, 0.3052631616592407, 0.5652173757553101, 0.7160916924476624, 0.30231329798698425, 0.046525269746780396, 0.046525269746780396, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.2750035524368286, 0.28259047865867615, 0.7278001308441162, 0.024007415398955345, 0.5157894492149353, 0.52173912525177, 0.703081488609314, 0.3300199508666992, 0.08132698386907578, 0.08132698386907578, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.3927527666091919, 0.47597992420196533, 0.5672175288200378, 0.08297774195671082, 0.6631578803062439, 0.47826087474823, 0.4162442684173584, 0.5024588704109192, 0.023499751463532448, 0.023499751463532448, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.567786455154419, 0.6398308277130127, 0.392142653465271, 0.030097603797912598, 0.6421052813529968, 0.43478259444236755, 0.40303486585617065, 0.2945617437362671, 0.06557145714759827, 0.06557145714759827, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6758876442909241, 0.6613656878471375, 0.3252999782562256, 0.05598178505897522, 0.4736842215061188, 0.3913043439388275, 0.39034584164619446, 0.45713284611701965, 0.035419683903455734, 0.035419683903455734, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6067799925804138, 0.5218054056167603, 0.433348685503006, 0.06648389250040054, 0.3368421196937561, 0.3478260934352875, 0.3770085275173187, 0.44806209206581116, 0.029668668285012245, 0.029668668285012245, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.33219149708747864, 0.3447727859020233, 0.6660985350608826, 0.02286168746650219, 0.5263158082962036, 0.260869562625885, 0.35123491287231445, 0.42745885252952576, 0.085147425532341, 0.085147425532341, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.436598539352417, 0.4929949641227722, 0.5362541675567627, 0.03817495331168175, 0.6105263233184814, 0.21739129722118378, 0.0648939236998558, 0.2867984175682068, 0.052010804414749146, 0.052010804414749146, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6153600811958313, 0.6007120013237, 0.38774821162223816, 0.04457264766097069, 0.4736842215061188, 0.1304347813129425, 0.038748566061258316, 0.4948866367340088, 0.04459531232714653, 0.04459531232714653, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.45056793093681335, 0.40904945135116577, 0.5723205208778381, 0.04106782376766205, 0.42105263471603394, 0.043478261679410934, 0.012867040000855923, 0.33683282136917114, 0.048385992646217346, 0.048385992646217346, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.3925012946128845, 0.4053120017051697, 0.6041624546051025, 0.032487500458955765, 0.5263158082962036, 0.0, 0.00017478904919698834, 0.65003901720047, 0.060900986194610596, 0.060900986194610596, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0]]>\n",
      "y_train : [-4.805 -1.959  9.798  0.297 77.505]\n"
     ]
    }
   ],
   "source": [
    "# split = 450000\n",
    "# split = 3\n",
    "# split = int(unparsed_train.shape[0]*0.90)  # use 1000 for validation with full dataset\n",
    "x_train, y_train = ragged_parser(unparsed_train)\n",
    "# x_train, y_train = ragged_parser(unparsed_train[:split])\n",
    "# x_test , y_test  = ragged_parser(unparsed_train[split:split+100])\n",
    "\n",
    "print(\"--==Types==--\")\n",
    "print(\"--x_train:--\")\n",
    "print(\"\\n  -> input_data: x_train[0]\\n  -> type expected: RaggedTensor\\n \"+str(type(x_train[0])))\n",
    "print(\"\\n  -> invCov: x_train[1]\\n  -> type expected: np.array\\n \"+str(type(x_train[1])))\n",
    "print(\"\\n  -> y_train: x_train[2]\\n  -> type expected: np.array\\n \"+str(type(x_train[2])))\n",
    "print(\"\\n--y_train:--\")\n",
    "print(\"\\n  -> y_train: y_train\\n  -> type expected: np.array\\n \"+str(type(y_train)))\n",
    "\n",
    "print(\"\\n\\n--==Shapes==--\")\n",
    "print(\"--x_train:--  \\nnum of events: \" + str(x_train[0].shape[0]))\n",
    "print(\"\\n  RaggedTensor | Input:  shape = \" + \"(\" + str(x_train[0].shape[0]) + \", \"+ str(x_train[0][0].shape[0]) + \", \"+ str(x_train[0][0][0].shape[0]) + \")\")\n",
    "print(\"  np.array     | InvCov: shape = \" + str(x_train[1].shape))\n",
    "print(\"  np.array     | Labels: shape = \" + str(x_train[2].shape))\n",
    "print(\"\\n--x_train:--\")\n",
    "print(\"  np.array     | Labels: shape = \" + str(y_train.shape))\n",
    "\n",
    "print(\"x_train : \" + str(x_train[0][0]))\n",
    "print(\"y_train : \" + str(y_train[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMvnihIl6ail"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdreWH016c08"
   },
   "source": [
    "## Defining Models\n",
    "\n",
    "- model\n",
    "  - Very basic testing RNN model\n",
    "  - Output every timestep\n",
    "\n",
    "- model_timeless\n",
    "  - Very basic testing RNN model\n",
    "  - Output only at the end\n",
    "\n",
    "- RNNTime\n",
    "  - Advanced\n",
    "  - Time distributed, output every timestep\n",
    "\n",
    "- RNNTimeless\n",
    "  - Advanced\n",
    "  - Only output at final layer\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e8qCwtNw6bFk"
   },
   "outputs": [],
   "source": [
    "def model(x):\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='input_lstm1', return_sequences=True)(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(32, activation='relu'), name=\"TD1-Dense\")(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(14, activation='linear'), name=\"output-Dense\")(x)\n",
    "  return x\n",
    "def model_timeless(x):\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='input_lstm1', return_sequences=False)(x)\n",
    "  x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "  x = keras.layers.Dense(5, activation='relu', name=\"output-Dense\")(x)\n",
    "  return x\n",
    "\n",
    "def RNNTime(x):\n",
    "  x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(32,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(32, activation='relu'), name=\"TD1-Dense\")(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(5, activation='linear'), name=\"output-Dense\")(x)\n",
    "  return x\n",
    "\n",
    "def RNNTimeless(x):\n",
    "  x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=False)(x)\n",
    "  x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "  x = keras.layers.Dense(1, activation='linear', name=\"output-Dense\")(x)\n",
    "  # x = keras.layers.Dense(5, activation='linear', name=\"output-Dense\")(x)\n",
    "  # x = keras.layers.lambda(# normalize)\n",
    "  return x\n",
    "\n",
    "\n",
    "# def RNNTimeless(x):\n",
    "#   x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "#   x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "#   x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=False)(x)\n",
    "#   x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "#   x = keras.layers.Dense(5, activation='linear', name=\"output-Dense\")(x)\n",
    "#   # x = keras.layers.lambda(# normalize)\n",
    "#   return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtf37tFTB5HU"
   },
   "source": [
    "## Custom Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOZIj6R8u0VG"
   },
   "source": [
    "### V1 Originial, unedited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v8iA9da3HLBs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_loss\n",
    "\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = x_train[2][0]\n",
    "# y_test = model.predict([x_train[0][0:1],x_train[1][0:1],x_train[2][0:1]])\n",
    "# y_test = np.squeeze(y_test)\n",
    "# inconv_test = x_train[1][0]\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXxfMKTqTvBC"
   },
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZX3nAlhgTu4h"
   },
   "outputs": [],
   "source": [
    "def customMetric(y_true, y_pred, cov, id=0):\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  cov = K.reshape(cov, (batch_size, 5,5)) # cov  shape is now (batch, 5,5)\n",
    "  # cov = tf.transpose(cov, perm=[0,2,1])     # cov shape is now (batch, 5,5)\n",
    "  y_diff = y_pred[:,id] - y_true[:,id]\n",
    "  # y_diff = K.reshape(y_diff, (batch_size,1))\n",
    "  cov = K.reshape(cov[:,id,id], (batch_size,1))\n",
    "  # print(\"diff:\\n\",y_diff)\n",
    "  print(\"cov:\\n\",cov)\n",
    "  # return (y_diff*y_diff)/(cov[:,id,id])\n",
    "  return tf.math.square(y_diff)/(cov)\n",
    "\n",
    "# ccov = x_train[2][0:6]\n",
    "# ccov = np.reshape(ccov, (6,5,5))\n",
    "\n",
    "# print(ccov)\n",
    "\n",
    "# metric = K.eval(customMetric(y_train[0:6],y_train[1:7],x_train[2][0:6],3))\n",
    "# print(\"metric: \\n\",metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qw41A73h-zGB"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZ7d8PMx652m",
    "outputId": "aab903f4-7785-4519-ef41-4067f641d5e8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d8ace5f5b9e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# model.compile(optimizer=optimizer, loss=\"mse\", loss_weights=loss_weights, metrics=[\"mae\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0minput_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0minput_incov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# --==Not in use?==--\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.8)\n",
    "from keras.layers import Dense\n",
    "\n",
    "# nInput = 10\n",
    "nInput = 18\n",
    "\n",
    "# --==Set seed to get identical results==-- begin\n",
    "# from tensorflow.random import set_seed\n",
    "# np.random.seed(1)\n",
    "# set_seed(2)\n",
    "# --==Set seed to get identical results==-- end\n",
    "\n",
    "#--==Set Weights==--\n",
    "# loss_weights = [1/(sd**2)]\n",
    "# loss_weights = np.array(loss_weights)/sum(loss_weights)\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\", loss_weights=loss_weights, metrics=[\"mae\"])\n",
    "\n",
    "inputs = keras.Input((None,nInput))\n",
    "input_true = keras.Input((5,))\n",
    "input_incov = keras.Input((25,))\n",
    "input_cov_f = keras.Input((25,))\n",
    "all_inputs = [inputs, input_incov, input_cov_f, input_true]\n",
    "# all_inputs = [inputs, input_incov, input_true]\n",
    "\n",
    "# --==Choose model==--\n",
    "# x = model(inputs)\n",
    "# x = model_timeless(inputs)\n",
    "# x = RNNTime(inputs)\n",
    "x = RNNTimeless(inputs)\n",
    "# x = RNNTimeStateful(inputs)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001) # lower, less of those spikes\n",
    "\n",
    "# outs = {\n",
    "#     \"q_pt\":Dense(1, name=\"q_pt\")(x),\n",
    "#     \"phi\":Dense(1, name=\"phi\")(x),\n",
    "#     \"tanl\":Dense(1, name=\"tanl\")(x),\n",
    "#     \"D\":Dense(1, name=\"D\")(x),\n",
    "#     \"z\":Dense(1, name=\"z\")(x)\n",
    "# }\n",
    "\n",
    "# y_dict = {\n",
    "#     \"q_pt\":y_train[:,0],\n",
    "#     \"phi\":y_train[:,1],\n",
    "#     \"tanl\":y_train[:,2],\n",
    "#     \"D\":y_train[:,3],\n",
    "#     \"z\":y_train[:,4]\n",
    "# }\n",
    "\n",
    "# model = keras.Model(inputs=all_inputs, outputs=outs, name=\"RNNModel\")\n",
    "\n",
    "model = keras.Model(inputs=all_inputs, outputs=x, name=\"RNNModel\")\n",
    "\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 0),name=\"q_pt\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 1),name=\"phi\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 2),name=\"tanl\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 3),name=\"D\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 4),name=\"z\")\n",
    "\n",
    "# model.add_loss(customLoss(input_true, x, input_incov))\n",
    "# model.compile(loss=None, optimizer=optimizer, metrics=[\"mae\"])\n",
    "\n",
    "model.add_metric(customMetric(input_true, x, input_cov_f, index))\n",
    "# try as loss\n",
    "\n",
    "# model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "model.compile(loss=None, optimizer=optimizer, metrics=[\"mae\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkjx1-6Gr76F"
   },
   "source": [
    "### Custom Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "t2pxxUfH7sjY"
   },
   "outputs": [],
   "source": [
    "def concat_hist(H1,H2):\n",
    "  H = {}\n",
    "  for i in H1.keys():\n",
    "    H[i] = list(np.append(np.array(H1[i]),np.array(H2[i])))\n",
    "  return H\n",
    "H = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjLh8Eh07HRW",
    "outputId": "e8312305-0acd-4240-c575-5b48161d1655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 1301.3196 - mae: 5.2196\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 1294.6191 - mae: 5.1012\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 1289.8618 - mae: 5.0806\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1284.3696 - mae: 5.0436\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 1280.4288 - mae: 5.0615\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 1276.4075 - mae: 5.0607\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 1271.9072 - mae: 5.0310\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 1268.8049 - mae: 5.0614\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 1271.6194 - mae: 5.2044\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 1269.7826 - mae: 5.5124\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1267.9020 - mae: 5.5935\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 1259.1145 - mae: 5.4141\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 1249.7775 - mae: 5.2823\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 1256.3385 - mae: 5.6855\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1263.4827 - mae: 5.9245\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 1262.1533 - mae: 6.1034\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1262.7585 - mae: 6.2758\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 1253.5753 - mae: 6.0256\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 1242.4972 - mae: 5.9046\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 1232.6204 - mae: 5.7973\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 1222.3500 - mae: 5.5338\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 1214.2302 - mae: 5.3876\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 1209.3909 - mae: 5.3149\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1203.0294 - mae: 5.2084\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 1197.2313 - mae: 5.2242\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1194.8066 - mae: 5.2237\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1187.9769 - mae: 5.1486\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1180.7880 - mae: 5.0344\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 1173.0320 - mae: 4.9240\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 1165.5189 - mae: 4.7947\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1159.9691 - mae: 4.7007\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1154.9358 - mae: 4.6745\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1151.7808 - mae: 4.7401\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1148.2203 - mae: 4.6924\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 1147.1222 - mae: 4.8253\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1143.6901 - mae: 4.8992\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 1140.3485 - mae: 4.9374\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 1135.0591 - mae: 4.9013\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 1133.7806 - mae: 4.9485\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1128.3523 - mae: 4.9750\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 1244.3784 - mae: 7.5842\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 1399.3584 - mae: 10.1969\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 2808.6091 - mae: 16.5574\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 3428.4106 - mae: 19.2563\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 2947.4922 - mae: 18.4599\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 2209.4663 - mae: 15.5170\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 1953.5968 - mae: 13.6696\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 3082.5535 - mae: 18.3049\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 2276.0361 - mae: 15.3769\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 1737.9944 - mae: 12.6496\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 1556.3373 - mae: 11.3326\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1430.4581 - mae: 10.5254\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 1342.6935 - mae: 9.7990\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 1302.2501 - mae: 9.3503\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 1270.4878 - mae: 8.9873\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 1224.6416 - mae: 8.5595\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 1197.2277 - mae: 8.1611\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 1177.1311 - mae: 7.8514\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 1160.9797 - mae: 7.6241\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 3s 78ms/step - loss: 1147.7642 - mae: 7.3656\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 1142.9423 - mae: 7.3656\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 1125.3362 - mae: 7.0686\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 1113.5453 - mae: 6.8737\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 1104.3934 - mae: 6.7255\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1093.6782 - mae: 6.5996\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 1084.5647 - mae: 6.4490\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 1076.7543 - mae: 6.3491\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 1068.2295 - mae: 6.2529\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 1061.6379 - mae: 6.1738\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1054.3405 - mae: 6.0497\n",
      "Epoch 71/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 1050.5220 - mae: 6.0966\n",
      "Epoch 72/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1044.3077 - mae: 5.9483\n",
      "Epoch 73/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 1033.3237 - mae: 5.7864\n",
      "Epoch 74/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 1026.0122 - mae: 5.7201\n",
      "Epoch 75/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 1021.4744 - mae: 5.6018\n",
      "Epoch 76/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 1013.6577 - mae: 5.5548\n",
      "Epoch 77/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 1007.4371 - mae: 5.4489\n",
      "Epoch 78/300\n",
      "37/37 [==============================] - 4s 95ms/step - loss: 1001.6953 - mae: 5.4170\n",
      "Epoch 79/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 999.3648 - mae: 5.4130\n",
      "Epoch 80/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 993.1224 - mae: 5.3902\n",
      "Epoch 81/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 986.4893 - mae: 5.3035\n",
      "Epoch 82/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 983.1010 - mae: 5.2922\n",
      "Epoch 83/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 975.5434 - mae: 5.2144\n",
      "Epoch 84/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 969.6463 - mae: 5.1214\n",
      "Epoch 85/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 963.7687 - mae: 5.0569\n",
      "Epoch 86/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 960.7429 - mae: 5.0105\n",
      "Epoch 87/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 953.9540 - mae: 4.9560\n",
      "Epoch 88/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 949.2537 - mae: 4.9143\n",
      "Epoch 89/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 944.6455 - mae: 4.8911\n",
      "Epoch 90/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 943.0262 - mae: 4.8986\n",
      "Epoch 91/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 934.1556 - mae: 4.8491\n",
      "Epoch 92/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 928.7396 - mae: 4.7767\n",
      "Epoch 93/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 924.4470 - mae: 4.7812\n",
      "Epoch 94/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 919.6950 - mae: 4.7626\n",
      "Epoch 95/300\n",
      "37/37 [==============================] - 3s 76ms/step - loss: 914.0554 - mae: 4.7261\n",
      "Epoch 96/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 908.8256 - mae: 4.6547\n",
      "Epoch 97/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 903.2373 - mae: 4.6253\n",
      "Epoch 98/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 898.2396 - mae: 4.5699\n",
      "Epoch 99/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 894.6234 - mae: 4.6132\n",
      "Epoch 100/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 889.3125 - mae: 4.5713\n",
      "Epoch 101/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 885.1204 - mae: 4.5571\n",
      "Epoch 102/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 880.9161 - mae: 4.5450\n",
      "Epoch 103/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 896.3311 - mae: 4.8511\n",
      "Epoch 104/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 898.7153 - mae: 5.4958\n",
      "Epoch 105/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 894.6961 - mae: 5.3874\n",
      "Epoch 106/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 882.5498 - mae: 5.3101\n",
      "Epoch 107/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 871.1953 - mae: 5.0339\n",
      "Epoch 108/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 861.7098 - mae: 4.8328\n",
      "Epoch 109/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 857.0312 - mae: 4.7800\n",
      "Epoch 110/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 850.7936 - mae: 4.6957\n",
      "Epoch 111/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 844.2757 - mae: 4.6556\n",
      "Epoch 112/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 837.7619 - mae: 4.5586\n",
      "Epoch 113/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 833.7537 - mae: 4.5741\n",
      "Epoch 114/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 828.9202 - mae: 4.5436\n",
      "Epoch 115/300\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 825.9989 - mae: 4.6027\n",
      "Epoch 116/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 821.1058 - mae: 4.5968\n",
      "Epoch 117/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 815.2146 - mae: 4.5295\n",
      "Epoch 118/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 811.0018 - mae: 4.5697\n",
      "Epoch 119/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 805.4792 - mae: 4.4910\n",
      "Epoch 120/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 805.0214 - mae: 4.5880\n",
      "Epoch 121/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 800.6898 - mae: 4.6046\n",
      "Epoch 122/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 793.2785 - mae: 4.4800\n",
      "Epoch 123/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 788.2665 - mae: 4.4740\n",
      "Epoch 124/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 784.8269 - mae: 4.5423\n",
      "Epoch 125/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 780.6553 - mae: 4.4778\n",
      "Epoch 126/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 775.4764 - mae: 4.4764\n",
      "Epoch 127/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 773.1324 - mae: 4.5600\n",
      "Epoch 128/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 768.0958 - mae: 4.5912\n",
      "Epoch 129/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 765.8077 - mae: 4.7168\n",
      "Epoch 130/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 760.0916 - mae: 4.6393\n",
      "Epoch 131/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 753.3974 - mae: 4.5391\n",
      "Epoch 132/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 759.9255 - mae: 4.6714\n",
      "Epoch 133/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 765.5453 - mae: 5.0296\n",
      "Epoch 134/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 761.4407 - mae: 5.2363\n",
      "Epoch 135/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 750.7749 - mae: 5.1013\n",
      "Epoch 136/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 753.2905 - mae: 5.2757\n",
      "Epoch 137/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 737.4656 - mae: 4.9498\n",
      "Epoch 138/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 729.5714 - mae: 4.8673\n",
      "Epoch 139/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 721.4918 - mae: 4.6777\n",
      "Epoch 140/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 718.2702 - mae: 4.7587\n",
      "Epoch 141/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 711.9631 - mae: 4.6492\n",
      "Epoch 142/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 706.7924 - mae: 4.6448\n",
      "Epoch 143/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 702.3526 - mae: 4.5061\n",
      "Epoch 144/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 697.1407 - mae: 4.5569\n",
      "Epoch 145/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 691.6324 - mae: 4.4869\n",
      "Epoch 146/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 686.7429 - mae: 4.4161\n",
      "Epoch 147/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 679.3270 - mae: 4.2492\n",
      "Epoch 148/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 676.9661 - mae: 4.2644\n",
      "Epoch 149/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 674.3248 - mae: 4.3588\n",
      "Epoch 150/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 672.7319 - mae: 4.4766\n",
      "Epoch 151/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 674.8528 - mae: 4.6891\n",
      "Epoch 152/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 670.9987 - mae: 4.8445\n",
      "Epoch 153/300\n",
      "37/37 [==============================] - 3s 75ms/step - loss: 671.6222 - mae: 4.9169\n",
      "Epoch 154/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 659.8849 - mae: 4.7612\n",
      "Epoch 155/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 653.1830 - mae: 4.5973\n",
      "Epoch 156/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 650.5591 - mae: 4.5998\n",
      "Epoch 157/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 648.5507 - mae: 4.6187\n",
      "Epoch 158/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 641.2694 - mae: 4.5326\n",
      "Epoch 159/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 632.6257 - mae: 4.4215\n",
      "Epoch 160/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 628.0201 - mae: 4.3031\n",
      "Epoch 161/300\n",
      "37/37 [==============================] - 3s 79ms/step - loss: 619.8752 - mae: 4.1325\n",
      "Epoch 162/300\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 615.0327 - mae: 4.1109\n",
      "Epoch 163/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 619.7257 - mae: 4.5062\n",
      "Epoch 164/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 610.4551 - mae: 4.3767\n",
      "Epoch 165/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 608.2144 - mae: 4.4115\n",
      "Epoch 166/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 613.5604 - mae: 4.7607\n",
      "Epoch 167/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 657.1982 - mae: 5.9510\n",
      "Epoch 168/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 676.3718 - mae: 6.8468\n",
      "Epoch 169/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 640.7184 - mae: 6.1193\n",
      "Epoch 170/300\n",
      "37/37 [==============================] - 3s 74ms/step - loss: 658.8952 - mae: 6.5533\n",
      "Epoch 171/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 661.7394 - mae: 6.8329\n",
      "Epoch 172/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 660.7490 - mae: 6.9821\n",
      "Epoch 173/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 638.1680 - mae: 6.5316\n",
      "Epoch 174/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 614.8284 - mae: 6.0276\n",
      "Epoch 175/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 603.7025 - mae: 5.7581\n",
      "Epoch 176/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 599.5322 - mae: 5.8771\n",
      "Epoch 177/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 602.6165 - mae: 5.8389\n",
      "Epoch 178/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 585.9791 - mae: 5.6624\n",
      "Epoch 179/300\n",
      "37/37 [==============================] - 3s 79ms/step - loss: 574.5494 - mae: 5.4413\n",
      "Epoch 180/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 565.4172 - mae: 5.1027\n",
      "Epoch 181/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 557.0399 - mae: 4.8803\n",
      "Epoch 182/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 552.7419 - mae: 4.7611\n",
      "Epoch 183/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 543.4094 - mae: 4.7888\n",
      "Epoch 184/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 548.7173 - mae: 5.0679\n",
      "Epoch 185/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 542.6885 - mae: 4.9967\n",
      "Epoch 186/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 533.7590 - mae: 4.8740\n",
      "Epoch 187/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 523.1358 - mae: 4.4523\n",
      "Epoch 188/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 519.9471 - mae: 4.3754\n",
      "Epoch 189/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 531.0735 - mae: 5.1949\n",
      "Epoch 190/300\n",
      "37/37 [==============================] - 3s 78ms/step - loss: 513.0140 - mae: 4.7520\n",
      "Epoch 191/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 499.7711 - mae: 4.2716\n",
      "Epoch 192/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 489.9588 - mae: 3.9439\n",
      "Epoch 193/300\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 485.0685 - mae: 3.8222\n",
      "Epoch 194/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 478.7874 - mae: 3.7073\n",
      "Epoch 195/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 473.1894 - mae: 3.5709\n",
      "Epoch 196/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 469.0423 - mae: 3.5164\n",
      "Epoch 197/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 465.8039 - mae: 3.5269\n",
      "Epoch 198/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 460.8698 - mae: 3.4968\n",
      "Epoch 199/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 457.0104 - mae: 3.4922\n",
      "Epoch 200/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 454.0134 - mae: 3.4627\n",
      "Epoch 201/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 449.0961 - mae: 3.4751\n",
      "Epoch 202/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 445.3980 - mae: 3.4798\n",
      "Epoch 203/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 441.7501 - mae: 3.4477\n",
      "Epoch 204/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 441.7245 - mae: 3.6914\n",
      "Epoch 205/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 438.7701 - mae: 3.7588\n",
      "Epoch 206/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 441.1812 - mae: 4.0174\n",
      "Epoch 207/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 441.3347 - mae: 4.2335\n",
      "Epoch 208/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 440.0314 - mae: 4.3835\n",
      "Epoch 209/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 437.7401 - mae: 4.4728\n",
      "Epoch 210/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 433.9566 - mae: 4.4023\n",
      "Epoch 211/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 427.9615 - mae: 4.3674\n",
      "Epoch 212/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 420.5162 - mae: 4.1560\n",
      "Epoch 213/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 416.9287 - mae: 4.1117\n",
      "Epoch 214/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 414.0187 - mae: 4.1687\n",
      "Epoch 215/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 412.9379 - mae: 4.2515\n",
      "Epoch 216/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 417.2003 - mae: 4.5917\n",
      "Epoch 217/300\n",
      "37/37 [==============================] - 4s 98ms/step - loss: 415.8368 - mae: 4.7053\n",
      "Epoch 218/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 407.8432 - mae: 4.5333\n",
      "Epoch 219/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 403.4325 - mae: 4.5051\n",
      "Epoch 220/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 406.3232 - mae: 4.7365\n",
      "Epoch 221/300\n",
      "37/37 [==============================] - 3s 74ms/step - loss: 416.6135 - mae: 5.1258\n",
      "Epoch 222/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 437.6074 - mae: 6.0354\n",
      "Epoch 223/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 466.9050 - mae: 6.7005\n",
      "Epoch 224/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 642.2708 - mae: 9.2253\n",
      "Epoch 225/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 1699.8823 - mae: 16.0792\n",
      "Epoch 226/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 5682.6396 - mae: 24.1603\n",
      "Epoch 227/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 4752.5054 - mae: 23.4400\n",
      "Epoch 228/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 3588.3081 - mae: 19.3015\n",
      "Epoch 229/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 2747.9792 - mae: 16.6040\n",
      "Epoch 230/300\n",
      "37/37 [==============================] - 3s 78ms/step - loss: 2606.1228 - mae: 15.9809\n",
      "Epoch 231/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 2352.3130 - mae: 14.0924\n",
      "Epoch 232/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 2054.6423 - mae: 12.3984\n",
      "Epoch 233/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1929.5109 - mae: 12.2492\n",
      "Epoch 234/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1639.9265 - mae: 11.0924\n",
      "Epoch 235/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 1651.4459 - mae: 11.3025\n",
      "Epoch 236/300\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 1625.3651 - mae: 11.3310\n",
      "Epoch 237/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 1326.3252 - mae: 10.5083\n",
      "Epoch 238/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 1124.5591 - mae: 9.7433\n",
      "Epoch 239/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 972.4733 - mae: 9.1729\n",
      "Epoch 240/300\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 818.7418 - mae: 8.6061\n",
      "Epoch 241/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 729.7524 - mae: 8.4472\n",
      "Epoch 242/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 649.4421 - mae: 8.4589\n",
      "Epoch 243/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 616.0594 - mae: 8.0844\n",
      "Epoch 244/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 474.1371 - mae: 8.5606\n",
      "Epoch 245/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 382.7624 - mae: 8.2879\n",
      "Epoch 246/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 256.4045 - mae: 7.5842\n",
      "Epoch 247/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 202.9732 - mae: 7.1270\n",
      "Epoch 248/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 155.1414 - mae: 6.7248\n",
      "Epoch 249/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 128.1619 - mae: 6.3994\n",
      "Epoch 250/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 112.1366 - mae: 6.1795\n",
      "Epoch 251/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 99.4258 - mae: 5.9543\n",
      "Epoch 252/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 90.2710 - mae: 5.7547\n",
      "Epoch 253/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 85.6994 - mae: 5.6591\n",
      "Epoch 254/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 77.8799 - mae: 5.5401\n",
      "Epoch 255/300\n",
      "37/37 [==============================] - 4s 96ms/step - loss: 73.0713 - mae: 5.4121\n",
      "Epoch 256/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 67.7908 - mae: 5.2993\n",
      "Epoch 257/300\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 63.9499 - mae: 5.1833\n",
      "Epoch 258/300\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 59.9245 - mae: 5.0904\n",
      "Epoch 259/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 56.6413 - mae: 4.9956\n",
      "Epoch 260/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 53.9620 - mae: 4.8823\n",
      "Epoch 261/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 51.5299 - mae: 4.8499\n",
      "Epoch 262/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 48.6234 - mae: 4.7363\n",
      "Epoch 263/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 46.9525 - mae: 4.6643\n",
      "Epoch 264/300\n",
      "37/37 [==============================] - 3s 88ms/step - loss: 46.0625 - mae: 4.6665\n",
      "Epoch 265/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 44.4022 - mae: 4.6194\n",
      "Epoch 266/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 43.9840 - mae: 4.5812\n",
      "Epoch 267/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 42.4387 - mae: 4.5445\n",
      "Epoch 268/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 42.7781 - mae: 4.5426\n",
      "Epoch 269/300\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 40.3993 - mae: 4.4534\n",
      "Epoch 270/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 40.5066 - mae: 4.4424\n",
      "Epoch 271/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 43.5033 - mae: 4.4972\n",
      "Epoch 272/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 41.1601 - mae: 4.4142\n",
      "Epoch 273/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 38.5745 - mae: 4.3611\n",
      "Epoch 274/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 36.0197 - mae: 4.2499\n",
      "Epoch 275/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 34.1300 - mae: 4.1334\n",
      "Epoch 276/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 36.7578 - mae: 4.2275\n",
      "Epoch 277/300\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 34.9919 - mae: 4.1751\n",
      "Epoch 278/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 34.1506 - mae: 4.1690\n",
      "Epoch 279/300\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 32.3032 - mae: 4.0659\n",
      "Epoch 280/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 31.1625 - mae: 3.9923\n",
      "Epoch 281/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 30.9570 - mae: 3.9570\n",
      "Epoch 282/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 31.9358 - mae: 4.0062\n",
      "Epoch 283/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 31.8103 - mae: 3.9625\n",
      "Epoch 284/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 32.4454 - mae: 3.9919\n",
      "Epoch 285/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 33.7199 - mae: 4.0193\n",
      "Epoch 286/300\n",
      "37/37 [==============================] - 3s 93ms/step - loss: 31.4200 - mae: 3.9439\n",
      "Epoch 287/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 29.2484 - mae: 3.8379\n",
      "Epoch 288/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 29.4961 - mae: 3.8408\n",
      "Epoch 289/300\n",
      "37/37 [==============================] - 3s 94ms/step - loss: 29.0605 - mae: 3.8380\n",
      "Epoch 290/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 28.0369 - mae: 3.7576\n",
      "Epoch 291/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 28.1298 - mae: 3.7817\n",
      "Epoch 292/300\n",
      "37/37 [==============================] - 3s 89ms/step - loss: 26.5852 - mae: 3.6942\n",
      "Epoch 293/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 26.2745 - mae: 3.6834\n",
      "Epoch 294/300\n",
      "37/37 [==============================] - 3s 91ms/step - loss: 25.5275 - mae: 3.6154\n",
      "Epoch 295/300\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 26.1595 - mae: 3.6322\n",
      "Epoch 296/300\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 27.0256 - mae: 3.6678\n",
      "Epoch 297/300\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 27.6491 - mae: 3.7048\n",
      "Epoch 298/300\n",
      "37/37 [==============================] - 3s 92ms/step - loss: 32.2092 - mae: 3.8351\n",
      "Epoch 299/300\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 37.5087 - mae: 3.9945\n",
      "Epoch 300/300\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 48.1430 - mae: 4.4484\n"
     ]
    }
   ],
   "source": [
    "H_t = []\n",
    "index = 0\n",
    "\n",
    "# Using custom loss and gen\n",
    "es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01, patience=25, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=300, validation_data=(x_test, y_test), verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=300, shuffle=True, verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train[:,2], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es])\n",
    "# H_t.append(model.fit(x=x_train, y=y_train[:,2], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es]).history)\n",
    "# H_t.append(model.fit(x=x_train, y=y_train[:,index], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es]).history)\n",
    "H_t.append(model.fit(x=x_train, y=y_train[:,index], batch_size=256, epochs=300, verbose=1, shuffle=True).history)\n",
    "if H == None:\n",
    "  H = H_t[-1]\n",
    "else:\n",
    "  H = concat_hist(H,H_t[-1])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=100, validation_data=test_gen, validation_steps=50, validation_batch_size=32, verbose=1)\n",
    "\n",
    "# Example how it kind of looks like\n",
    "# H = model.fit(x=[x_train, invCov, y_train], y=y_train, batch_size=64, epochs=100, verbose=1)\n",
    "\n",
    "# Overfit\n",
    "# es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=100, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=1, epochs=100, verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=1, epochs=100, verbose=1, validation_data=(x_test,y_test), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ5XBwDV7MGY"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 1\n",
    "final_loss = round(H[\"loss\"][-1],2)\n",
    "date = \"03-03\"\n",
    "def gen_name(m_type):\n",
    "    global model_index, index, date, final_loss\n",
    "    variable = [\"q_pt\",\"phi\",\"tanl\",\"D\",\"z\"][index]\n",
    "    m_str = \"models/\" + str(date) + \"-2021_\" + str(variable) + \"-\" + str(model_index) + \"_loss=\" + str(final_loss) + \".\" + str(m_type)\n",
    "    model_index+=1 # create a check for if file exists, for now just increment\n",
    "    return m_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xWvST6o27MYI"
   },
   "outputs": [],
   "source": [
    "# model.save('model.h5', save_format=\"h5\")\n",
    "# TODO check if file exists, increment counter\n",
    "# model.save('drive/MyDrive/Models/RealRNN_1-3-2021_141Ep_Onlytanl-2.h5', save_format=\"h5\")\n",
    "model.save(gen_name(\"h5\"), save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6LGBSia7fg3"
   },
   "source": [
    "## Graph loss and mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "OUPAStLMtq7m",
    "outputId": "d4bada3c-6f91-4a9c-b6d7-d7abe8cc1689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(H.keys())\n",
    "# print(\"loss: \", H[\"loss\"])\n",
    "# print(\"mae: \", H[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 0\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "ax[0].plot(H[\"loss\"][lim:])\n",
    "# ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "ax[0].legend([\"train\",\"val\"])\n",
    "ax[0].grid(True)\n",
    "\n",
    "\n",
    "ax[1].plot(H[\"mae\"][lim:])\n",
    "# ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "ax[1].legend([\"train\",\"val\"])\n",
    "ax[1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index -= 1\n",
    "plt.savefig(gen_name(\"png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_array = np.array(list(H.items()))\n",
    "save_array = np.array([an_array[0][1],an_array[1][1]])\n",
    "model_index -= 1\n",
    "np.save(gen_name(\"npy\"),save_array,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw4xzpuKYEld"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZHLFMS1YE40",
    "outputId": "09e30fe4-cc1b-444f-83b3-c74d1b923762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aay:   -4.80515106805654\n",
      "pred:  -4.6805277\n",
      "\n",
      "aay:   -5.880640826519476\n",
      "pred:  -7.8385115\n",
      "\n",
      "aay:   -13.532025643360104\n",
      "pred:  -8.280085\n",
      "\n",
      "aay:   -2.5098015385823422\n",
      "pred:  -5.797836\n",
      "\n",
      "aay:   -17.129251187238633\n",
      "pred:  -8.839087\n",
      "\n",
      "aay:   -51.73188484667799\n",
      "pred:  -113.66106\n",
      "\n",
      "aay:   -7.786843448307644\n",
      "pred:  -7.7952166\n",
      "\n",
      "aay:   -9.941927480382164\n",
      "pred:  -6.5259876\n",
      "\n",
      "aay:   -3.7463145395164834\n",
      "pred:  -7.564467\n",
      "\n",
      "aay:   -111.60526069232534\n",
      "pred:  -110.15316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True, precision=3)\n",
    "from time import sleep\n",
    "\n",
    "for i in range(10):\n",
    "  aax = x_train[0][i:i+1]\n",
    "  oax = x_train[1][i:i+1]\n",
    "  cax = x_train[2][i:i+1]\n",
    "  yax = x_train[3][i:i+1]\n",
    "  aay = y_train[i][index]\n",
    "  pred = model.predict([aax, oax, cax, yax])[0][0]\n",
    "  diff = pred - aay\n",
    "  # print(\"i: \",i)\n",
    "  # print(\"aax:  \",aax[0,0])\n",
    "  print(\"aay:  \",aay)\n",
    "  print(\"pred: \",pred)\n",
    "  # print(\"diff: \",diff)\n",
    "  print(\"\")\n",
    "\n",
    "  # plt.plot(aay[0])\n",
    "  # plt.plot(pred[0])\n",
    "  # plt.show()\n",
    "# [Q/PT, phi, tanl, D, z]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T56J0X6g7O2k"
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgLQh0jf7Ova"
   },
   "outputs": [],
   "source": [
    "# model.save('drive/MyDrive/RealRNN_1-2-2021_2.h5', save_format=\"h5\")\n",
    "model = keras.models.load_model('drive/MyDrive/Models/RealRNN_1-3-2021_300Ep_Onlyphi.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X72YH9S87cvW"
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H.keys())\n",
    "# print(\"loss: \", H[\"loss\"])\n",
    "# print(\"mae: \", H[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 0\n",
    "\n",
    "if len(H.keys()) > 4:\n",
    "  # fig, ax = plt.subplots(4,1,figsize=(5,20))\n",
    "  fig, ax = plt.subplots(3,1,figsize=(5,15))\n",
    "  fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "  ax[0].plot(H[\"loss\"][lim:])\n",
    "  # ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "  ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "  ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "  # ax[0].set_yscale(\"log\")\n",
    "  ax[0].legend([\"train\",\"val\"])\n",
    "  ax[0].grid(True)\n",
    "\n",
    "\n",
    "  ax[1].plot(H[\"mae\"][lim:])\n",
    "  # ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "  ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "  ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "  ax[1].legend([\"train\",\"val\"])\n",
    "  ax[1].grid(True)\n",
    "\n",
    "  ax[2].plot(H[\"q_pt\"][lim:])\n",
    "  ax[2].plot(H[\"phi\"][lim:])\n",
    "  ax[2].plot(H[\"tanl\"][lim:])\n",
    "  ax[2].plot(H[\"D\"][lim:])\n",
    "  ax[2].plot(H[\"z\"][lim:])\n",
    "  ax[2].set_title(\"data vs epoch\", fontsize=20)\n",
    "  ax[2].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[2].set_ylabel(\"data\", fontsize=15)\n",
    "  ax[2].legend([\"q_pt\",\"phi\",\"tanl\",\"D\",\"z\"])\n",
    "  # ax[2].legend([\"phi\",\"tanl\",\"D\",\"z\"])\n",
    "  # ax[2].legend([\"phi\",\"D\",\"z\"])\n",
    "  ax[2].grid(True)\n",
    "\n",
    "  # ax[3].plot(H.history[\"val_q_pt\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_phi\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_tanl\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_D\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_z\"][lim:])\n",
    "  # ax[3].set_title(\"data vs epoch\", fontsize=20)\n",
    "  # ax[3].set_xlabel(\"epoch\", fontsize=15)\n",
    "  # ax[3].set_ylabel(\"data\", fontsize=15)\n",
    "  # ax[3].legend([\"val_q_pt\",\"val_phi\",\"val_tanl\",\"val_D\",\"val_z\"])\n",
    "  # # ax[3].legend([\"val_phi\",\"val_tanl\",\"val_D\",\"val_z\"])\n",
    "  # # ax[3].legend([\"val_phi\",\"val_D\",\"val_z\"])\n",
    "  # ax[3].grid(True)\n",
    "\n",
    "else:\n",
    "  fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "  fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "  ax[0].plot(H[\"loss\"][lim:])\n",
    "  # ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "  ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "  ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "  # ax[0].set_yscale(\"log\")\n",
    "  ax[0].legend([\"train\",\"val\"])\n",
    "  ax[0].grid(True)\n",
    "\n",
    "\n",
    "  ax[1].plot(H[\"mae\"][lim:])\n",
    "  # ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "  ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "  ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "  ax[1].legend([\"train\",\"val\"])\n",
    "  ax[1].grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "k6XSgSit7dE3",
    "outputId": "b3a1caaf-1b21-4348-bfe7-1315ccb39171"
   },
   "outputs": [],
   "source": [
    "print(H.history.keys())\n",
    "print(\"loss: \", H.history[\"loss\"])\n",
    "print(\"mae: \", H.history[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 2\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "ax[0].plot(H.history[\"loss\"][lim:])\n",
    "# ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[0].legend([\"train\",\"val\"])\n",
    "ax[0].grid(True)\n",
    "\n",
    "\n",
    "ax[1].plot(H.history[\"mae\"][lim:])\n",
    "# ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "ax[1].legend([\"train\",\"val\"])\n",
    "ax[1].grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOi6MC8u7swr"
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqqb7Riq7tAY"
   },
   "outputs": [],
   "source": [
    "# Maybe copy over previous function and edit that?\n",
    "def graph(pred, true, diff):\n",
    "\n",
    "  values = [\"u\",\"v\",\"sin(v)\",\"cos(v)\",\"sin(u)\",\"cos(u)\",\"s\",\"ds\",\"wire\",\"glayer\",\"z\",\"time\",\"dE_amp\",\"q\"]\n",
    "  limits = [[\"todo\"]]\n",
    "\n",
    "  size = len(values)\n",
    "\n",
    "  fig, axs = plt.subplots(4,size,figsize=(size*5,20))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "\n",
    "  for i in range(size):\n",
    "    (mu, sigma) = norm.fit(diff[:,i])\n",
    "    print(\"data\" , values[i] ,\" |: mu: \", mu, \"sigma: \" , sigma)\n",
    "    _, bins, _ = axs[0,i].hist(diff[:,i], 20, density=True)\n",
    "    y = norm.pdf(bins, mu, sigma)\n",
    "    l = axs[0,i].plot(bins, y, 'r--', linewidth=2)\n",
    "\n",
    "    axs[0,i].set_title(values[i] + ' diff')\n",
    "    axs[0,i].set_ylabel('freq')\n",
    "    axs[0,i].set_xlabel(values[i] + ' diff')\n",
    "\n",
    "  #--------------------------------------\n",
    "  # PREDICTED VS TRUE\n",
    "  #--------------------------------------\n",
    "    \n",
    "  for i in range(size):\n",
    "    axs[1,i].scatter(true[:,i],pred[:,i])\n",
    "    axs[1,i].grid(True)\n",
    "\n",
    "    axs[1,i].set_title(values[i] + ' (predicted vs true)')\n",
    "    axs[1,i].set_ylabel('pred ' + values[i])\n",
    "    axs[1,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "    # axs[1,i].set_xlim(limits[i])\n",
    "    # axs[1,i].set_ylim(limits[i])\n",
    "    # axs[1,i].plot(limits[i],limits[i], color='b')\n",
    "\n",
    "  #--------------------------------------\n",
    "  # DIFFERENCE VS TRUE\n",
    "  #--------------------------------------\n",
    "\n",
    "  for i in range(size):\n",
    "    axs[2,i].scatter(true[:,i],diff[:,i])\n",
    "    l, r = axs[2,i].get_xlim()\n",
    "    axs[2,i].hlines(0, l, r)\n",
    "    axs[2,i].grid(True)\n",
    "\n",
    "    axs[2,i].set_title(values[i] + ' (difference vs true)')\n",
    "    axs[2,i].set_ylabel('diff ' + values[i])\n",
    "    axs[2,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "  #--------------------------------------\n",
    "  # DIFFERENCE VS TRUE 2D HIST\n",
    "  #--------------------------------------\n",
    "\n",
    "  for i in range(size):\n",
    "    axs[3,i].hist2d(true[:,i],diff[:,i],bins=20)\n",
    "\n",
    "    axs[2,i].set_title(values[i] + ' (difference vs true)')\n",
    "    axs[2,i].set_ylabel('diff ' + values[i])\n",
    "    axs[2,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "  fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHuUmXPCiXt9"
   },
   "outputs": [],
   "source": [
    "def gen_test_data(x_test, y_test, size=1000):\n",
    "  pred = model.predict(x_test)\n",
    "  diff = pred - y_test\n",
    "  return pred, y_test, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AMRhD6Wi7Xh"
   },
   "outputs": [],
   "source": [
    "graph(gen_test_data(x_test, y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAUyMneo7Xj1"
   },
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBsHjTgG7X2y"
   },
   "outputs": [],
   "source": [
    "# make test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWkYEQvFZ7HC"
   },
   "source": [
    "# Verification of proper data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqKxKcOQYEO4"
   },
   "source": [
    "## Using Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml3HHzoEaAhr"
   },
   "outputs": [],
   "source": [
    "aax, aay = next(train_gen)\n",
    "print(aax.shape)\n",
    "print(aay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv72llYMaUYd"
   },
   "outputs": [],
   "source": [
    "print(\"x\",aax[0])\n",
    "print(\"y\",aay[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcwDKKLLe079"
   },
   "source": [
    "## Non Genenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlU3tRGpYxQV",
    "outputId": "a24ded2d-d1d0-4555-84a9-428edf5b2e7d"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  aax = x_train[i]\n",
    "  aay = y_train[i]\n",
    "  # print(aax.shape)\n",
    "  # print(aay.shape)\n",
    "  # print(\"x\",aax)\n",
    "  print(\"y\",aay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCpP2wsfdeGl"
   },
   "source": [
    "## Graphs of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAxLbw94GYzm"
   },
   "source": [
    "### filter_ignore\n",
    "\n",
    "Filters out large and small values and graphs them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiUKfbtfAoLm"
   },
   "outputs": [],
   "source": [
    "def filter_ignore(var,min=None,max=None,bins=25,ylog=False,xlog=False,cut=True):\n",
    "  list_ignore = []\n",
    "\n",
    "  print(\"--== {} ==--\\n\".format(var))\n",
    "\n",
    "  largest = 0\n",
    "  smallest = 0\n",
    "  for i in range(len(csv_train[var])):\n",
    "    if csv_train[var][i] > csv_train[var][largest]:\n",
    "      largest = i\n",
    "    if csv_train[var][i] < csv_train[var][smallest]:\n",
    "      smallest = i\n",
    "  print(\"largest value:  ({}, {:.3f})\".format(largest,csv_train[var][largest]))\n",
    "  print(\"smallest value: ({}, {:.3f})\".format(smallest,csv_train[var][smallest]))\n",
    "\n",
    "  print(\"\")\n",
    "\n",
    "  if min:\n",
    "    for i in range(len(csv_train[var])):\n",
    "      if csv_train[var][i] < min:\n",
    "        list_ignore.append(i)\n",
    "    print(\"min IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "  if max:\n",
    "    for i in range(len(csv_train[var])):\n",
    "      if csv_train[var][i] > max:\n",
    "        list_ignore.append(i)\n",
    "    print(\"max IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "  if min and max:\n",
    "    print(\"total IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "    plt.hist(csv_train[var],range=[min,max],bins=bins)\n",
    "  elif min:\n",
    "    plt.hist(csv_train[var],range=[min,csv_train[var][largest]],bins=bins)\n",
    "  elif max:\n",
    "    plt.hist(csv_train[var],range=[csv_train[var][smallest],max],bins=bins)\n",
    "  else:\n",
    "    plt.hist(csv_train[var],bins=bins)\n",
    "  \n",
    "  plt.title(var)\n",
    "  if cut:\n",
    "    plt.xlim(left=min,right=max)\n",
    "  if ylog:\n",
    "    plt.yscale(\"log\")\n",
    "  if xlog:\n",
    "    plt.xscale(\"log\")\n",
    "  plt.show()\n",
    "  return list_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "BwsJ0YIoBiew",
    "outputId": "962724f0-0845-4ab5-8b27-043de6744e44"
   },
   "outputs": [],
   "source": [
    "filter_ignore(\"q_over_pt\",min=-4000,bins=30,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "2Wyjq07YGjBF",
    "outputId": "9a82db49-7e7e-49aa-8d4b-a0da738ec1f5"
   },
   "outputs": [],
   "source": [
    "filter_ignore(\"tanl\",max=1000,bins=25,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xauTTOyrHPyT"
   },
   "outputs": [],
   "source": [
    "rms_ignore = filter_ignore(\"rms\",max=0.1,bins=25,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "ERa-RZDbN6Hm",
    "outputId": "ac4577c9-f719-47d4-bc16-8fdbb7e413a5"
   },
   "outputs": [],
   "source": [
    "# 'q_over_pt', 'phi', 'tanl', 'D', 'z'\n",
    "# filter_ignore(\"D\", min=-200, ylog=True,bins=25)\n",
    "filter_ignore(\"z\",bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBO9wKCH2ePH"
   },
   "outputs": [],
   "source": [
    "csv_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc3WNfPaYwaL"
   },
   "source": [
    "### 1D Hist of all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "2VqlQxZRgTB7",
    "outputId": "2a5ce7d3-1665-4d71-80f5-5b7ed7ceb673"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"phi\"],bins=50) # -3 to 3, even distrib\n",
    "plt.title(\"phi\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"D\"],range=[-3000,80],bins=25) # -3000 to 50, but val in 65\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"D\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"z\"],bins=100)\n",
    "plt.title(\"z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TbqYK55L05mB",
    "outputId": "81dc2a8d-b572-4505-983a-8958a9bea8b6"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "# fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "plt.hist(csv_train[\"cov_00\"],range=[0,1e8],bins=25) # 0 to 1e13\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"cov_00\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"cov_01\"],bins=25) # -1e6 to over 1e5\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"cov_01\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"chisq\"],bins=25) # 0 to 200\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"chisq\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"Ndof\"],range=[0,44],bins=45) # ? this one weird 0 to ~43\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Ndof\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"rms\"],range=[0,0.1],bins=25) # \n",
    "# plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"rms\")\n",
    "plt.show()\n",
    "# ---\n",
    "# plt.hist(csv_train[\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "plt.hist(csv_train[csv_train[\"t_start_cntr_valid\"] == 1][\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "plt.title(\"t_start_cntr\")\n",
    "plt.show()\n",
    "\n",
    "# plt.hist(csv_train[\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "plt.hist(csv_train[csv_train[\"t_tof_valid\"] == 1][\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "plt.title(\"t_tof\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_bcal\"],bins=25) # ~-22 to 20\n",
    "plt.title(\"t_bcal\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_fcal\"],bins=25) # ~-100 to ~75\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"t_fcal\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"t_start_cntr_valid\"],bins=25) # a lot more 0s\n",
    "plt.title(\"t_start_cntr_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_tof_valid\"],bins=25) # about 5050\n",
    "plt.title(\"t_tof_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_bcal_valid\"],bins=25) # almost all 0s\n",
    "plt.title(\"t_bcal_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_fcal_valid\"],bins=25) # almost all 0s\n",
    "plt.title(\"t_fcal_valid\")\n",
    "plt.show()\n",
    "# ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbzN70C9MCrQ"
   },
   "source": [
    "### 1D Hist of Hit1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VGzHLbKsLUZ9",
    "outputId": "0e1a0a14-8881-439d-c2f3-f0ec09006cc4"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"hit1_u\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_u\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_v\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_v\")\n",
    "plt.show()\n",
    "# plt.hist(csv_train[\"hit1_sinv\"],bins=25) # most are 0.96603 almost all are around that though\n",
    "# plt.title(\"hit1_sinv\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_cosv\"],bins=25) # most -0.2585\n",
    "# plt.title(\"hit1_cosv\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_sinu\"],bins=25) # most 0.96585\n",
    "# plt.title(\"hit1_sinu\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_cosu\"],bins=25) # most 0.2591\n",
    "# plt.title(\"hit1_cosu\")\n",
    "# plt.show()\n",
    "plt.hist(csv_train[\"hit1_s\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_s\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_ds\"],bins=25) # 0.01 to 0.04\n",
    "plt.title(\"hit1_ds\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_wire\"],bins=101,range=[0,100]) # 0 to 100\n",
    "plt.title(\"hit1_wire\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_glayer\"],bins=25,range=[0,26]) # 6 to 23\n",
    "plt.title(\"hit1_glayer\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_z\"],bins=25) # spaced out between 180 and 340\n",
    "plt.title(\"hit1_z\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_time\"],bins=25) # -75 to 270\n",
    "plt.title(\"hit1_time\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_dE_amp\"],bins=25) # 0 to 2e-7\n",
    "plt.title(\"hit1_dE_amp\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_q\"],bins=25) # 0 to 85\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"hit1_q\")\n",
    "plt.show()\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxIRSBXpYzU7"
   },
   "source": [
    "### 2D Scatters of various data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "8hgtNdVxY-4_",
    "outputId": "9e12924e-9cb0-492c-fcbb-8cbc7ed4680e"
   },
   "outputs": [],
   "source": [
    "plt.scatter(csv_train[\"tanl\"],abs(csv_train[\"q_over_pt\"]),s=0.01) # a lot more 0s\n",
    "plt.title(\"q_over_pt vs tanl\")\n",
    "plt.xlabel(\"tanl\")\n",
    "plt.ylabel(\"q_over_pt\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "4pjOeJd85uWM",
    "outputId": "887b1312-0399-4223-ff5d-576875baa7e8"
   },
   "outputs": [],
   "source": [
    "# all create a plus sign\n",
    "plt.scatter(csv_train[\"t_start_cntr\"],csv_train[\"t_tof\"]) # a lot more 0s\n",
    "plt.title(\"t_tof vs t_start_cntr\")\n",
    "plt.xlabel(\"t_start_cntr\")\n",
    "plt.ylabel(\"t_tof\")\n",
    "plt.show()\n",
    "\n",
    "# plt.hist(csv_train[\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "# plt.hist(csv_train[\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "# plt.hist(csv_train[\"t_bcal\"],bins=25) # ~-22 to 20\n",
    "# plt.hist(csv_train[\"t_fcal\"],bins=25) # ~-100 to ~75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "IFF2_7npOseE",
    "outputId": "8e8bec48-6610-41d1-8faa-64814bb75af9"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"hit1_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit2_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit3_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit4_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit5_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit6_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit10_glayer\"],bins=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdRImNvKZ0Fa"
   },
   "source": [
    "### 2D Scatters of various hit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mxK94YV2FM2"
   },
   "outputs": [],
   "source": [
    "# Oval\n",
    "plt.scatter(csv_train[\"hit1_u\"],csv_train[\"hit1_v\"]) # -3 to 3, even distrib\n",
    "plt.title(\"v vs u\")\n",
    "plt.xlabel(\"u\")\n",
    "plt.ylabel(\"v\")\n",
    "plt.show()\n",
    "\n",
    "# like a flame\n",
    "plt.scatter(csv_train[\"hit1_s\"],csv_train[\"hit1_ds\"]) # -3 to 3, even distrib\n",
    "plt.title(\"ds vs s\")\n",
    "plt.xlabel(\"s\")\n",
    "plt.ylabel(\"ds\")\n",
    "plt.show()\n",
    "\n",
    "# hit1_wire, with single letters, forms an oval\n",
    "plt.scatter(csv_train[\"hit1_wire\"],csv_train[\"hit1_s\"]) # -3 to 3, even distrib\n",
    "plt.title(\"hit1_s vs hit1_wire\")\n",
    "plt.xlabel(\"hit1_wire\")\n",
    "plt.ylabel(\"hit1_s\")\n",
    "plt.show()\n",
    "\n",
    "# go up in steps\n",
    "plt.scatter(csv_train[\"hit1_glayer\"],csv_train[\"hit1_z\"]) # -3 to 3, even distrib\n",
    "plt.title(\"z vs glayer\")\n",
    "plt.xlabel(\"glayer\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.show()\n",
    "\n",
    "# 1:1\n",
    "plt.scatter(csv_train[\"hit1_q\"],csv_train[\"hit1_dE_amp\"]) # -3 to 3, even distrib\n",
    "plt.title(\"dE_amp vs q\")\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"dE_amp\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhNGqPbNQ8fX"
   },
   "outputs": [],
   "source": [
    "aax = \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dM6aySx8xO-"
   },
   "source": [
    "# Depricated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie8Rhqf765N5"
   },
   "source": [
    "## Non Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpJHFBHy74vp"
   },
   "outputs": [],
   "source": [
    "# --==Not in use?==--\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.8)\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "nInput = 10\n",
    "\n",
    "# inputs = keras.layers.Input((None,nInput))\n",
    "# print(\"train shape of one batch:\", x_train.shape[1:])\n",
    "\n",
    "# --==Set seed to get identical results==-- begin\n",
    "# from tensorflow.random import set_seed\n",
    "# np.random.seed(1)\n",
    "# set_seed(2)\n",
    "# --==Set seed to get identical results==-- end\n",
    "\n",
    "\n",
    "#--==Set Weights==--\n",
    "# loss_weights = [1/(sd**2)]\n",
    "# loss_weights = np.array(loss_weights)/sum(loss_weights)\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\", loss_weights=loss_weights, metrics=[\"mae\"])\n",
    "\n",
    "inputs = keras.Input((None,nInput),ragged=True)\n",
    "\n",
    "# --==Choose model==--\n",
    "# x = model(inputs)\n",
    "# x = model_timeless(inputs)\n",
    "# x = RNNTime(inputs)\n",
    "x = RNNTimeless(inputs)\n",
    "# x = RNNTimeStateful(inputs)\n",
    "\n",
    "\n",
    "outs = {\n",
    "    \"q_pt\":Dense(1, name=\"q_pt\")(x),\n",
    "    \"phi\":Dense(1, name=\"phi\")(x),\n",
    "    \"tanl\":Dense(1, name=\"tanl\")(x),\n",
    "    \"D\":Dense(1, name=\"D\")(x),\n",
    "    \"z\":Dense(1, name=\"z\")(x)\n",
    "}\n",
    "\n",
    "y_dict = {\n",
    "    \"q_pt\":y_train[:,0],\n",
    "    \"phi\":y_train[:,1],\n",
    "    \"tanl\":y_train[:,2],\n",
    "    \"D\":y_train[:,3],\n",
    "    \"z\":y_train[:,4]\n",
    "}\n",
    "\n",
    "\n",
    "# model = keras.Model(inputs=inputs, outputs=x, name=\"RNNModel\")\n",
    "# model = keras.Model(inputs=inputs, outputs=x, name=\"RNNModel\")\n",
    "model = keras.Model(inputs=inputs, outputs=outs, name=\"RNNModel\")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hw07sS4jFgJI"
   },
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01, patience=50, mode='min', verbose=1, restore_best_weights=True)\n",
    "rag_x = x_train[0]\n",
    "H = model.fit(x=rag_x, y=y_dict, batch_size=64, epochs=100, verbose=1, callbacks=[es])\n",
    "\n",
    "# Overfit\n",
    "# es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=100, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train[:10], y=y_train[:10], batch_size=1, epochs=200, verbose=1, shuffle=True, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMcu1m3k7rHK"
   },
   "source": [
    "## Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUBj5VjBuyL9"
   },
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7-v4CJdyQk8",
    "outputId": "940df566-bd20-4356-a2cb-da3bcfaa5fd9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "\n",
    "  # print(y_pred)\n",
    "\n",
    "  y_pred_a = []\n",
    "  for k in y_pred.keys():\n",
    "    y_pred_a.append(np.squeeze(y_pred[k]))\n",
    "\n",
    "  y_pred = np.array(y_pred_a).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  # print(y_pred.shape)\n",
    "  print(y_pred)\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "# for k in y_test.keys():\n",
    "#   y_test[k] = np.squeeze(y_test[k])\n",
    "# print(y_test)\n",
    "\n",
    "# print(y_test)\n",
    "# y_test_a = []\n",
    "# for k in y_test.keys():\n",
    "#   y_test_a.append(y_test[k])\n",
    "# y_test = np.squeeze(np.array(y_test_a))\n",
    "# print(y_test.shape)\n",
    "# print(y_test)\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test, y_test, inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAP0kzRvu2hz"
   },
   "source": [
    "### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7mcvkxuuweK",
    "outputId": "1197158f-904e-4103-f45a-2f015a71f639"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(q_pt_true, phi_true, tanl_true, D_true, z_true, q_pt_pred, phi_pred, tanl_pred, D_pred, z_pred, invcov):\n",
    "\n",
    "\n",
    "  y_pred = [q_pt_pred, phi_pred, tanl_pred, D_pred, z_pred]\n",
    "  # y_pred = np.array(y_pred).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  y_pred = tf.cast(y_pred, \"float64\")\n",
    "\n",
    "  y_true = [q_pt_true, phi_true, tanl_true, D_true, z_true]\n",
    "  # y_true = np.array(y_true).astype(\"float64\")\n",
    "  y_true = tf.transpose(y_true, perm=(1,0))\n",
    "  y_true = tf.cast(y_true, \"float64\")\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "y_test_a = []\n",
    "for k in y_test.keys():\n",
    "  y_test_a.append(y_test[k])\n",
    "y_test = np.squeeze(np.array(y_test_a))\n",
    "y_test = np.swapaxes(y_test, 0, 1)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test[:,0],x_test[:,1],x_test[:,2],x_test[:,3],x_test[:,4], y_test[:,0], y_test[:,1],y_test[:,2],y_test[:,3],y_test[:,4], inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kuUpxAuz1sz"
   },
   "source": [
    "### V4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRUvQVjHAD2U"
   },
   "source": [
    "So, we have the prediction and true vector\n",
    "\n",
    "$$\n",
    "y_{pred}=\n",
    "\\begin{bmatrix}\n",
    "q\\_pt \\\\ phi \\\\ tanl \\\\ D \\\\ z\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We have the inverse covariance matrix, we'll label it $C^{-1}$:\n",
    "\n",
    "and $y_p = y_{predict}$\n",
    "\n",
    "$$\n",
    "C^{-1} = \n",
    "\\begin{bmatrix}\n",
    "qq & qp & qt & qd & qz \\\\\n",
    "qp & pp & pt & pd & pz \\\\\n",
    "qt & pt & tt & td & tz \\\\\n",
    "qd & pd & td & dd & dz \\\\\n",
    "qz & pz & tz & dz & zz \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus, the formula before was:\n",
    "\n",
    "$$\n",
    "loss = C^{-1} \\cdot \\vec{y_p}  \\cdot \\vec{y_p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{dot} =\n",
    "  \\begin{bmatrix}\n",
    "    qq & qp & qt & qd & qz \\\\\n",
    "    qp & pp & pt & pd & pz \\\\\n",
    "    qt & pt & tt & td & tz \\\\\n",
    "    qd & pd & td & dd & dz \\\\\n",
    "    qz & pz & tz & dz & zz \n",
    "  \\end{bmatrix} \n",
    "  \\cdot\n",
    "  \\begin{bmatrix}\n",
    "    q\\_pt \\\\ phi \\\\ tanl \\\\ D \\\\ z\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{dot} = \n",
    "  \\begin{bmatrix}\n",
    "    qq*q\\_pt + qp*phi + qt*tanl + qd*D + qz*z \\\\\n",
    "    qp*q\\_pt + pp*phi + pt*tanl + pd*D + pz*z \\\\\n",
    "    qt*q\\_pt + pt*phi + tt*tanl + td*D + tz*z \\\\\n",
    "    qd*q\\_pt + pd*phi + td*tanl + dd*D + dz*z \\\\\n",
    "    qz*q\\_pt + dz*phi + tz*tanl + pz*D + zz*z\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Now, working on the output split for each variable, im looking for ways to seperate the variables after that operation.\n",
    "So maybe just sum the q_pt column of the matrix and multiply by q_pt?\n",
    "\n",
    "Maybe this would work? :\n",
    "\n",
    "$$\n",
    "loss_{q\\_pt} =  y_p^{q\\_pt} * \\sum_{i=0}^{4} C^{-1}_{qi}\n",
    "$$\n",
    "\n",
    "Where $C^{-1}_q$ is one row or column of the matrix of that variable\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8shIh2axz1g_",
    "outputId": "bbc9620f-ee91-4ed5-a02c-8e29e698e46b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  # print(y_pred)\n",
    "\n",
    "  # Theoretically:\n",
    "  # K.dot(invcov[0,:] * q_pt,q_pt)    # ?\n",
    "  \n",
    "\n",
    "\n",
    "  y_pred_a = []\n",
    "  for k in y_pred.keys():\n",
    "    y_pred_a.append(np.squeeze(y_pred[k]))\n",
    "\n",
    "  y_pred = np.array(y_pred_a).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  # print(y_pred.shape)\n",
    "  print(y_pred)\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "# for k in y_test.keys():\n",
    "#   y_test[k] = np.squeeze(y_test[k])\n",
    "# print(y_test)\n",
    "\n",
    "# print(y_test)\n",
    "# y_test_a = []\n",
    "# for k in y_test.keys():\n",
    "#   y_test_a.append(y_test[k])\n",
    "# y_test = np.squeeze(np.array(y_test_a))\n",
    "# print(y_test.shape)\n",
    "# print(y_test)\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test, y_test, inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7htvZHbENgC7"
   },
   "source": [
    "### V5 unedited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK6jqei9Nf5W"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  # y_dict = {\n",
    "  #     \"q_pt\":y_diff[0]*y_diff[0],\n",
    "  #     \"phi\":y_diff[0]*y_diff[0]\n",
    "  # }\n",
    "  # y_diff[0] / invcov[0][0]\n",
    "  return y_loss\n",
    "\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = x_train[2][0]\n",
    "# y_test = model.predict([x_train[0][0:1],x_train[1][0:1],x_train[2][0:1]])\n",
    "# y_test = np.squeeze(y_test)\n",
    "# inconv_test = x_train[1][0]\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "# # print('loss shape: '    + str(loss.shape)    )\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4XBVGilP4tc"
   },
   "source": [
    "### V6 New Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsGirPS0P4jb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "def customLoss(m_invcov):\n",
    "  def customLoss_fn(y_true, y_pred):\n",
    "    batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "    y_pred = tf.cast(K.reshape(y_pred, (batch_size, 5,1)),\"float64\") # y_pred  shape is now (batch, 5,1)\n",
    "    y_true = tf.cast(K.reshape(y_true, (batch_size, 5,1)),\"float64\") # y_state shape is now (batch, 5,1)\n",
    "    invcov = tf.cast(K.reshape(m_invcov, (batch_size, 5,5)),\"float64\") # invcov  shape is now (batch, 5,5)\n",
    "    \n",
    "    # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "    invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "    \n",
    "    # Difference between prediction and true state vectors\n",
    "    y_diff = y_pred - y_true\n",
    "\n",
    "    # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "    y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "    y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "    y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "    y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "    return y_loss\n",
    "  return customLoss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkuUUNJWjM_i"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "\n",
    "  y_pred = tf.cast(K.reshape(y_pred, (batch_size, 5,1)),\"float64\") # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = tf.cast(K.reshape(y_true, (batch_size, 5,1)),\"float64\") # y_state shape is now (batch, 5,1)\n",
    "  invcov = tf.cast(K.reshape(invcov, (batch_size, 5,5)),\"float64\") # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqNp5hG98yPr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DBknRRcl6LMm"
   ],
   "name": "CopyOfRNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
